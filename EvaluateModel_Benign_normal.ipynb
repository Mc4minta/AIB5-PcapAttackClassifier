{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mc4minta/AIB5-PcapAttackClassifier/blob/main/EvaluateModel_Benign_normal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWsPbK9AGFIL"
      },
      "source": [
        "# Full Step for evaluation (my model)\n",
        "- list pcap zip\n",
        "  - in `/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline`\n",
        "- unzip each zip\n",
        "- iterate through each attack\n",
        "- iterate to each attack pcap files\n",
        "- convert to FlowCSV\n",
        "- process packet map\n",
        "- preprocess_df\n",
        "- load model\n",
        "- predict\n",
        "- combined label with packet_index\n",
        "- convert flow-label-index to packetindex-label\n",
        "- list of predicted label from high to low (final_df)\n",
        "\n",
        "## final_df format:\n",
        "\n",
        "| pcap label     | packet label   | ranked_label                    |\n",
        "| -------------- | -------------- | ------------------------------- |\n",
        "| FTP-Bruteforce | FTP-Bruteforce | [FTP-Bruteforce,SSH-Bruteforce] |\n",
        "| Benign         | Benign         | [Benign]                        |\n",
        "\n",
        "## Evaluate by comparing:\n",
        "- pcap label and label\n",
        "- y_test = pcap label\n",
        "- y_pred = label\n",
        "\n",
        "## label_rank\n",
        "- is for tie breaker with baseline just in case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c967fa09",
        "outputId": "f4a581c4-26da-407e-b810-988b2e493211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpoFaRFw4fqB"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-jodantZ4fqB"
      },
      "outputs": [],
      "source": [
        "# define function for model loading\n",
        "\n",
        "import joblib\n",
        "def load_model(model_name):\n",
        "  model_path = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/' + model_name\n",
        "  model = joblib.load(model_path)\n",
        "  return model\n",
        "\n",
        "# model = load_model(model_name)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a079Fhu4fqC",
        "outputId": "665173eb-1b2c-4617-b4e8-f9af6addb900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=42)\n",
            "['Benign' 'DoS-HTTP-Flood' 'DoS-Slow-Rate' 'FTP-Bruteforce' 'PortScan'\n",
            " 'SSH-Bruteforce']\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "\n",
        "model_name = 'RandomForest400IntPortCIC1718-2.pkl'\n",
        "model = load_model(model_name)\n",
        "print(model)\n",
        "attacks = model.classes_\n",
        "print(attacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gj1OKUXaOMBW"
      },
      "outputs": [],
      "source": [
        "attacks = ['Benign']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-9ire2OO4F",
        "outputId": "813b506b-27fe-4abd-9ef4-846e3912848e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Benign']\n"
          ]
        }
      ],
      "source": [
        "print(attacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffr1MGrg4gil"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMMIz8EPIRK9",
        "outputId": "fafb4200-bfbd-4fcd-b1ea-7d3088b0ccca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoS.zip\n",
            "DDoS.zip\n",
            "PortScan.zip\n",
            "Benign.zip\n",
            "DoS-HTTP-Flood.zip\n",
            "Benign2.zip\n",
            "DoS-overall.zip\n",
            "DoS-Layer3and4.zip\n",
            "FTP-Bruteforce.zip\n",
            "SSH-Bruteforce.zip\n",
            "DoS-Slow-Rate.zip\n"
          ]
        }
      ],
      "source": [
        "# list pcap zip\n",
        "import os\n",
        "pcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline/'\n",
        "files = os.listdir(pcap_zip_dir)\n",
        "for file in files:\n",
        "  print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnsGRCFPEK8m",
        "outputId": "5cfc8a5a-2106-4751-a696-77a2fab6dd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoS_csv.zip\n",
            "DDoS_csv.zip\n",
            "PortScan_csv.zip\n",
            "Benign_csv.zip\n",
            "Benign2_csv.zip\n",
            "DoS-HTTP-Flood_csv.zip\n",
            "DoS-Layer3and4_csv.zip\n",
            "DoS-overall_csv.zip\n",
            "FTP-Bruteforce_csv.zip\n",
            "SSH-Bruteforce_csv.zip\n",
            "DoS-Slow-Rate_csv.zip\n"
          ]
        }
      ],
      "source": [
        "# list flow zip\n",
        "import os\n",
        "pcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForbaseline_csv/'\n",
        "files = os.listdir(pcap_zip_dir)\n",
        "for file in files:\n",
        "  print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uNcbNRbgLhfh",
        "outputId": "11f0e5ac-2c93-4648-cfab-747f2cbd35f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# list each attack folder without '.zip' and append to attacks[]\\npcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline/'\\nfiles = os.listdir(pcap_zip_dir)\\nattacks = []\\nfor file in files:\\n  attacks.append(file[:-4])\\nfor attack in attacks:\\n  print(attack)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''\n",
        "# list each attack folder without '.zip' and append to attacks[]\n",
        "pcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline/'\n",
        "files = os.listdir(pcap_zip_dir)\n",
        "attacks = []\n",
        "for file in files:\n",
        "  attacks.append(file[:-4])\n",
        "for attack in attacks:\n",
        "  print(attack)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7-6GKbTxV2W",
        "outputId": "8a6bc010-2af3-4712-f5e6-5ccd2702cb0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Benign']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "COBbu0u9JJe3"
      },
      "outputs": [],
      "source": [
        "# define function to unzip(attack.zip)\n",
        "import zipfile\n",
        "\n",
        "def unzip_attack_zip(attack):\n",
        "  pcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline/'\n",
        "  if(attack[-4:]=='.zip'):\n",
        "    zip_file_name = attack\n",
        "  else:\n",
        "    zip_file_name = attack + '.zip'\n",
        "  zip_file_path = os.path.join(pcap_zip_dir, zip_file_name)\n",
        "  destination_directory = '/content/'\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(destination_directory)\n",
        "    print(f\"'{zip_file_name}' unzipped to '{destination_directory}'.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error : {e}\")\n",
        "\n",
        "# unzip_attack_zip('DDoS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VqQjqkKbEXPc"
      },
      "outputs": [],
      "source": [
        "# define function to unzip(attack_csv.zip)\n",
        "import zipfile\n",
        "\n",
        "def unzip_attack_flow(attack):\n",
        "  pcap_zip_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForbaseline_csv/'\n",
        "  zip_file_name = attack + '_csv.zip'\n",
        "  zip_file_path = os.path.join(pcap_zip_dir, zip_file_name)\n",
        "  destination_directory = '/content/'\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(destination_directory)\n",
        "    print(f\"'{zip_file_name}' unzipped to '{destination_directory}'.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error : {e}\")\n",
        "\n",
        "# unzip_attack_flow('DDoS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCPKIXyUEj69",
        "outputId": "73d3cd2b-a239-4883-baee-2ac0afd7a61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Benign_csv...\n",
            "'Benign_csv.zip' unzipped to '/content/'.\n"
          ]
        }
      ],
      "source": [
        "# iterate through each attack_csv .zip and unzip it\n",
        "for attack in attacks:\n",
        "  print(f\"Processing {attack}_csv...\")\n",
        "  if(f'{attack}_csv' not in os.listdir('/content/')):\n",
        "    unzip_attack_flow(attack)\n",
        "  else:\n",
        "    print(f\"{attack}_csv already unzipped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKHO1ouhJmt9",
        "outputId": "6e84e060-ff0d-4155-bb9e-73101bd75fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Benign...\n",
            "'Benign.zip' unzipped to '/content/'.\n"
          ]
        }
      ],
      "source": [
        "# iterate through each attack .zip and unzip it\n",
        "for attack in attacks:\n",
        "  print(f\"Processing {attack}...\")\n",
        "  if(attack not in os.listdir('/content/')):\n",
        "    unzip_attack_zip(attack)\n",
        "  else:\n",
        "    print(f\"{attack} already unzipped\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.remove('/content/Benign/normal_4.pcap')\n",
        "  print('Removed')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "try:\n",
        "  os.remove('/content/Benign_csv/normal_4_ISCX.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mAQoG0R2-I8",
        "outputId": "f90af155-fc49-4fb9-941e-8de443ae5e74"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.remove('/content/Benign/normal_3.pcap')\n",
        "  print('Removed')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "try:\n",
        "  os.remove('/content/Benign_csv/normal_3_ISCX.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBHciHUL4nq7",
        "outputId": "29fe494f-b60d-4e26-ae48-2753c3ea5bbd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear unused file\n",
        "for attack in attacks:\n",
        "  pcap_dir = f'/content/{attack}'\n",
        "  flow_dir = f'/content/{attack}_csv'\n",
        "  files = os.listdir(pcap_dir)\n",
        "  for file in files:\n",
        "    file_name = file[:-5]\n",
        "    if 'normal_' not in file_name:\n",
        "      try:\n",
        "        os.remove(f'{pcap_dir}/{file_name}.pcap')\n",
        "        print(f'Removed {pcap_dir}/{file_name}')\n",
        "      except Exception as e:\n",
        "        print(f'{file_name} not found')\n",
        "    if 'normal_' not in file_name:\n",
        "      try:\n",
        "        os.remove(f'{flow_dir}/{file_name}_ISCX.csv')\n",
        "        print(f'Removed {flow_dir}/{file_name}')\n",
        "      except Exception as e:\n",
        "        print(f'{file_name} not found')"
      ],
      "metadata": {
        "id": "sgw2bgu7wAGU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear unused file\n",
        "for attack in attacks:\n",
        "  pcap_dir = f'/content/{attack}'\n",
        "  flow_dir = f'/content/{attack}_csv'\n",
        "  files = os.listdir(flow_dir)\n",
        "  for file in files:\n",
        "    file_name = file[:-4]\n",
        "    if 'normal_' not in file_name:\n",
        "      try:\n",
        "        os.remove(f'{flow_dir}/{file_name}.csv')\n",
        "        print(f'Removed {flow_dir}/{file_name}')\n",
        "      except Exception as e:\n",
        "        print(f'{file_name} not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNvEFyW64GD0",
        "outputId": "72350f32-8d9f-4a6e-f5d1-8483d1a90a63"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed /content/Benign_csv/Normal-h3_10_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_11_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_6_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_2_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_5_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_8_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_9_ISCX\n",
            "Removed /content/Benign_csv/DoS-Hulk_benign_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_1_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_12_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_3_ISCX\n",
            "Removed /content/Benign_csv/Normal-h3_4_ISCX\n",
            "Removed /content/Benign_csv/DoS-GoldenEye_benign_ISCX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTosyZGCOQjb",
        "outputId": "99da851f-d254-4fc9-90a6-8f558aa63dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "PCAP for Benign\n",
            "--------------------------------\n",
            "normal_2.pcap\n",
            "normal_10.pcap\n",
            "normal_13.pcap\n",
            "normal_12.pcap\n",
            "normal_1.pcap\n",
            "normal_11.pcap\n"
          ]
        }
      ],
      "source": [
        "# list all pcap file in each attack\n",
        "\n",
        "for attack in attacks:\n",
        "  print('--------------------------------')\n",
        "  print(f\"PCAP for {attack}\")\n",
        "  print('--------------------------------')\n",
        "  attack_pcaps = os.listdir(f'/content/{attack}')\n",
        "  for attack_pcap in attack_pcaps:\n",
        "    print(attack_pcap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4313y7t4Gua6",
        "outputId": "40b1a859-8e20-4e69-82cf-6ea5dcea8512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "CSV for Benign\n",
            "--------------------------------\n",
            "normal_13_ISCX.csv\n",
            "normal_11_ISCX.csv\n",
            "normal_10_ISCX.csv\n",
            "normal_12_ISCX.csv\n",
            "normal_1_ISCX.csv\n",
            "normal_2_ISCX.csv\n"
          ]
        }
      ],
      "source": [
        "# list all csv file in each attack\n",
        "\n",
        "for attack in attacks:\n",
        "  print('--------------------------------')\n",
        "  print(f\"CSV for {attack}\")\n",
        "  print('--------------------------------')\n",
        "  attack_flows = os.listdir(f'/content/{attack}_csv')\n",
        "  for attack_flow in attack_flows:\n",
        "    print(attack_flow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3lxmyW1G2xg"
      },
      "source": [
        "# Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzjTd5l_O1-t",
        "outputId": "72aebe99-9958-4bc6-f8c4-1a16f240e2cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Benign_index folder\n"
          ]
        }
      ],
      "source": [
        "# create attack_index folder\n",
        "\n",
        "import os\n",
        "for attack in attacks:\n",
        "  os.makedirs(f'/content/{attack}_index', exist_ok=True)\n",
        "  print(f\"Created {attack}_index folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "987RkmArRbp1"
      },
      "outputs": [],
      "source": [
        "# install scapy\n",
        "\n",
        "!pip install scapy > /content/log.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "7GkfYqDGRV1-"
      },
      "outputs": [],
      "source": [
        "# define function for simulating cicflowmter\n",
        "\n",
        "\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "import ipaddress # For robust IP address comparison\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Import Scapy for PCAP reading and parsing\n",
        "# You might need to install it: pip install scapy\n",
        "try:\n",
        "    from scapy.all import rdpcap, IP, TCP, UDP\n",
        "except ImportError:\n",
        "    print(\"Scapy not found. Please install it using: pip install scapy\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Define a Packet structure to standardize data from Scapy packets\n",
        "# 'index' corresponds to the sequential position in the PCAP file,\n",
        "# which is similar to the 'id' in CICFlowMeter's BasicPacketInfo.java\n",
        "Packet = namedtuple('Packet', ['index', 'timestamp', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'protocol', 'length', 'has_fin_flag'])\n",
        "\n",
        "class Flow:\n",
        "    \"\"\"\n",
        "    Simulates a network flow, similar to BasicFlow.java.\n",
        "    A flow is identified by its 5-tuple and stores a list of packet indices\n",
        "    that belong to this flow. It also tracks basic flow statistics and timestamps.\n",
        "    \"\"\"\n",
        "    def __init__(self, flow_key, first_packet):\n",
        "        self.flow_key = flow_key\n",
        "        # This list directly maps the flow to its packet indices (IDs)\n",
        "        self.packet_indices = [first_packet.index]\n",
        "        # Store packet timestamps as well for export\n",
        "        self.packet_timestamps = [int(first_packet.timestamp * 1_000_000)] # Convert to microseconds\n",
        "\n",
        "        self.start_time = int(first_packet.timestamp * 1_000_000) # Microseconds\n",
        "        self.last_packet_time = int(first_packet.timestamp * 1_000_000) # Microseconds\n",
        "        self.packet_count = 1\n",
        "        self.byte_count = first_packet.length\n",
        "        self.fwd_packets = [] # Simulating BasicFlow's 'forward' list\n",
        "        self.bwd_packets = [] # Simulating BasicFlow's 'backward' list\n",
        "\n",
        "        # Determine the initial direction based on the first packet's original IPs\n",
        "        # This is used for 'forward' and 'backward' packet grouping within the flow,\n",
        "        # distinct from the canonical direction used for the flow_key.\n",
        "        self.initial_src_ip = first_packet.src_ip\n",
        "\n",
        "        # Add first packet to appropriate directional list\n",
        "        if self._is_forward_packet(first_packet):\n",
        "            self.fwd_packets.append(first_packet)\n",
        "        else:\n",
        "            self.bwd_packets.append(first_packet)\n",
        "\n",
        "    def add_packet(self, packet):\n",
        "        \"\"\"Adds a packet to the flow and updates flow statistics.\"\"\"\n",
        "        self.packet_indices.append(packet.index)\n",
        "        self.packet_timestamps.append(int(packet.timestamp * 1_000_000)) # Store timestamp in microseconds\n",
        "\n",
        "        self.packet_count += 1\n",
        "        self.byte_count += packet.length\n",
        "\n",
        "        # Update directional packet lists and IATs (simplified for simulation)\n",
        "        if self._is_forward_packet(packet):\n",
        "            self.fwd_packets.append(packet)\n",
        "            # In real CICFlowMeter, IATs and other stats would be updated here, e.g.:\n",
        "            # if len(self.fwd_packets) > 1:\n",
        "            #     self.fwd_iat.add_value(packet.timestamp - self.fwd_packets[-2].timestamp)\n",
        "        else:\n",
        "            self.bwd_packets.append(packet)\n",
        "            # if len(self.bwd_packets) > 1:\n",
        "            #     self.bwd_iat.add_value(packet.timestamp - self.bwd_packets[-2].timestamp)\n",
        "\n",
        "        # Update last packet time for overall flow duration/IAT calculation\n",
        "        self.last_packet_time = int(packet.timestamp * 1_000_000) # Microseconds\n",
        "\n",
        "    def get_flow_duration(self):\n",
        "        \"\"\"Calculates the duration of the flow in microseconds.\"\"\"\n",
        "        return self.last_packet_time - self.start_time\n",
        "\n",
        "    def _is_forward_packet(self, packet):\n",
        "        \"\"\"\n",
        "        Determines if a packet is in the forward direction relative to the flow's initial direction.\n",
        "        This uses the original src_ip of the *first* packet to define \"forward\" for feature accumulation,\n",
        "        which is consistent with CICFlowMeter's internal `BasicFlow` logic.\n",
        "        \"\"\"\n",
        "        return packet.src_ip == self.initial_src_ip\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Flow(key={self.flow_key}, total_packets={self.packet_count}, \"\n",
        "                f\"duration={self.get_flow_duration():.4f}us, \"\n",
        "                f\"total_bytes={self.byte_count}B, \"\n",
        "                f\"fwd_pkts={len(self.fwd_packets)}, bwd_pkts={len(self.bwd_packets)}, \"\n",
        "                f\"packet_indices={self.packet_indices})\")\n",
        "\n",
        "    def to_csv_row(self):\n",
        "        \"\"\"\n",
        "        Converts the flow data into a dictionary suitable for CSV writing.\n",
        "        This provides a simplified representation of the features CICFlowMeter extracts,\n",
        "        but crucially includes the 'Packet Indices' and 'Packet Timestamps' columns.\n",
        "        The 'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port' are taken from the\n",
        "        canonical flow_key for consistency with CICFlowMeter's output format.\n",
        "        \"\"\"\n",
        "        # The flow_key tuple already holds the normalized 5-tuple as generated by generate_flow_key\n",
        "        canonical_src_ip, canonical_dst_ip, canonical_src_port, canonical_dst_port, proto = self.flow_key\n",
        "\n",
        "        return {\n",
        "            'Flow ID': f\"{canonical_src_ip}-{canonical_dst_ip}-{canonical_src_port}-{canonical_dst_port}-{proto}\",\n",
        "            'Src IP': canonical_src_ip,\n",
        "            'Src Port': canonical_src_port,\n",
        "            'Dst IP': canonical_dst_ip,\n",
        "            'Dst Port': canonical_dst_port,\n",
        "            'Protocol': proto,\n",
        "            'Flow Duration (us)': self.get_flow_duration(),\n",
        "            'Total Packets': self.packet_count,\n",
        "            'Total Bytes': self.byte_count,\n",
        "            'Fwd Packets': len(self.fwd_packets), # These counts are based on internal 'initial_src_ip'\n",
        "            'Bwd Packets': len(self.bwd_packets), # These counts are based on internal 'initial_src_ip'\n",
        "            'Packet Indices': str(self.packet_indices), # Convert list to string for CSV column\n",
        "            'Packet Timestamps': str(self.packet_timestamps) # Convert list to string for CSV column\n",
        "            # Add more CICFlowMeter-like features here if needed\n",
        "            # 'Flow Pkts/s': self.packet_count / (self.get_flow_duration() / 1_000_000.0) if self.get_flow_duration() > 0 else 0,\n",
        "            # 'Avg Fwd Pkt Len': sum(p.length for p in self.fwd_packets) / len(self.fwd_packets) if self.fwd_packets else 0,\n",
        "        }\n",
        "\n",
        "\n",
        "def generate_flow_key(packet):\n",
        "    \"\"\"\n",
        "    Generates a unique key for a flow based on the 5-tuple,\n",
        "    mimicking CICFlowMeter's Java BasicPacketInfo.generateFlowId() logic\n",
        "    for canonicalizing IP addresses and ports.\n",
        "\n",
        "    IMPORTANT: This function uses the raw protocol number, matching BasicPacketInfo.java.\n",
        "    Any mapping of non-TCP/UDP/ICMP protocols to '0' happens *after* Flow ID generation\n",
        "    in CICFlowMeter's pipeline (e.g., for display in FlowFeature.featureValue2String),\n",
        "    and is NOT part of the Flow ID itself.\n",
        "    \"\"\"\n",
        "    # Access attributes by name from the Packet namedtuple\n",
        "    src_ip_str = packet.src_ip\n",
        "    dst_ip_str = packet.dst_ip\n",
        "    src_port = packet.src_port\n",
        "    dst_port = packet.dst_port\n",
        "    protocol_int = packet.protocol # Use the raw protocol number here\n",
        "\n",
        "    # Use ipaddress for robust IP comparison, mirroring Java's byte-by-byte comparison\n",
        "    try:\n",
        "        src_ip_obj = ipaddress.ip_address(src_ip_str)\n",
        "        dst_ip_obj = ipaddress.ip_address(dst_ip_str)\n",
        "    except ValueError:\n",
        "        # Fallback for invalid IPs if any, should not happen with valid PCAP data\n",
        "        return (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "\n",
        "\n",
        "    # Determine 'forward' based on IP comparison: canonical_src_ip will be the \"smaller\" IP\n",
        "    if src_ip_obj < dst_ip_obj:\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "    elif dst_ip_obj < src_ip_obj:\n",
        "        # Swap IPs and their corresponding ports for normalization\n",
        "        normalized_src_ip = dst_ip_str\n",
        "        normalized_dst_ip = src_ip_str\n",
        "        normalized_src_port = dst_port\n",
        "        normalized_dst_port = src_port\n",
        "    else: # IPs are equal (e.g., multicast or broadcast)\n",
        "        # If IPs are the same, Java's logic does NOT swap ports based on IP.\n",
        "        # It keeps original src/dst IPs and ports as they are.\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "\n",
        "    # The canonical 5-tuple key for the hash map\n",
        "    return (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, protocol_int)\n",
        "\n",
        "def process_packets_into_flows(packets, flow_timeout_us=120000000, idle_timeout_us=5000000):\n",
        "    \"\"\"\n",
        "    Processes a list of packets and groups them into flows, simulating FlowGenerator.java.\n",
        "    Args:\n",
        "        packets (list): A list of Packet namedtuples, derived from PCAP.\n",
        "        flow_timeout_us (int): Max flow duration in microseconds (120 seconds).\n",
        "        idle_timeout_us (int): Max idle time within a flow in microseconds (5 seconds).\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are unique flow identifiers (combining 5-tuple and a counter)\n",
        "              and values are Flow objects.\n",
        "    \"\"\"\n",
        "    active_flows = {} # {flow_key_5_tuple: Flow_object}\n",
        "    completed_flows = {} # {unique_completed_flow_id: Flow_object}\n",
        "\n",
        "    # Packets are assumed to be already sorted by timestamp when passed from PCAP reader\n",
        "\n",
        "    completed_flow_counter = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        flow_key = generate_flow_key(packet)\n",
        "        current_timestamp_us = int(packet.timestamp * 1_000_000) # Convert seconds to microseconds\n",
        "\n",
        "        # Check if this packet belongs to an existing active flow\n",
        "        if flow_key in active_flows:\n",
        "            flow = active_flows[flow_key]\n",
        "\n",
        "            # Check for IDLE timeout first (packet arrival AFTER idle period)\n",
        "            if (current_timestamp_us - flow.last_packet_time) > idle_timeout_us:\n",
        "                # Flow idle timed out, finish current flow and start new one\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove old flow from active\n",
        "\n",
        "                new_flow = Flow(flow_key, packet)\n",
        "                active_flows[flow_key] = new_flow\n",
        "\n",
        "            # Check for TOTAL flow timeout (flow duration)\n",
        "            elif (current_timestamp_us - flow.start_time) > flow_timeout_us:\n",
        "                # Flow timed out based on total duration, finish current flow and start new one\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove old flow from active\n",
        "\n",
        "                new_flow = Flow(flow_key, packet)\n",
        "                active_flows[flow_key] = new_flow\n",
        "\n",
        "            # Simulate TCP FIN flag termination\n",
        "            # Only apply if the protocol is TCP (6) and FIN flag is set.\n",
        "            # Add the FIN packet to the flow before deciding if it's finished.\n",
        "            elif packet.protocol == 6 and packet.has_fin_flag:\n",
        "                flow.add_packet(packet) # Add the FIN packet\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove from active flows as it's finished\n",
        "\n",
        "            # Otherwise, add packet to existing active flow\n",
        "            else:\n",
        "                flow.add_packet(packet)\n",
        "                # No explicit idle time update needed in Flow object here,\n",
        "                # as it's checked upon next packet arrival.\n",
        "\n",
        "        else:\n",
        "            # New flow, or a flow that previously completed and was removed from active_flows\n",
        "            new_flow = Flow(flow_key, packet)\n",
        "            active_flows[flow_key] = new_flow\n",
        "\n",
        "    # After processing all packets, move any remaining active flows to completed flows\n",
        "    for flow_key, flow in list(active_flows.items()): # Iterate over a copy to allow modification\n",
        "        completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "        completed_flow_counter += 1\n",
        "        # No need to pop from active_flows here, as loop is over.\n",
        "\n",
        "    return completed_flows\n",
        "\n",
        "def extract_packet_info_from_pcap(pcap_file_path):\n",
        "    \"\"\"\n",
        "    Reads a PCAP file using Scapy and extracts relevant information into Packet namedtuples.\n",
        "    Assigns a sequential index to each packet as it's read.\n",
        "    Broadened to include all IP packets, not just TCP/UDP.\n",
        "    \"\"\"\n",
        "    print(f\"Reading packets from {pcap_file_path}...\")\n",
        "    extracted_packets = []\n",
        "\n",
        "    try:\n",
        "        packets_scapy = rdpcap(pcap_file_path)\n",
        "        total_packets = len(packets_scapy)\n",
        "        print(f\"Successfully read {total_packets} packets from {pcap_file_path}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PCAP file not found at {pcap_file_path}.\")\n",
        "        return [], 0 # Return empty list and 0 total packets on error\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading PCAP file {pcap_file_path}: {e}\")\n",
        "        return [], 0 # Return empty list and 0 total packets on other errors\n",
        "\n",
        "\n",
        "    for i, pkt in enumerate(packets_scapy):\n",
        "        src_ip = None\n",
        "        dst_ip = None\n",
        "        src_port = 0 # Default to 0 for non-TCP/UDP protocols or if ports are missing\n",
        "        dst_port = 0 # Default to 0 for non-TCP/UDP protocols or if ports are missing\n",
        "        protocol = None\n",
        "        has_fin = False\n",
        "\n",
        "        # Ensure IP layer exists\n",
        "        if IP in pkt:\n",
        "            src_ip = pkt[IP].src\n",
        "            dst_ip = pkt[IP].dst\n",
        "            protocol = pkt[IP].proto # e.g., 6 for TCP, 17 for UDP, 1 for ICMP, 2 for IGMP, etc.\n",
        "\n",
        "            # Check for transport layer (TCP or UDP) to get ports and flags\n",
        "            if TCP in pkt:\n",
        "                src_port = pkt[TCP].sport\n",
        "                dst_port = pkt[TCP].dport\n",
        "                has_fin = bool(pkt[TCP].flags & 0x01) # FIN is bit 0 in TCP flags\n",
        "            elif UDP in pkt:\n",
        "                src_port = pkt[UDP].sport\n",
        "                dst_port = pkt[UDP].dport\n",
        "            # For other IP protocols (like ICMP, IGMP, etc.), src_port and dst_port remain 0.\n",
        "\n",
        "            # Only process packets with valid IP information\n",
        "            # This condition is now implicitly true for any packet with an IP layer,\n",
        "            # as src_ip, dst_ip, and protocol will be extracted.\n",
        "            extracted_packets.append(Packet(\n",
        "                index=i,\n",
        "                timestamp=pkt.time, # Scapy's pkt.time is already in seconds (float)\n",
        "                src_ip=src_ip,\n",
        "                dst_ip=dst_ip,\n",
        "                src_port=src_port,\n",
        "                dst_port=dst_port,\n",
        "                protocol=protocol,\n",
        "                length=len(pkt), # Total packet length\n",
        "                has_fin_flag=has_fin\n",
        "            ))\n",
        "\n",
        "    # Scapy's rdpcap usually returns packets in capture order (by timestamp),\n",
        "    # but explicit sorting ensures strict chronological processing as in CICFlowMeter.\n",
        "    extracted_packets.sort(key=lambda p: p.timestamp)\n",
        "    print(f\"Extracted {len(extracted_packets)} valid packets from PCAP.\")\n",
        "    return extracted_packets,total_packets\n",
        "\n",
        "def extract_flows_from_pcap(pcap_file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire process of extracting network flows from a PCAP file,\n",
        "    mimicking CICFlowMeter's logic, and returns the flows as a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        pcap_file_path (str): The path to the input PCAP file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row represents a discovered flow,\n",
        "                      including 'Packet Indices' and 'Packet Timestamps'.\n",
        "                      Returns an empty DataFrame if no valid packets are found or\n",
        "                      if PCAP file cannot be read.\n",
        "    \"\"\"\n",
        "    # Step 1: Extract packet information from the PCAP file\n",
        "    packets_from_pcap,total_packets = extract_packet_info_from_pcap(pcap_file_path)\n",
        "\n",
        "    if not packets_from_pcap:\n",
        "        print(\"No valid packets found or PCAP file could not be read. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame(), total_packets # Return empty DataFrame and total_packets (which will be 0 on error)\n",
        "\n",
        "    print(\"\\nProcessing packets into flows (simulating CICFlowMeter logic)...\")\n",
        "    # Step 2: Process the extracted packets into flows\n",
        "    # Default timeouts are 120 seconds (flow) and 5 seconds (idle) for CICFlowMeter\n",
        "    flows_data = process_packets_into_flows(packets_from_pcap,\n",
        "                                            flow_timeout_us=120_000_000,\n",
        "                                            idle_timeout_us=5_000_000)\n",
        "\n",
        "    print(f\"\\nDiscovered {len(flows_data)} flows.\")\n",
        "\n",
        "    # Step 3: Convert discovered flows to a list of dictionaries for DataFrame creation\n",
        "    flows_list_of_dicts = []\n",
        "    for flow_unique_id, flow_obj in flows_data.items():\n",
        "        flows_list_of_dicts.append(flow_obj.to_csv_row())\n",
        "\n",
        "    # Create DataFrame from the list of flow dictionaries\n",
        "    flows_df = pd.DataFrame(flows_list_of_dicts)\n",
        "\n",
        "    # Step 4: Display a summary of generated flows (for console output)\n",
        "    if not flows_df.empty:\n",
        "        print(\"\\nHead of the generated Flows DataFrame:\")\n",
        "        print(flows_df.head())\n",
        "        print(\"\\nColumns in the generated Flows DataFrame:\")\n",
        "        print(flows_df.columns.tolist())\n",
        "    else:\n",
        "        print(\"\\nNo flows generated to display.\")\n",
        "\n",
        "    return flows_df,total_packets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCx91HegRlrc",
        "outputId": "71328707-ac87-4758-c5f3-3ed1fc6866eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading packets from /content/Benign/normal_2.pcap...\n",
            "Successfully read 2000000 packets from /content/Benign/normal_2.pcap.\n",
            "Extracted 328349 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 4878 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                    Flow ID          Src IP  Src Port  \\\n",
            "0    117.18.237.29-192.168.1.195-80-49812-6   117.18.237.29        80   \n",
            "1    117.18.237.29-192.168.1.195-80-49812-6   117.18.237.29        80   \n",
            "2    117.18.237.29-192.168.1.195-80-49812-6   117.18.237.29        80   \n",
            "3  111.221.29.253-192.168.1.195-443-50314-6  111.221.29.253       443   \n",
            "4  111.221.29.253-192.168.1.195-443-50314-6  111.221.29.253       443   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0  192.168.1.195     49812         6              304658              2   \n",
            "1  192.168.1.195     49812         6             1203705              2   \n",
            "2  192.168.1.195     49812         6             4812228              2   \n",
            "3  192.168.1.195     50314         6              851541             21   \n",
            "4  192.168.1.195     50314         6                 295              3   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0          124            2            0   \n",
            "1          124            2            0   \n",
            "2          124            2            0   \n",
            "3         7243           12            9   \n",
            "4          222            2            1   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0                                       [1143, 1193]   \n",
            "1                                       [1306, 1516]   \n",
            "2                                       [1948, 2816]   \n",
            "3  [5451, 5487, 5488, 5489, 5511, 5512, 5513, 551...   \n",
            "4                                 [5639, 5640, 5641]   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0               [1554230132881143, 1554230133185801]  \n",
            "1               [1554230133795000, 1554230134998705]  \n",
            "2               [1554230137404691, 1554230142216919]  \n",
            "3  [1554230156294561, 1554230156472827, 155423015...  \n",
            "4  [1554230157273550, 1554230157273751, 155423015...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n",
            "Reading packets from /content/Benign/normal_10.pcap...\n",
            "Successfully read 2000000 packets from /content/Benign/normal_10.pcap.\n",
            "Extracted 264287 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 3584 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                   Flow ID        Src IP  Src Port  \\\n",
            "0   52.147.9.175-192.168.1.195-443-54303-6  52.147.9.175       443   \n",
            "1   52.147.9.175-192.168.1.195-443-54303-6  52.147.9.175       443   \n",
            "2   52.147.9.175-192.168.1.195-443-54304-6  52.147.9.175       443   \n",
            "3   52.147.9.175-192.168.1.195-443-54304-6  52.147.9.175       443   \n",
            "4  192.168.1.1-192.168.1.195-49152-54305-6   192.168.1.1     49152   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0  192.168.1.195     54303         6              189303             18   \n",
            "1  192.168.1.195     54303         6                 206              2   \n",
            "2  192.168.1.195     54304         6              315830             12   \n",
            "3  192.168.1.195     54304         6                  73              2   \n",
            "4  192.168.1.195     54305         6              126269              7   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0        12616           11            7   \n",
            "1          124            2            0   \n",
            "2        10453            7            5   \n",
            "3          124            2            0   \n",
            "4         1616            4            3   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0  [3629, 3632, 3633, 3634, 3645, 3646, 3648, 365...   \n",
            "1                                       [3681, 3682]   \n",
            "2  [3692, 3710, 3711, 3749, 3775, 3776, 3777, 377...   \n",
            "3                                       [3796, 3797]   \n",
            "4         [5644, 5666, 5668, 5669, 5672, 5673, 5675]   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0  [1554323906192058, 1554323906204819, 155432390...  \n",
            "1               [1554323906381454, 1554323906381660]  \n",
            "2  [1554323906383664, 1554323906461965, 155432390...  \n",
            "3               [1554323906699649, 1554323906699722]  \n",
            "4  [1554323917420444, 1554323917526448, 155432391...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n",
            "Reading packets from /content/Benign/normal_13.pcap...\n",
            "Successfully read 1918446 packets from /content/Benign/normal_13.pcap.\n",
            "Extracted 298552 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 6773 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                    Flow ID         Src IP  Src Port  \\\n",
            "0   192.168.1.152-216.58.199.74-38406-443-6  192.168.1.152     38406   \n",
            "1   192.168.1.152-216.58.199.74-38406-443-6  192.168.1.152     38406   \n",
            "2  192.168.1.152-192.168.1.193-1880-49238-6  192.168.1.152      1880   \n",
            "3  192.168.1.152-192.168.1.193-1880-49238-6  192.168.1.152      1880   \n",
            "4    52.114.74.43-192.168.1.195-443-52139-6   52.114.74.43       443   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0  216.58.199.74       443         6                 536              3   \n",
            "1  216.58.199.74       443         6                3004              2   \n",
            "2  192.168.1.193     49238         6                   4              2   \n",
            "3  192.168.1.193     49238         6                   5              2   \n",
            "4  192.168.1.195     52139         6             2305127             16   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0          267            3            0   \n",
            "1          136            2            0   \n",
            "2          130            1            1   \n",
            "3          130            1            1   \n",
            "4         7425            9            7   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0                                 [1219, 1220, 1221]   \n",
            "1                                       [1228, 1229]   \n",
            "2                                       [2788, 2790]   \n",
            "3                                     [11027, 11030]   \n",
            "4  [18871, 18945, 18946, 18947, 19002, 19019, 190...   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0  [1554274047612303, 1554274047612653, 155427404...  \n",
            "1               [1554274047653109, 1554274047656113]  \n",
            "2               [1554274056077215, 1554274056077219]  \n",
            "3               [1554274101259942, 1554274101259947]  \n",
            "4  [1554274144662285, 1554274145094885, 155427414...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n",
            "Reading packets from /content/Benign/normal_12.pcap...\n",
            "Successfully read 2000000 packets from /content/Benign/normal_12.pcap.\n",
            "Extracted 269561 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 4654 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                    Flow ID         Src IP  Src Port  \\\n",
            "0    3.122.49.24-192.168.1.152-1883-52976-6    3.122.49.24      1883   \n",
            "1  192.168.1.152-192.168.1.195-1880-54163-6  192.168.1.152      1880   \n",
            "2          127.0.0.1-127.0.0.1-42100-7878-6      127.0.0.1     42100   \n",
            "3          127.0.0.1-127.0.0.1-7878-42100-6      127.0.0.1      7878   \n",
            "4  192.168.1.190-192.168.1.190-43530-7878-6  192.168.1.190     43530   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0  192.168.1.152     52976         6           119716482            880   \n",
            "1  192.168.1.195     54163         6           119719870           1112   \n",
            "2      127.0.0.1      7878         6           119999438            242   \n",
            "3      127.0.0.1     42100         6           119999301            121   \n",
            "4  192.168.1.190      7878         6           119999478            242   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0       173296          482          398   \n",
            "1       595912          585          527   \n",
            "2        19360          242            0   \n",
            "3        35585          121            0   \n",
            "4        19360          242            0   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0  [13, 73, 74, 104, 108, 124, 167, 168, 171, 172...   \n",
            "1  [21, 22, 80, 81, 113, 114, 115, 116, 130, 131,...   \n",
            "2  [30, 33, 217, 220, 389, 391, 557, 559, 717, 72...   \n",
            "3  [32, 219, 390, 558, 719, 890, 1079, 1247, 1420...   \n",
            "4  [31, 35, 218, 222, 392, 394, 560, 562, 721, 72...   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0  [1554346868424605, 1554346868751802, 155434686...  \n",
            "1  [1554346868472355, 1554346868472547, 155434686...  \n",
            "2  [1554346868517667, 1554346868517810, 155434686...  \n",
            "3  [1554346868517793, 1554346869517457, 155434687...  \n",
            "4  [1554346868517739, 1554346868517858, 155434686...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n",
            "Reading packets from /content/Benign/normal_1.pcap...\n",
            "Successfully read 2000000 packets from /content/Benign/normal_1.pcap.\n",
            "Extracted 503830 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 4789 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                     Flow ID         Src IP  Src Port  \\\n",
            "0   192.168.1.152-192.168.1.193-1880-49179-6  192.168.1.152      1880   \n",
            "1     117.18.237.29-192.168.1.190-80-34263-6  117.18.237.29        80   \n",
            "2     52.35.21.241-192.168.1.190-443-52937-6   52.35.21.241       443   \n",
            "3    52.89.179.237-192.168.1.190-443-54128-6  52.89.179.237       443   \n",
            "4  192.168.1.1-239.255.255.250-59892-1900-17    192.168.1.1     59892   \n",
            "\n",
            "            Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0    192.168.1.193     49179         6                 156              2   \n",
            "1    192.168.1.190     34263         6               54756              7   \n",
            "2    192.168.1.190     52937         6              939826             12   \n",
            "3    192.168.1.190     54128         6              660562             19   \n",
            "4  239.255.255.250      1900        17               14287              5   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0          130            1            1   \n",
            "1         1719            4            3   \n",
            "2         5966            7            5   \n",
            "3        12182           10            9   \n",
            "4         1841            5            0   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0                                       [2412, 2413]   \n",
            "1  [13300, 13308, 13309, 13310, 13314, 13315, 13316]   \n",
            "2  [13162, 13205, 13206, 13207, 13242, 13244, 132...   \n",
            "3  [13412, 13463, 13464, 13465, 13510, 13512, 135...   \n",
            "4                [14647, 14649, 14650, 14652, 14653]   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0               [1554220335875204, 1554220335875360]  \n",
            "1  [1554220384282926, 1554220384324161, 155422038...  \n",
            "2  [1554220383662493, 1554220383873424, 155422038...  \n",
            "3  [1554220384594790, 1554220384798943, 155422038...  \n",
            "4  [1554220390147132, 1554220390151193, 155422039...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n",
            "Reading packets from /content/Benign/normal_11.pcap...\n",
            "Successfully read 2000000 packets from /content/Benign/normal_11.pcap.\n",
            "Extracted 268263 valid packets from PCAP.\n",
            "\n",
            "Processing packets into flows (simulating CICFlowMeter logic)...\n",
            "\n",
            "Discovered 3503 flows.\n",
            "\n",
            "Head of the generated Flows DataFrame:\n",
            "                                    Flow ID         Src IP  Src Port  \\\n",
            "0           192.168.1.103-224.0.0.252-0-0-2  192.168.1.103         0   \n",
            "1    3.122.49.24-192.168.1.152-1883-52976-6    3.122.49.24      1883   \n",
            "2          127.0.0.1-127.0.0.1-42100-7878-6      127.0.0.1     42100   \n",
            "3          127.0.0.1-127.0.0.1-7878-42100-6      127.0.0.1      7878   \n",
            "4  192.168.1.190-192.168.1.190-43530-7878-6  192.168.1.190     43530   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol  Flow Duration (us)  Total Packets  \\\n",
            "0    224.0.0.252         0         2                   0              1   \n",
            "1  192.168.1.152     52976         6           119792092            870   \n",
            "2      127.0.0.1      7878         6           119000177            240   \n",
            "3      127.0.0.1     42100         6           119000021            120   \n",
            "4  192.168.1.190      7878         6           119000215            240   \n",
            "\n",
            "   Total Bytes  Fwd Packets  Bwd Packets  \\\n",
            "0           62            1            0   \n",
            "1       172471          461          409   \n",
            "2        19200          240            0   \n",
            "3        35304          120            0   \n",
            "4        19200          240            0   \n",
            "\n",
            "                                      Packet Indices  \\\n",
            "0                                            [13269]   \n",
            "1  [9, 72, 73, 128, 129, 168, 191, 197, 198, 246,...   \n",
            "2  [41, 44, 236, 238, 416, 418, 583, 585, 748, 75...   \n",
            "3  [43, 237, 417, 584, 749, 920, 1110, 1284, 1451...   \n",
            "4  [42, 46, 239, 241, 419, 421, 586, 588, 751, 75...   \n",
            "\n",
            "                                   Packet Timestamps  \n",
            "0                                 [1554335477269776]  \n",
            "1  [1554335401323726, 1554335401660027, 155433540...  \n",
            "2  [1554335401517646, 1554335401517819, 155433540...  \n",
            "3  [1554335401517791, 1554335402517387, 155433540...  \n",
            "4  [1554335401517727, 1554335401517857, 155433540...  \n",
            "\n",
            "Columns in the generated Flows DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Flow Duration (us)', 'Total Packets', 'Total Bytes', 'Fwd Packets', 'Bwd Packets', 'Packet Indices', 'Packet Timestamps']\n"
          ]
        }
      ],
      "source": [
        "# simulate cicflowmeter for each pcap file\n",
        "\n",
        "for attack in attacks:\n",
        "  pcap_files = os.listdir(f'/content/{attack}')\n",
        "  for pcap_file in pcap_files:\n",
        "    file_name = pcap_file.split('.')[0]\n",
        "    pcap_file_name = f'{file_name}.pcap'\n",
        "    pcap_file_dir = f'/content/{attack}/'\n",
        "\n",
        "    csv_file_name = f'{file_name}.csv'\n",
        "    csv_file_dir = f'/content/{attack}_csv/'\n",
        "\n",
        "    index_file_name = f'{file_name}_index.csv'\n",
        "    index_file_dir = f'/content/{attack}_index/'\n",
        "\n",
        "    simulated_flows_df,total_packets = extract_flows_from_pcap(pcap_file_dir+pcap_file_name)\n",
        "\n",
        "    # Check if the dataframe is not empty before adding the column and saving\n",
        "    if not simulated_flows_df.empty:\n",
        "      simulated_flows_df['Total_Packets'] = total_packets\n",
        "      simulated_flows_df.to_csv(index_file_dir+index_file_name, index=False)\n",
        "    else:\n",
        "      print(f\"No flows extracted from {pcap_file_name}. Skipping CSV creation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsXX-DxpSOpf",
        "outputId": "a6c11011-f8be-4dd5-a85a-b77f65f312f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "CSV in Benign_index\n",
            "--------------------------------\n",
            "normal_12_index.csv\n",
            "normal_13_index.csv\n",
            "normal_10_index.csv\n",
            "normal_1_index.csv\n",
            "normal_11_index.csv\n",
            "normal_2_index.csv\n"
          ]
        }
      ],
      "source": [
        "# list all index.csv files\n",
        "\n",
        "for attack in attacks:\n",
        "  print('--------------------------------')\n",
        "  print(f\"CSV in {attack}_index\")\n",
        "  print('--------------------------------')\n",
        "  attack_indexs = os.listdir(f'/content/{attack}_index')\n",
        "  for attack_index in attack_indexs:\n",
        "    print(attack_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "dXbxMCsERvrN"
      },
      "outputs": [],
      "source": [
        "# define function for combining simulated and original flow\n",
        "\n",
        "import pandas as pd\n",
        "import ipaddress # For robust IP address comparison\n",
        "\n",
        "# --- Start: Helper Functions (from previous scripts, for self-contained use) ---\n",
        "\n",
        "def generate_flow_key(packet_components):\n",
        "    \"\"\"\n",
        "    Generates a unique key for a flow based on the 5-tuple,\n",
        "    mimicking CICFlowMeter's Java BasicPacketInfo.generateFlowId() logic\n",
        "    for canonicalizing IP addresses and ports.\n",
        "\n",
        "    This function uses the raw protocol number, matching BasicPacketInfo.java.\n",
        "    Any mapping of non-TCP/UDP/ICMP protocols to '0' happens *after* Flow ID generation\n",
        "    in CICFlowMeter's pipeline (e.g., for display in FlowFeature.featureValue2String),\n",
        "    and is NOT part of the Flow ID itself.\n",
        "\n",
        "    Args:\n",
        "        packet_components (tuple): A tuple containing (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "    Returns:\n",
        "        tuple: Canonical 5-tuple (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, normalized_protocol_int)\n",
        "    \"\"\"\n",
        "    src_ip_str, dst_ip_str, src_port, dst_port, protocol_int = packet_components\n",
        "\n",
        "    # Use ipaddress for robust IP comparison, mirroring Java's byte-by-byte comparison\n",
        "    try:\n",
        "        src_ip_obj = ipaddress.ip_address(src_ip_str)\n",
        "        dst_ip_obj = ipaddress.ip_address(dst_ip_str)\n",
        "    except ValueError:\n",
        "        # Fallback for invalid IPs if any, should not happen with valid PCAP data\n",
        "        return (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "\n",
        "\n",
        "    # Determine 'forward' based on IP comparison: canonical_src_ip will be the \"smaller\" IP\n",
        "    if src_ip_obj < dst_ip_obj:\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "    elif dst_ip_obj < src_ip_obj:\n",
        "        # Swap IPs and their corresponding ports for normalization\n",
        "        normalized_src_ip = dst_ip_str\n",
        "        normalized_dst_ip = src_ip_str\n",
        "        normalized_src_port = dst_port\n",
        "        normalized_dst_port = src_port\n",
        "    else: # IPs are equal (e.g., multicast or broadcast)\n",
        "        # If IPs are the same, Java's logic does NOT swap ports based on IP.\n",
        "        # It keeps original src/dst IPs and ports as they are.\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "\n",
        "    # The canonical 5-tuple key for the hash map\n",
        "    return (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, protocol_int)\n",
        "\n",
        "def parse_flow_id_string(flow_id_str):\n",
        "    \"\"\"\n",
        "    Parses a Flow ID string (e.g., 'IP1-IP2-Port1-Port2-Protocol') into its components.\n",
        "    Returns a tuple (src_ip, dst_ip, src_port, dst_port, protocol_int) or None if parsing fails.\n",
        "    \"\"\"\n",
        "    parts = flow_id_str.split('-')\n",
        "    if len(parts) == 5:\n",
        "        try:\n",
        "            return (parts[0], parts[1], int(parts[2]), int(parts[3]), int(parts[4]))\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def read_flows_to_dataframe(filepath: str, is_simulated_output: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads flow data from a CSV file into a Pandas DataFrame.\n",
        "    Adds a 'Canonical_Flow_ID' column for merging.\n",
        "    Parses 'Packet Indices' and 'Packet Timestamps' for simulated output.\n",
        "    This function keeps all original rows and does not deduplicate based on Canonical Flow ID.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file.\n",
        "        is_simulated_output (bool): True if reading our simulated output (with 'Packet Indices' column),\n",
        "                                    False if reading original CICFlowMeter output.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded DataFrame with an added 'Canonical_Flow_ID' column.\n",
        "                      Returns an empty DataFrame if the file is not found or an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Create a new column with parsed components for canonicalization\n",
        "        df['Parsed_Flow_Components'] = df['Flow ID'].apply(parse_flow_id_string)\n",
        "\n",
        "        # Filter out rows where parsing failed\n",
        "        df = df.dropna(subset=['Parsed_Flow_Components'])\n",
        "\n",
        "        # Apply canonicalization to create 'Canonical_Flow_ID'\n",
        "        df['Canonical_Flow_ID'] = df['Parsed_Flow_Components'].apply(generate_flow_key).apply(lambda x: \"-\".join(map(str, x)))\n",
        "\n",
        "        # Clean up temporary column\n",
        "        df = df.drop(columns=['Parsed_Flow_Components'])\n",
        "\n",
        "        total_packets = 0\n",
        "        # Special handling for our simulated output's 'Packet Indices' and 'Packet Timestamps'\n",
        "        if is_simulated_output:\n",
        "            if 'Total_Packets' in df.columns:\n",
        "                total_packets = df['Total_Packets'].iloc[0]\n",
        "            if 'Packet Indices' in df.columns:\n",
        "                try:\n",
        "                    df['Packet Indices'] = df['Packet Indices'].apply(eval)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not parse 'Packet Indices' in {filepath}: {e}\")\n",
        "                    df['Packet Indices'] = [[]] * len(df) # Assign empty list on error\n",
        "            if 'Packet Timestamps' in df.columns:\n",
        "                try:\n",
        "                    df['Packet Timestamps'] = df['Packet Timestamps'].apply(eval)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not parse 'Packet Timestamps' in {filepath}: {e}\")\n",
        "                    df['Packet Timestamps'] = [[]] * len(df) # Assign empty list on error\n",
        "\n",
        "        print(f\"Successfully loaded {len(df)} flows from '{filepath}'.\")\n",
        "        return df,total_packets\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{filepath}'\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing CSV file '{filepath}': {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "# --- End: Helper Functions ---\n",
        "\n",
        "def merge_flows_and_return_dataframe(simulated_df: pd.DataFrame, original_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges a DataFrame of simulated flows with a DataFrame of original CICFlowMeter flows\n",
        "    based on a canonical flow ID. It adds the 'Simulated Packet Indices' column to the\n",
        "    original flow data where a match is found.\n",
        "\n",
        "    Args:\n",
        "        simulated_df (pd.DataFrame): DataFrame containing flows generated by the simulation.\n",
        "                                     Expected to have 'Canonical_Flow_ID' and 'Packet Indices'.\n",
        "        original_df (pd.DataFrame): DataFrame containing flows from the original CICFlowMeter.\n",
        "                                    Expected to have 'Canonical_Flow_ID' and original flow features.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A merged DataFrame containing original CICFlowMeter features\n",
        "                      and 'Simulated Packet Indices' for matching flows.\n",
        "                      Returns an empty DataFrame if inputs are invalid.\n",
        "    \"\"\"\n",
        "    if simulated_df.empty or original_df.empty:\n",
        "        print(\"Cannot merge flows due to empty input DataFrames.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Perform a left merge on the 'Canonical_Flow_ID'\n",
        "    # Keep all rows from original_df, and add matching data from simulated_df.\n",
        "    # Select only 'Canonical_Flow_ID' and 'Packet Indices' from the simulated_df.\n",
        "    merged_df = pd.merge(\n",
        "        original_df,\n",
        "        simulated_df[['Canonical_Flow_ID', 'Packet Indices']],\n",
        "        on='Canonical_Flow_ID',\n",
        "        how='left',\n",
        "        suffixes=('_original', '_simulated')\n",
        "    )\n",
        "\n",
        "    # Rename the new column for clarity\n",
        "    merged_df = merged_df.rename(columns={\n",
        "        'Packet Indices': 'Simulated Packet Indices'\n",
        "    })\n",
        "\n",
        "    # Drop the 'Canonical_Flow_ID' column, as it's only for merging\n",
        "    merged_df = merged_df.drop(columns=['Canonical_Flow_ID'])\n",
        "\n",
        "    print(\"\\n--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\")\n",
        "    print(f\"Total rows in merged DataFrame: {len(merged_df)}\")\n",
        "    print(\"Head of the merged DataFrame:\")\n",
        "    print(merged_df.head())\n",
        "\n",
        "    print(\"\\nColumns in the merged DataFrame:\")\n",
        "    print(merged_df.columns.tolist())\n",
        "\n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWWktpjBVgnn",
        "outputId": "8b650294-4948-43c4-fc69-2e065da88ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 4878 flows from '/content/Benign_index//normal_2_index.csv'.\n",
            "Successfully loaded 29 flows from '/content/Benign_csv//normal_2_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 29\n",
            "Head of the merged DataFrame:\n",
            "                               Flow ID           Src IP  Src Port  \\\n",
            "0  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "1  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "2  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "3     192.168.1.152-41.210.176.2-0-0-0     41.210.176.2         0   \n",
            "4              0.0.0.0-0.0.96.31-0-0-0          0.0.0.0         0   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
            "0  1.194.192.168         0         0  02/04/2019 06:37:41      115223628   \n",
            "1  1.194.192.168         0         0  02/04/2019 06:41:35       31309414   \n",
            "2  1.194.192.168         0         0  02/04/2019 06:46:52        9125475   \n",
            "3  192.168.1.152         0         0  02/04/2019 06:36:52       45007888   \n",
            "4      0.0.96.31         0         0  02/04/2019 06:51:27       10805388   \n",
            "\n",
            "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  \\\n",
            "0             2             0  ...          0.0         0.0         0.0   \n",
            "1             2             0  ...          0.0         0.0         0.0   \n",
            "2             2             0  ...          0.0         0.0         0.0   \n",
            "3             2             0  ...          0.0         0.0         0.0   \n",
            "4             2             0  ...          0.0         0.0         0.0   \n",
            "\n",
            "   Active Min    Idle Mean  Idle Std     Idle Max     Idle Min     Label  \\\n",
            "0         0.0  115223628.0       0.0  115223628.0  115223628.0  No Label   \n",
            "1         0.0   31309414.0       0.0   31309414.0   31309414.0  No Label   \n",
            "2         0.0    9125475.0       0.0    9125475.0    9125475.0  No Label   \n",
            "3         0.0   45007888.0       0.0   45007888.0   45007888.0  No Label   \n",
            "4         0.0   10805388.0       0.0   10805388.0   10805388.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "3                       NaN  \n",
            "4                       NaN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n",
            "Successfully loaded 3584 flows from '/content/Benign_index//normal_10_index.csv'.\n",
            "Successfully loaded 3 flows from '/content/Benign_csv//normal_10_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 3\n",
            "Head of the merged DataFrame:\n",
            "                             Flow ID         Src IP  Src Port         Dst IP  \\\n",
            "0    177.30.87.144-192.168.1.1-0-0-0  177.30.87.144         0    192.168.1.1   \n",
            "1   192.168.1.152-41.210.176.2-0-0-0   41.210.176.2         0  192.168.1.152   \n",
            "2  150.212.3.122-49.24.192.168-0-0-0  150.212.3.122         0  49.24.192.168   \n",
            "\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0         0         0  03/04/2019 08:44:50       35125981             2   \n",
            "1         0         0  03/04/2019 11:30:19       25022679             2   \n",
            "2         0         0  03/04/2019 08:45:55        6375455             2   \n",
            "\n",
            "   Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  Active Min  \\\n",
            "0             0  ...            0           0           0           0   \n",
            "1             0  ...            0           0           0           0   \n",
            "2             0  ...            0           0           0           0   \n",
            "\n",
            "    Idle Mean  Idle Std    Idle Max    Idle Min     Label  \\\n",
            "0  35125981.0       0.0  35125981.0  35125981.0  No Label   \n",
            "1  25022679.0       0.0  25022679.0  25022679.0  No Label   \n",
            "2   6375455.0       0.0   6375455.0   6375455.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "\n",
            "[3 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n",
            "Successfully loaded 6773 flows from '/content/Benign_index//normal_13_index.csv'.\n",
            "Successfully loaded 25 flows from '/content/Benign_csv//normal_13_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 25\n",
            "Head of the merged DataFrame:\n",
            "                               Flow ID           Src IP  Src Port  \\\n",
            "0  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "1     192.168.1.152-41.210.176.2-0-0-0     41.210.176.2         0   \n",
            "2            0.0.166.145-0.0.0.0-0-0-0          0.0.0.0         0   \n",
            "3  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "4  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
            "0  1.194.192.168         0         0  03/04/2019 06:58:37      109826868   \n",
            "1  192.168.1.152         0         0  03/04/2019 07:11:14       39023193   \n",
            "2    0.0.166.145         0         0  03/04/2019 07:14:04       90115331   \n",
            "3  1.194.192.168         0         0  03/04/2019 07:23:19       54838704   \n",
            "4  1.194.192.168         0         0  03/04/2019 07:25:46       78130877   \n",
            "\n",
            "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  \\\n",
            "0             2             0  ...          0.0         0.0         0.0   \n",
            "1             2             0  ...          0.0         0.0         0.0   \n",
            "2             3             0  ...          0.0         0.0         0.0   \n",
            "3             2             0  ...          0.0         0.0         0.0   \n",
            "4             2             0  ...          0.0         0.0         0.0   \n",
            "\n",
            "   Active Min    Idle Mean      Idle Std     Idle Max     Idle Min     Label  \\\n",
            "0         0.0  109826868.0  0.000000e+00  109826868.0  109826868.0  No Label   \n",
            "1         0.0   39023193.0  0.000000e+00   39023193.0   39023193.0  No Label   \n",
            "2         0.0   45057665.5  1.174327e+07   53361408.0   36753923.0  No Label   \n",
            "3         0.0   54838704.0  0.000000e+00   54838704.0   54838704.0  No Label   \n",
            "4         0.0   78130877.0  0.000000e+00   78130877.0   78130877.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "3                       NaN  \n",
            "4                       NaN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n",
            "Successfully loaded 4654 flows from '/content/Benign_index//normal_12_index.csv'.\n",
            "Successfully loaded 4 flows from '/content/Benign_csv//normal_12_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 4\n",
            "Head of the merged DataFrame:\n",
            "                              Flow ID          Src IP  Src Port  \\\n",
            "0           0.0.166.145-0.0.0.0-0-0-0         0.0.0.0         0   \n",
            "1     177.30.87.144-192.168.1.1-0-0-0   177.30.87.144         0   \n",
            "2              0.0.0.0-0.0.2.12-0-0-0         0.0.0.0         0   \n",
            "3  1.152.192.168-25.211.192.168-0-0-0  25.211.192.168         0   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
            "0    0.0.166.145         0         0  04/04/2019 04:27:42       82024523   \n",
            "1    192.168.1.1         0         0  04/04/2019 03:21:14           4280   \n",
            "2       0.0.2.12         0         0  04/04/2019 03:57:13       67172076   \n",
            "3  1.152.192.168         0         0  04/04/2019 04:45:18       92435862   \n",
            "\n",
            "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  \\\n",
            "0             2             0  ...            0           0           0   \n",
            "1             2             0  ...            0           0           0   \n",
            "2             2             0  ...            0           0           0   \n",
            "3             2             0  ...            0           0           0   \n",
            "\n",
            "   Active Min   Idle Mean  Idle Std    Idle Max    Idle Min     Label  \\\n",
            "0           0  82024523.0       0.0  82024523.0  82024523.0  No Label   \n",
            "1           0         0.0       0.0         0.0         0.0  No Label   \n",
            "2           0  67172076.0       0.0  67172076.0  67172076.0  No Label   \n",
            "3           0  92435862.0       0.0  92435862.0  92435862.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "3                       NaN  \n",
            "\n",
            "[4 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n",
            "Successfully loaded 4789 flows from '/content/Benign_index//normal_1_index.csv'.\n",
            "Successfully loaded 38 flows from '/content/Benign_csv//normal_1_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 38\n",
            "Head of the merged DataFrame:\n",
            "                               Flow ID           Src IP  Src Port  \\\n",
            "0            0.0.166.145-0.0.0.0-0-0-0          0.0.0.0         0   \n",
            "1            0.0.166.145-0.0.0.0-0-0-0          0.0.0.0         0   \n",
            "2  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "3      177.30.87.144-192.168.1.1-0-0-0    177.30.87.144         0   \n",
            "4  181.216.192.168-1.194.192.168-0-0-0  181.216.192.168         0   \n",
            "\n",
            "          Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
            "0    0.0.166.145         0         0  02/04/2019 04:04:06       25297655   \n",
            "1    0.0.166.145         0         0  02/04/2019 04:06:44       48008225   \n",
            "2  1.194.192.168         0         0  02/04/2019 04:13:19       50599600   \n",
            "3    192.168.1.1         0         0  02/04/2019 04:09:20      106888701   \n",
            "4  1.194.192.168         0         0  02/04/2019 04:15:41      113176849   \n",
            "\n",
            "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  \\\n",
            "0             2             0  ...          0.0         0.0         0.0   \n",
            "1             2             0  ...          0.0         0.0         0.0   \n",
            "2             3             0  ...    3071797.0         0.0   3071797.0   \n",
            "3             4             0  ...          0.0         0.0         0.0   \n",
            "4             2             0  ...          0.0         0.0         0.0   \n",
            "\n",
            "   Active Min    Idle Mean      Idle Std     Idle Max     Idle Min     Label  \\\n",
            "0         0.0   25297655.0  0.000000e+00   25297655.0   25297655.0  No Label   \n",
            "1         0.0   48008225.0  0.000000e+00   48008225.0   48008225.0  No Label   \n",
            "2   3071797.0   47527803.0  0.000000e+00   47527803.0   47527803.0  No Label   \n",
            "3         0.0   35629567.0  9.918701e+06   45027280.0   25261242.0  No Label   \n",
            "4         0.0  113176849.0  0.000000e+00  113176849.0  113176849.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "3                       NaN  \n",
            "4                       NaN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n",
            "Successfully loaded 3503 flows from '/content/Benign_index//normal_11_index.csv'.\n",
            "Successfully loaded 3 flows from '/content/Benign_csv//normal_11_ISCX.csv'.\n",
            "\n",
            "--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\n",
            "Total rows in merged DataFrame: 3\n",
            "Head of the merged DataFrame:\n",
            "                             Flow ID           Src IP  Src Port       Dst IP  \\\n",
            "0          0.0.166.145-0.0.0.0-0-0-0          0.0.0.0         0  0.0.166.145   \n",
            "1  213.240.192.168-1.152.3.122-0-0-0  213.240.192.168         0  1.152.3.122   \n",
            "2   1.152.3.122-19.208.192.168-0-0-0   19.208.192.168         0  1.152.3.122   \n",
            "\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0         0         0  04/04/2019 02:50:52       38860995             2   \n",
            "1         0         0  04/04/2019 01:32:18       52002269             2   \n",
            "2         0         0  04/04/2019 12:22:48       30349464             2   \n",
            "\n",
            "   Tot Bwd Pkts  ...  Active Mean  Active Std  Active Max  Active Min  \\\n",
            "0             0  ...            0           0           0           0   \n",
            "1             0  ...            0           0           0           0   \n",
            "2             0  ...            0           0           0           0   \n",
            "\n",
            "    Idle Mean  Idle Std    Idle Max    Idle Min     Label  \\\n",
            "0  38860995.0       0.0  38860995.0  38860995.0  No Label   \n",
            "1  52002269.0       0.0  52002269.0  52002269.0  No Label   \n",
            "2  30349464.0       0.0  30349464.0  30349464.0  No Label   \n",
            "\n",
            "   Simulated Packet Indices  \n",
            "0                       NaN  \n",
            "1                       NaN  \n",
            "2                       NaN  \n",
            "\n",
            "[3 rows x 85 columns]\n",
            "\n",
            "Columns in the merged DataFrame:\n",
            "['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Simulated Packet Indices']\n"
          ]
        }
      ],
      "source": [
        "# combine simulate and original flow , for each flow\n",
        "\n",
        "for attack in attacks:\n",
        "  pcap_files = os.listdir(f'/content/{attack}')\n",
        "  for pcap_file in pcap_files:\n",
        "    file_name = pcap_file.split('.')[0]\n",
        "    pcap_file_name = f'{file_name}.pcap'\n",
        "    pcap_file_dir = f'/content/{attack}/'\n",
        "\n",
        "    csv_file_name = f'{file_name}_ISCX.csv'\n",
        "    csv_file_dir = f'/content/{attack}_csv/'\n",
        "\n",
        "    index_file_name = f'{file_name}_index.csv'\n",
        "    index_file_dir = f'/content/{attack}_index/'\n",
        "\n",
        "    simulated_csv_path = f'{index_file_dir}/{index_file_name}'\n",
        "    original_cic_csv_path = f'{csv_file_dir}/{csv_file_name}'\n",
        "\n",
        "    # Read the data from both CSVs into DataFrames using the helper function\n",
        "    simulated_df,total_packets = read_flows_to_dataframe(simulated_csv_path, is_simulated_output=True)\n",
        "    original_df,placeholder = read_flows_to_dataframe(original_cic_csv_path, is_simulated_output=False)\n",
        "\n",
        "    # Merge the flows and get the resulting DataFrame\n",
        "    final_merged_df = merge_flows_and_return_dataframe(simulated_df, original_df)\n",
        "    final_merged_df['Total_Packets'] = total_packets\n",
        "    final_merged_df.to_csv(f'{index_file_dir}/{index_file_name}', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUsuVyYqX6Ei"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "STzSwfrBX8Y1"
      },
      "outputs": [],
      "source": [
        "# define function for dataframe preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "def map_port(port):\n",
        "    if port == 21:\n",
        "        return 1  # FTP\n",
        "    elif port == 22:\n",
        "        return 2  # SSH\n",
        "    elif port == 53:\n",
        "        return 3  # DNS\n",
        "    elif port == 80:\n",
        "        return 4  # HTTP\n",
        "    elif port == 443:\n",
        "        return 5  # HTTPS\n",
        "    else:\n",
        "        return 6  # Other\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    original_indices = set(df.index)\n",
        "\n",
        "    # replace space in columns name with underscore\n",
        "    df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "    # drop objects type columns\n",
        "    columns_to_drop = [\n",
        "        'Flow_ID','Src_IP','Dst_IP','Src_Port','Protocol','Timestamp','Label'\n",
        "    ]\n",
        "\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # remove rows with missing and infinite values\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # map destination port to 1-6 numbers\n",
        "    df['Dst_Port'] = df['Dst_Port'].apply(map_port)\n",
        "\n",
        "    return df\n",
        "\n",
        "# preprocess_dataframe(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "35mrjNeMYBCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e380a69-ab8c-4cbf-f7b1-d8a51baca602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normal_2_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_2_index.csv. Skipping prediction.\n",
            "normal_10_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_10_index.csv. Skipping prediction.\n",
            "normal_13_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_13_index.csv. Skipping prediction.\n",
            "normal_12_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_12_index.csv. Skipping prediction.\n",
            "normal_1_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_1_index.csv. Skipping prediction.\n",
            "normal_11_index.csv\n",
            "Warning: DataFrame is empty after preprocessing for normal_11_index.csv. Skipping prediction.\n"
          ]
        }
      ],
      "source": [
        "# get prediction for each packet\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def choose_label(labels):\n",
        "    if all(label == 'Benign' for label in labels):\n",
        "        return 'Benign'\n",
        "\n",
        "    # Count non-Benign labels\n",
        "    non_benign_labels = [label for label in labels if label != 'Benign']\n",
        "    if not non_benign_labels: # Handle case where only 'Benign' labels are present after filtering\n",
        "        return 'Benign'\n",
        "    label_counts = Counter(non_benign_labels)\n",
        "\n",
        "    # Return the most common non-Benign label\n",
        "    most_common_label, _ = label_counts.most_common(1)[0]\n",
        "    return most_common_label\n",
        "\n",
        "for attack in attacks:\n",
        "  pcap_files = os.listdir(f'/content/{attack}')\n",
        "  for pcap_file in pcap_files:\n",
        "    file_name = pcap_file.split('.')[0]\n",
        "    pcap_file_name = f'{file_name}.pcap'\n",
        "    pcap_file_dir = f'/content/{attack}/'\n",
        "\n",
        "    csv_file_name = f'{file_name}_ISCX.csv'\n",
        "    csv_file_dir = f'/content/{attack}_csv/'\n",
        "\n",
        "    index_file_name = f'{file_name}_index.csv'\n",
        "    index_file_dir = f'/content/{attack}_index/'\n",
        "\n",
        "    prediction_file_name = f'{file_name}_prediction.csv'\n",
        "    prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "\n",
        "    df = pd.read_csv(f'{index_file_dir}/{index_file_name}')\n",
        "\n",
        "    # Get total_packets before preprocessing, as preprocessing might remove rows\n",
        "    if 'Total_Packets' in df.columns and not df.empty:\n",
        "      total_packets = df['Total_Packets'].iloc[0]\n",
        "    else:\n",
        "      print(f\"Warning: 'Total_Packets' column not found or DataFrame is empty for {index_file_name}. Skipping.\")\n",
        "      continue # Skip this file if no total_packets\n",
        "\n",
        "    df = preprocess_dataframe(df)\n",
        "    print(index_file_name)\n",
        "\n",
        "    # Check if DataFrame is empty after preprocessing\n",
        "    if df.empty:\n",
        "        print(f\"Warning: DataFrame is empty after preprocessing for {index_file_name}. Skipping prediction.\")\n",
        "        continue\n",
        "\n",
        "    # Drop 'Total_Packets' after getting its value\n",
        "    if 'Total_Packets' in df.columns:\n",
        "        df = df.drop(columns=['Total_Packets'])\n",
        "\n",
        "    df_packet_index = df['Simulated_Packet_Indices']\n",
        "    df_packet_index = df_packet_index.apply(ast.literal_eval)\n",
        "\n",
        "    df = df.drop(columns=['Simulated_Packet_Indices'])\n",
        "    df_prediction = model.predict(df)\n",
        "\n",
        "    df_prediction = pd.DataFrame({\n",
        "    'Packet_Indices': df_packet_index,\n",
        "    'Label': df_prediction\n",
        "    })\n",
        "\n",
        "    df_prediction = df_prediction.explode('Packet_Indices').reset_index(drop=True)\n",
        "\n",
        "    # after edit\n",
        "    df_prediction = df_prediction.groupby('Packet_Indices')['Label'].apply(list).reset_index()\n",
        "    df_prediction['Chosen_Label'] = df_prediction['Label'].apply(choose_label)\n",
        "\n",
        "    full_indices = set(range(1, total_packets + 1))\n",
        "    existing_indices = set(df_prediction['Packet_Indices'])\n",
        "    missing_indices = sorted(full_indices - existing_indices)\n",
        "\n",
        "    df_missing = pd.DataFrame({\n",
        "    'Packet_Indices': missing_indices,\n",
        "    'Label': [['Benign']] * len(missing_indices),\n",
        "    'Chosen_Label': ['Benign'] * len(missing_indices),\n",
        "    })\n",
        "\n",
        "    df_prediction = pd.concat([df_prediction, df_missing], ignore_index=True)\n",
        "\n",
        "    df_prediction.sort_values(by='Packet_Indices', inplace=True)\n",
        "    df_prediction.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    os.makedirs(prediction_file_dir, exist_ok=True)\n",
        "    df_prediction.to_csv(f'{prediction_file_dir}/{prediction_file_name}', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vdVopIgJsgyF",
        "outputId": "a58c3e48-4365-4086-feaa-c4a4c61d0d76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Benign_prediction//normal_2_prediction.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-df1e608b7f9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprediction_file_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/{attack}_prediction/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{prediction_file_dir}/{prediction_file_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{pcap_file_name} = {df['Chosen_Label'].value_counts()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Benign_prediction//normal_2_prediction.csv'"
          ]
        }
      ],
      "source": [
        "# just seeing choosen_label value counts\n",
        "\n",
        "for attack in attacks:\n",
        "  pcap_files = os.listdir(f'/content/{attack}')\n",
        "  for pcap_file in pcap_files:\n",
        "    file_name = pcap_file.split('.')[0]\n",
        "    pcap_file_name = f'{file_name}.pcap'\n",
        "    pcap_file_dir = f'/content/{attack}/'\n",
        "\n",
        "    csv_file_name = f'{file_name}_ISCX.csv'\n",
        "    csv_file_dir = f'/content/{attack}_csv/'\n",
        "\n",
        "    index_file_name = f'{file_name}_index.csv'\n",
        "    index_file_dir = f'/content/{attack}_index/'\n",
        "\n",
        "    prediction_file_name = f'{file_name}_prediction.csv'\n",
        "    prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "\n",
        "    df = pd.read_csv(f'{prediction_file_dir}/{prediction_file_name}')\n",
        "    print(f\"{pcap_file_name} = {df['Chosen_Label'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoIDleSovPmF"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/Result', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaZzbHqktyYx"
      },
      "outputs": [],
      "source": [
        "# get the pcap label with most label packets and save to result\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_dominant_label(label_series):\n",
        "    counts = label_series.value_counts()\n",
        "    if len(counts) == 1 and 'Benign' in counts:\n",
        "        return 'Benign'\n",
        "    counts = counts.drop('Benign', errors='ignore')\n",
        "    return counts.idxmax() if not counts.empty else 'Benign'\n",
        "\n",
        "\n",
        "\n",
        "for attack in attacks:\n",
        "    results = []\n",
        "    pcap_files = os.listdir(f'/content/{attack}')\n",
        "    for pcap_file in pcap_files:\n",
        "        file_name = pcap_file.split('.')[0]\n",
        "        pcap_file_name = f'{file_name}.pcap'\n",
        "\n",
        "        prediction_file_name = f'{file_name}_prediction.csv'\n",
        "        prediction_file_dir = f'/content/{attack}_prediction'\n",
        "        prediction_csv_path = os.path.join(prediction_file_dir, prediction_file_name)\n",
        "\n",
        "        if not os.path.isfile(prediction_csv_path):\n",
        "            continue  # Skip missing prediction files\n",
        "\n",
        "        df = pd.read_csv(prediction_csv_path)\n",
        "        final_pcap_label = get_dominant_label(df['Chosen_Label'])\n",
        "\n",
        "        results.append({\n",
        "            'File_Name': pcap_file_name,\n",
        "            'Result': final_pcap_label,\n",
        "            'Attack': attack\n",
        "        })\n",
        "\n",
        "    df_all_results = pd.DataFrame(results)\n",
        "    df_all_results.to_csv(f'/content/Result/{attack}_result.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRqtIpYEwImD"
      },
      "outputs": [],
      "source": [
        "# prompt: generated sklearn classification result of each attack_result with y_test being Attack column and y_pred being Result column\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for attack in attacks:\n",
        "  result_csv_path = f'/content/Result/{attack}_result.csv'\n",
        "  if os.path.exists(result_csv_path):\n",
        "    df_result = pd.read_csv(result_csv_path)\n",
        "\n",
        "    y_true = df_result['Attack']\n",
        "    y_pred = df_result['Result']\n",
        "\n",
        "    print(f\"Classification Report for {attack}:\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0)) # Use zero_division=0 to handle cases where a class has no predicted samples\n",
        "\n",
        "  else:\n",
        "    print(f\"No result file found for {attack}.\")\n",
        "  print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCVJt-YJtA3s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "\n",
        "for attack in attacks:\n",
        "    pcap_files = os.listdir(f'/content/{attack}')\n",
        "    for pcap_file in pcap_files:\n",
        "        file_name = pcap_file.split('.')[0]\n",
        "        prediction_file_name = f'{file_name}_prediction.csv'\n",
        "        prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "        prediction_csv_path = f'{prediction_file_dir}/{prediction_file_name}'\n",
        "\n",
        "        try:\n",
        "            df_prediction = pd.read_csv(prediction_csv_path)\n",
        "            # The true label for all packets in this file is the 'attack' name\n",
        "            if attack != 'DoS-SlowRate':\n",
        "                true_label = attack\n",
        "            else:\n",
        "                true_label = 'DoS-Slow-Rate'\n",
        "\n",
        "            # Collect the predicted label for each packet from 'Chosen_Label'\n",
        "            predicted_labels = df_prediction['Chosen_Label'].tolist()\n",
        "\n",
        "            # Extend our master lists\n",
        "            all_true_labels.extend([true_label] * len(predicted_labels))\n",
        "            all_predicted_labels.extend(predicted_labels)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Prediction file not found for {pcap_file}: {prediction_csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing prediction file {prediction_csv_path}: {e}\")\n",
        "\n",
        "# Now, print the classification report using the collected true and predicted labels\n",
        "if all_true_labels and all_predicted_labels:\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    # Ensure both lists are of the same length\n",
        "    if len(all_true_labels) == len(all_predicted_labels):\n",
        "        print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n",
        "    else:\n",
        "        print(\"Error: Mismatch in the number of true labels and predicted labels.\")\n",
        "else:\n",
        "    print(\"\\nNo data collected to generate a classification report.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M3gBxr13s1W"
      },
      "outputs": [],
      "source": [
        "# prompt: do confusion matrix of the from sklearn.metrics import classification_report\n",
        "# all_true_labels = []\n",
        "# all_predicted_labels = []\n",
        "# for attack in attacks:\n",
        "#     pcap_files = os.listdir(f'/content/{attack}')\n",
        "#     for pcap_file in pcap_files:\n",
        "#         file_name = pcap_file.split('.')[0]\n",
        "#         prediction_file_name = f'{file_name}_prediction.csv'\n",
        "#         prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "#         prediction_csv_path = f'{prediction_file_dir}/{prediction_file_name}'\n",
        "#         try:\n",
        "#             df_prediction = pd.read_csv(prediction_csv_path)\n",
        "#             # The true label for all packets in this file is the 'attack' name\n",
        "#             if attack != 'DoS-SlowRate':\n",
        "#                 true_label = attack\n",
        "#             else:\n",
        "#                 true_label = 'DoS-Slow-Rate'\n",
        "#             # Collect the predicted label for each packet from 'Chosen_Label'\n",
        "#             predicted_labels = df_prediction['Chosen_Label'].tolist()\n",
        "#             # Extend our master lists\n",
        "#             all_true_labels.extend([true_label] * len(predicted_labels))\n",
        "#             all_predicted_labels.extend(predicted_labels)\n",
        "#         except FileNotFoundError:\n",
        "#             print(f\"Prediction file not found for {pcap_file}: {prediction_csv_path}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing prediction file {prediction_csv_path}: {e}\")\n",
        "# # Now, print the classification report using the collected true and predicted labels\n",
        "# if all_true_labels and all_predicted_labels:\n",
        "#     print(\"\\n--- Classification Report ---\")\n",
        "#     # Ensure both lists are of the same length\n",
        "#     if len(all_true_labels) == len(all_predicted_labels):\n",
        "#         print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n",
        "#     else:\n",
        "#         print(\"Error: Mismatch in the number of true labels and predicted labels.\")\n",
        "# else:\n",
        "#     print(\"\\nNo data collected to generate a classification report.\") too\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the confusion matrix\n",
        "# Ensure the unique labels are consistent across true and predicted lists\n",
        "unique_labels = sorted(list(set(all_true_labels + all_predicted_labels)))\n",
        "\n",
        "if all_true_labels and all_predicted_labels and len(all_true_labels) == len(all_predicted_labels):\n",
        "    cm = confusion_matrix(all_true_labels, all_predicted_labels, labels=unique_labels)\n",
        "\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "    print(cm)\n",
        "\n",
        "    # Optional: Visualize the confusion matrix using seaborn and matplotlib\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nCannot generate Confusion Matrix: Mismatch in the number of true labels and predicted labels or no data collected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thSpiNCNb0tR"
      },
      "source": [
        "# Additional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XWcQAeEbSiW"
      },
      "outputs": [],
      "source": [
        "# prompt: print confsion matrix of each pcap file instead possible?\n",
        "# # prompt: do confusion matrix of the from sklearn.metrics import classification_report\n",
        "# # all_true_labels = []\n",
        "# # all_predicted_labels = []\n",
        "# # for attack in attacks:\n",
        "# #     pcap_files = os.listdir(f'/content/{attack}')\n",
        "# #     for pcap_file in pcap_files:\n",
        "# #         file_name = pcap_file.split('.')[0]\n",
        "# #         prediction_file_name = f'{file_name}_prediction.csv'\n",
        "# #         prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "# #         prediction_csv_path = f'{prediction_file_dir}/{prediction_file_name}'\n",
        "# #         try:\n",
        "# #             df_prediction = pd.read_csv(prediction_csv_path)\n",
        "# #             # The true label for all packets in this file is the 'attack' name\n",
        "# #             if attack != 'DoS-SlowRate':\n",
        "# #                 true_label = attack\n",
        "# #             else:\n",
        "# #                 true_label = 'DoS-Slow-Rate'\n",
        "# #             # Collect the predicted label for each packet from 'Chosen_Label'\n",
        "# #             predicted_labels = df_prediction['Chosen_Label'].tolist()\n",
        "# #             # Extend our master lists\n",
        "# #             all_true_labels.extend([true_label] * len(predicted_labels))\n",
        "# #             all_predicted_labels.extend(predicted_labels)\n",
        "# #         except FileNotFoundError:\n",
        "# #             print(f\"Prediction file not found for {pcap_file}: {prediction_csv_path}\")\n",
        "# #         except Exception as e:\n",
        "# #             print(f\"Error processing prediction file {prediction_csv_path}: {e}\")\n",
        "# # # Now, print the classification report using the collected true and predicted labels\n",
        "# # if all_true_labels and all_predicted_labels:\n",
        "# #     print(\"\\n--- Classification Report ---\")\n",
        "# #     # Ensure both lists are of the same length\n",
        "# #     if len(all_true_labels) == len(all_predicted_labels):\n",
        "# #         print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n",
        "# #     else:\n",
        "# #         print(\"Error: Mismatch in the number of true labels and predicted labels.\")\n",
        "# # else:\n",
        "# #     print(\"\\nNo data collected to generate a classification report.\")\n",
        "\n",
        "# Print confusion matrix for each pcap file\n",
        "for attack in attacks:\n",
        "    pcap_files = os.listdir(f'/content/{attack}')\n",
        "    for pcap_file in pcap_files:\n",
        "        file_name = pcap_file.split('.')[0]\n",
        "        prediction_file_name = f'{file_name}_prediction.csv'\n",
        "        prediction_file_dir = f'/content/{attack}_prediction/'\n",
        "        prediction_csv_path = f'{prediction_file_dir}/{prediction_file_name}'\n",
        "\n",
        "        try:\n",
        "            df_prediction = pd.read_csv(prediction_csv_path)\n",
        "\n",
        "            # Determine the true label for this file\n",
        "            if attack != 'DoS-SlowRate':\n",
        "                true_label = attack\n",
        "            else:\n",
        "                true_label = 'DoS-Slow-Rate'\n",
        "\n",
        "            # Extract true and predicted labels for this specific file\n",
        "            true_labels_file = [true_label] * len(df_prediction)\n",
        "            predicted_labels_file = df_prediction['Chosen_Label'].tolist()\n",
        "\n",
        "            print(f\"\\n--- Confusion Matrix for {pcap_file} ---\")\n",
        "\n",
        "            if true_labels_file and predicted_labels_file and len(true_labels_file) == len(predicted_labels_file):\n",
        "                # Get all unique labels present in both true and predicted lists for this file\n",
        "                unique_labels_file = sorted(list(set(true_labels_file + predicted_labels_file)))\n",
        "\n",
        "                # Generate confusion matrix for this file\n",
        "                cm_file = confusion_matrix(true_labels_file, predicted_labels_file, labels=unique_labels_file)\n",
        "                print(cm_file)\n",
        "\n",
        "                # Optional: Visualize the confusion matrix for this file\n",
        "                plt.figure(figsize=(6, 5))\n",
        "                sns.heatmap(cm_file, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels_file, yticklabels=unique_labels_file)\n",
        "                plt.xlabel('Predicted Label')\n",
        "                plt.ylabel('True Label')\n",
        "                plt.title(f'Confusion Matrix for {pcap_file}')\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(f\"No valid data to generate confusion matrix for {pcap_file}.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"\\nPrediction file not found for {pcap_file}: {prediction_csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing prediction file {prediction_csv_path}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK35vH66bBTe"
      },
      "outputs": [],
      "source": [
        "# prompt: print every result as dataframe display\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "for attack in attacks:\n",
        "    result_csv_path = f'/content/Result/{attack}_result.csv'\n",
        "    if os.path.exists(result_csv_path):\n",
        "        df_result = pd.read_csv(result_csv_path)\n",
        "        print(f\"\\n--- Results for {attack} ---\")\n",
        "        print(df_result)\n",
        "    else:\n",
        "        print(f\"\\nNo result file found for {attack}.\")\n",
        "    print(\"-\" * 40)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vpoFaRFw4fqB",
        "ffr1MGrg4gil",
        "thSpiNCNb0tR",
        "h91XlaLdRUbj",
        "cWj8Jc8PaP1a",
        "TQXF-l4UUkLe",
        "TLm6-7pXFFmy",
        "srI7OsUjFAoz",
        "fL2Jdmy58F7C",
        "fFyVqFnOkshn",
        "iyfKM07nOPDu",
        "EZ_mNHJ7mHrt",
        "ZE5Hn1tQntgb",
        "aKPqgcBYThy9",
        "18RZ1HMbugQi",
        "VisBSN1_TlPz"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNfSLhcNOtghJW3bYj2NCPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}