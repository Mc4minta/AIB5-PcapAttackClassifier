{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0FSnKNDecp7i",
        "Fbuw0fiObgaR",
        "2hRtjS45cHCt",
        "NJBqdQjLbJIq"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+hGxcPIGj7qnwdV5aZvBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mc4minta/AIB5-PcapAttackClassifier/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook?\n",
        "- Setup google api key as a secret with the name 'GOOGLE_API_KEY'\n",
        "- click run all cell\n",
        "- see the password of local tunnel (public ip)\n",
        "- click the link generated and enter the public ip as password\n",
        "- Enjoy!\n",
        "\n",
        "## LLM Model\n",
        "- LLM model configuration > GEMINI_MODEL = \"gemini-1.5-flash\" in main.py"
      ],
      "metadata": {
        "id": "TdzZnjVYmYv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies setup"
      ],
      "metadata": {
        "id": "0FSnKNDecp7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnpAv5ejMTbY",
        "outputId": "fd3e5132-cc0c-411c-cb37-9e03ab9f4c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0\n",
            "--2025-06-10 06:57:54--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64 [following]\n",
            "--2025-06-10 06:57:54--  https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250610%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250610T065754Z&X-Amz-Expires=300&X-Amz-Signature=db4b1086d44d419aa27a432dcdad36da0be32fbee3745334c56d38f9cd17939b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-10 06:57:54--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250610%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250610T065754Z&X-Amz-Expires=300&X-Amz-Signature=db4b1086d44d419aa27a432dcdad36da0be32fbee3745334c56d38f9cd17939b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37839075 (36M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared’\n",
            "\n",
            "cloudflared         100%[===================>]  36.09M   228MB/s    in 0.2s    \n",
            "\n",
            "2025-06-10 06:57:54 (228 MB/s) - ‘cloudflared’ saved [37839075/37839075]\n",
            "\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libpcap0.8:amd64.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcap0.8_1.10.1-4ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libpcap0.8:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../libdbus-1-dev_1.12.20-2ubuntu4.1_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Selecting previously unselected package libpcap0.8-dev:amd64.\n",
            "Preparing to unpack .../libpcap0.8-dev_1.10.1-4ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libpcap0.8-dev:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package libpcap-dev:amd64.\n",
            "Preparing to unpack .../libpcap-dev_1.10.1-4ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libpcap-dev:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Setting up libpcap0.8:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Setting up libpcap0.8-dev:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Setting up libpcap-dev:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting scapy\n",
            "  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scapy\n",
            "Successfully installed scapy-2.6.1\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Install Streamlit\n",
        "!pip install streamlit\n",
        "\n",
        "# For Cloudflare host\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "\n",
        "# For CICFlowMeter\n",
        "!apt-get update -qq\n",
        "!apt-get install -y libpcap-dev -qq\n",
        "\n",
        "# For the main program\n",
        "!pip install scapy\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# src/*.py setup"
      ],
      "metadata": {
        "id": "Fbuw0fiObgaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create the 'src' directory if it doesn't exist\n",
        "if not os.path.exists('src'):\n",
        "    os.makedirs('src')"
      ],
      "metadata": {
        "id": "mEMSIvlebzWP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/merge_flow.py\n",
        "# define function for combining simulated and original flow\n",
        "\n",
        "import pandas as pd\n",
        "import ipaddress # For robust IP address comparison\n",
        "\n",
        "# --- Start: Helper Functions (from previous scripts, for self-contained use) ---\n",
        "\n",
        "def generate_flow_key(packet_components):\n",
        "    \"\"\"\n",
        "    Generates a unique key for a flow based on the 5-tuple,\n",
        "    mimicking CICFlowMeter's Java BasicPacketInfo.generateFlowId() logic\n",
        "    for canonicalizing IP addresses and ports.\n",
        "\n",
        "    This function uses the raw protocol number, matching BasicPacketInfo.java.\n",
        "    Any mapping of non-TCP/UDP/ICMP protocols to '0' happens *after* Flow ID generation\n",
        "    in CICFlowMeter's pipeline (e.g., for display in FlowFeature.featureValue2String),\n",
        "    and is NOT part of the Flow ID itself.\n",
        "\n",
        "    Args:\n",
        "        packet_components (tuple): A tuple containing (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "    Returns:\n",
        "        tuple: Canonical 5-tuple (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, normalized_protocol_int)\n",
        "    \"\"\"\n",
        "    src_ip_str, dst_ip_str, src_port, dst_port, protocol_int = packet_components\n",
        "\n",
        "    # Use ipaddress for robust IP comparison, mirroring Java's byte-by-byte comparison\n",
        "    try:\n",
        "        src_ip_obj = ipaddress.ip_address(src_ip_str)\n",
        "        dst_ip_obj = ipaddress.ip_address(dst_ip_str)\n",
        "    except ValueError:\n",
        "        # Fallback for invalid IPs if any, should not happen with valid PCAP data\n",
        "        return (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "\n",
        "\n",
        "    # Determine 'forward' based on IP comparison: canonical_src_ip will be the \"smaller\" IP\n",
        "    if src_ip_obj < dst_ip_obj:\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "    elif dst_ip_obj < src_ip_obj:\n",
        "        # Swap IPs and their corresponding ports for normalization\n",
        "        normalized_src_ip = dst_ip_str\n",
        "        normalized_dst_ip = src_ip_str\n",
        "        normalized_src_port = dst_port\n",
        "        normalized_dst_port = src_port\n",
        "    else: # IPs are equal (e.g., multicast or broadcast)\n",
        "        # If IPs are the same, Java's logic does NOT swap ports based on IP.\n",
        "        # It keeps original src/dst IPs and ports as they are.\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "\n",
        "    # The canonical 5-tuple key for the hash map\n",
        "    return (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, protocol_int)\n",
        "\n",
        "def parse_flow_id_string(flow_id_str):\n",
        "    \"\"\"\n",
        "    Parses a Flow ID string (e.g., 'IP1-IP2-Port1-Port2-Protocol') into its components.\n",
        "    Returns a tuple (src_ip, dst_ip, src_port, dst_port, protocol_int) or None if parsing fails.\n",
        "    \"\"\"\n",
        "    parts = flow_id_str.split('-')\n",
        "    if len(parts) == 5:\n",
        "        try:\n",
        "            return (parts[0], parts[1], int(parts[2]), int(parts[3]), int(parts[4]))\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def read_flows_to_dataframe(filepath: str, is_simulated_output: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads flow data from a CSV file into a Pandas DataFrame.\n",
        "    Adds a 'Canonical_Flow_ID' column for merging.\n",
        "    Parses 'Packet Indices' and 'Packet Timestamps' for simulated output.\n",
        "    This function keeps all original rows and does not deduplicate based on Canonical Flow ID.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file.\n",
        "        is_simulated_output (bool): True if reading our simulated output (with 'Packet Indices' column),\n",
        "                                    False if reading original CICFlowMeter output.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded DataFrame with an added 'Canonical_Flow_ID' column.\n",
        "                      Returns an empty DataFrame if the file is not found or an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Create a new column with parsed components for canonicalization\n",
        "        df['Parsed_Flow_Components'] = df['Flow ID'].apply(parse_flow_id_string)\n",
        "\n",
        "        # Filter out rows where parsing failed\n",
        "        df = df.dropna(subset=['Parsed_Flow_Components'])\n",
        "\n",
        "        # Apply canonicalization to create 'Canonical_Flow_ID'\n",
        "        df['Canonical_Flow_ID'] = df['Parsed_Flow_Components'].apply(generate_flow_key).apply(lambda x: \"-\".join(map(str, x)))\n",
        "\n",
        "        # Clean up temporary column\n",
        "        df = df.drop(columns=['Parsed_Flow_Components'])\n",
        "\n",
        "        total_packets = 0\n",
        "        # Special handling for our simulated output's 'Packet Indices' and 'Packet Timestamps'\n",
        "        if is_simulated_output:\n",
        "            if 'Total_Packets' in df.columns:\n",
        "                total_packets = df['Total_Packets'].iloc[0]\n",
        "            if 'Packet Indices' in df.columns:\n",
        "                try:\n",
        "                    df['Packet Indices'] = df['Packet Indices'].apply(eval)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not parse 'Packet Indices' in {filepath}: {e}\")\n",
        "                    df['Packet Indices'] = [[]] * len(df) # Assign empty list on error\n",
        "            if 'Packet Timestamps' in df.columns:\n",
        "                try:\n",
        "                    df['Packet Timestamps'] = df['Packet Timestamps'].apply(eval)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not parse 'Packet Timestamps' in {filepath}: {e}\")\n",
        "                    df['Packet Timestamps'] = [[]] * len(df) # Assign empty list on error\n",
        "\n",
        "        print(f\"Successfully loaded {len(df)} flows from '{filepath}'.\")\n",
        "        return df,total_packets\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{filepath}'\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing CSV file '{filepath}': {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "# --- End: Helper Functions ---\n",
        "\n",
        "def merge_flows_and_return_dataframe(simulated_df: pd.DataFrame, original_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges a DataFrame of simulated flows with a DataFrame of original CICFlowMeter flows\n",
        "    based on a canonical flow ID. It adds the 'Simulated Packet Indices' column to the\n",
        "    original flow data where a match is found.\n",
        "\n",
        "    Args:\n",
        "        simulated_df (pd.DataFrame): DataFrame containing flows generated by the simulation.\n",
        "                                     Expected to have 'Canonical_Flow_ID' and 'Packet Indices'.\n",
        "        original_df (pd.DataFrame): DataFrame containing flows from the original CICFlowMeter.\n",
        "                                    Expected to have 'Canonical_Flow_ID' and original flow features.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A merged DataFrame containing original CICFlowMeter features\n",
        "                      and 'Simulated Packet Indices' for matching flows.\n",
        "                      Returns an empty DataFrame if inputs are invalid.\n",
        "    \"\"\"\n",
        "    if simulated_df.empty or original_df.empty:\n",
        "        print(\"Cannot merge flows due to empty input DataFrames.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Perform a left merge on the 'Canonical_Flow_ID'\n",
        "    # Keep all rows from original_df, and add matching data from simulated_df.\n",
        "    # Select only 'Canonical_Flow_ID' and 'Packet Indices' from the simulated_df.\n",
        "    merged_df = pd.merge(\n",
        "        original_df,\n",
        "        simulated_df[['Canonical_Flow_ID', 'Packet Indices']],\n",
        "        on='Canonical_Flow_ID',\n",
        "        how='left',\n",
        "        suffixes=('_original', '_simulated')\n",
        "    )\n",
        "\n",
        "    # Rename the new column for clarity\n",
        "    merged_df = merged_df.rename(columns={\n",
        "        'Packet Indices': 'Simulated Packet Indices'\n",
        "    })\n",
        "\n",
        "    # Drop the 'Canonical_Flow_ID' column, as it's only for merging\n",
        "    merged_df = merged_df.drop(columns=['Canonical_Flow_ID'])\n",
        "\n",
        "    print(\"\\n--- Merged Flow Data (Original CICFlowMeter with Simulated Packet Indices) ---\")\n",
        "    print(f\"Total rows in merged DataFrame: {len(merged_df)}\")\n",
        "    print(\"Head of the merged DataFrame:\")\n",
        "    print(merged_df.head())\n",
        "\n",
        "    print(\"\\nColumns in the merged DataFrame:\")\n",
        "    print(merged_df.columns.tolist())\n",
        "\n",
        "    return merged_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHfO6LP1bVdU",
        "outputId": "e9133e77-3176-43f5-cb3a-ccef234de151"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/merge_flow.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/simulate_flow.py\n",
        "# define function for simulating cicflowmter\n",
        "\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "import ipaddress # For robust IP address comparison\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Import Scapy for PCAP reading and parsing\n",
        "# You might need to install it: pip install scapy\n",
        "try:\n",
        "    from scapy.all import rdpcap, IP, TCP, UDP # type: ignore\n",
        "except ImportError:\n",
        "    print(\"Scapy not found. Please install it using: pip install scapy\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Define a Packet structure to standardize data from Scapy packets\n",
        "# 'index' corresponds to the sequential position in the PCAP file,\n",
        "# which is similar to the 'id' in CICFlowMeter's BasicPacketInfo.java\n",
        "Packet = namedtuple('Packet', ['index', 'timestamp', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'protocol', 'length', 'has_fin_flag'])\n",
        "\n",
        "class Flow:\n",
        "    \"\"\"\n",
        "    Simulates a network flow, similar to BasicFlow.java.\n",
        "    A flow is identified by its 5-tuple and stores a list of packet indices\n",
        "    that belong to this flow. It also tracks basic flow statistics and timestamps.\n",
        "    \"\"\"\n",
        "    def __init__(self, flow_key, first_packet):\n",
        "        self.flow_key = flow_key\n",
        "        # This list directly maps the flow to its packet indices (IDs)\n",
        "        self.packet_indices = [first_packet.index]\n",
        "        # Store packet timestamps as well for export\n",
        "        self.packet_timestamps = [int(first_packet.timestamp * 1_000_000)] # Convert to microseconds\n",
        "\n",
        "        self.start_time = int(first_packet.timestamp * 1_000_000) # Microseconds\n",
        "        self.last_packet_time = int(first_packet.timestamp * 1_000_000) # Microseconds\n",
        "        self.packet_count = 1\n",
        "        self.byte_count = first_packet.length\n",
        "        self.fwd_packets = [] # Simulating BasicFlow's 'forward' list\n",
        "        self.bwd_packets = [] # Simulating BasicFlow's 'backward' list\n",
        "\n",
        "        # Determine the initial direction based on the first packet's original IPs\n",
        "        # This is used for 'forward' and 'backward' packet grouping within the flow,\n",
        "        # distinct from the canonical direction used for the flow_key.\n",
        "        self.initial_src_ip = first_packet.src_ip\n",
        "\n",
        "        # Add first packet to appropriate directional list\n",
        "        if self._is_forward_packet(first_packet):\n",
        "            self.fwd_packets.append(first_packet)\n",
        "        else:\n",
        "            self.bwd_packets.append(first_packet)\n",
        "\n",
        "    def add_packet(self, packet):\n",
        "        \"\"\"Adds a packet to the flow and updates flow statistics.\"\"\"\n",
        "        self.packet_indices.append(packet.index)\n",
        "        self.packet_timestamps.append(int(packet.timestamp * 1_000_000)) # Store timestamp in microseconds\n",
        "\n",
        "        self.packet_count += 1\n",
        "        self.byte_count += packet.length\n",
        "\n",
        "        # Update directional packet lists and IATs (simplified for simulation)\n",
        "        if self._is_forward_packet(packet):\n",
        "            self.fwd_packets.append(packet)\n",
        "            # In real CICFlowMeter, IATs and other stats would be updated here, e.g.:\n",
        "            # if len(self.fwd_packets) > 1:\n",
        "            #     self.fwd_iat.add_value(packet.timestamp - self.fwd_packets[-2].timestamp)\n",
        "        else:\n",
        "            self.bwd_packets.append(packet)\n",
        "            # if len(self.bwd_packets) > 1:\n",
        "            #     self.bwd_iat.add_value(packet.timestamp - self.bwd_packets[-2].timestamp)\n",
        "\n",
        "        # Update last packet time for overall flow duration/IAT calculation\n",
        "        self.last_packet_time = int(packet.timestamp * 1_000_000) # Microseconds\n",
        "\n",
        "    def get_flow_duration(self):\n",
        "        \"\"\"Calculates the duration of the flow in microseconds.\"\"\"\n",
        "        return self.last_packet_time - self.start_time\n",
        "\n",
        "    def _is_forward_packet(self, packet):\n",
        "        \"\"\"\n",
        "        Determines if a packet is in the forward direction relative to the flow's initial direction.\n",
        "        This uses the original src_ip of the *first* packet to define \"forward\" for feature accumulation,\n",
        "        which is consistent with CICFlowMeter's internal `BasicFlow` logic.\n",
        "        \"\"\"\n",
        "        return packet.src_ip == self.initial_src_ip\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Flow(key={self.flow_key}, total_packets={self.packet_count}, \"\n",
        "                f\"duration={self.get_flow_duration():.4f}us, \"\n",
        "                f\"total_bytes={self.byte_count}B, \"\n",
        "                f\"fwd_pkts={len(self.fwd_packets)}, bwd_pkts={len(self.bwd_packets)}, \"\n",
        "                f\"packet_indices={self.packet_indices})\")\n",
        "\n",
        "    def to_csv_row(self):\n",
        "        \"\"\"\n",
        "        Converts the flow data into a dictionary suitable for CSV writing.\n",
        "        This provides a simplified representation of the features CICFlowMeter extracts,\n",
        "        but crucially includes the 'Packet Indices' and 'Packet Timestamps' columns.\n",
        "        The 'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port' are taken from the\n",
        "        canonical flow_key for consistency with CICFlowMeter's output format.\n",
        "        \"\"\"\n",
        "        # The flow_key tuple already holds the normalized 5-tuple as generated by generate_flow_key\n",
        "        canonical_src_ip, canonical_dst_ip, canonical_src_port, canonical_dst_port, proto = self.flow_key\n",
        "\n",
        "        return {\n",
        "            'Flow ID': f\"{canonical_src_ip}-{canonical_dst_ip}-{canonical_src_port}-{canonical_dst_port}-{proto}\",\n",
        "            'Src IP': canonical_src_ip,\n",
        "            'Src Port': canonical_src_port,\n",
        "            'Dst IP': canonical_dst_ip,\n",
        "            'Dst Port': canonical_dst_port,\n",
        "            'Protocol': proto,\n",
        "            'Flow Duration (us)': self.get_flow_duration(),\n",
        "            'Total Packets': self.packet_count,\n",
        "            'Total Bytes': self.byte_count,\n",
        "            'Fwd Packets': len(self.fwd_packets), # These counts are based on internal 'initial_src_ip'\n",
        "            'Bwd Packets': len(self.bwd_packets), # These counts are based on internal 'initial_src_ip'\n",
        "            'Packet Indices': str(self.packet_indices), # Convert list to string for CSV column\n",
        "            'Packet Timestamps': str(self.packet_timestamps) # Convert list to string for CSV column\n",
        "            # Add more CICFlowMeter-like features here if needed\n",
        "            # 'Flow Pkts/s': self.packet_count / (self.get_flow_duration() / 1_000_000.0) if self.get_flow_duration() > 0 else 0,\n",
        "            # 'Avg Fwd Pkt Len': sum(p.length for p in self.fwd_packets) / len(self.fwd_packets) if self.fwd_packets else 0,\n",
        "        }\n",
        "\n",
        "\n",
        "def generate_flow_key(packet):\n",
        "    \"\"\"\n",
        "    Generates a unique key for a flow based on the 5-tuple,\n",
        "    mimicking CICFlowMeter's Java BasicPacketInfo.generateFlowId() logic\n",
        "    for canonicalizing IP addresses and ports.\n",
        "\n",
        "    IMPORTANT: This function uses the raw protocol number, matching BasicPacketInfo.java.\n",
        "    Any mapping of non-TCP/UDP/ICMP protocols to '0' happens *after* Flow ID generation\n",
        "    in CICFlowMeter's pipeline (e.g., for display in FlowFeature.featureValue2String),\n",
        "    and is NOT part of the Flow ID itself.\n",
        "    \"\"\"\n",
        "    # Access attributes by name from the Packet namedtuple\n",
        "    src_ip_str = packet.src_ip\n",
        "    dst_ip_str = packet.dst_ip\n",
        "    src_port = packet.src_port\n",
        "    dst_port = packet.dst_port\n",
        "    protocol_int = packet.protocol # Use the raw protocol number here\n",
        "\n",
        "    # Use ipaddress for robust IP comparison, mirroring Java's byte-by-byte comparison\n",
        "    try:\n",
        "        src_ip_obj = ipaddress.ip_address(src_ip_str)\n",
        "        dst_ip_obj = ipaddress.ip_address(dst_ip_str)\n",
        "    except ValueError:\n",
        "        # Fallback for invalid IPs if any, should not happen with valid PCAP data\n",
        "        return (src_ip_str, dst_ip_str, src_port, dst_port, protocol_int)\n",
        "\n",
        "\n",
        "    # Determine 'forward' based on IP comparison: canonical_src_ip will be the \"smaller\" IP\n",
        "    if src_ip_obj < dst_ip_obj:\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "    elif dst_ip_obj < src_ip_obj:\n",
        "        # Swap IPs and their corresponding ports for normalization\n",
        "        normalized_src_ip = dst_ip_str\n",
        "        normalized_dst_ip = src_ip_str\n",
        "        normalized_src_port = dst_port\n",
        "        normalized_dst_port = src_port\n",
        "    else: # IPs are equal (e.g., multicast or broadcast)\n",
        "        # If IPs are the same, Java's logic does NOT swap ports based on IP.\n",
        "        # It keeps original src/dst IPs and ports as they are.\n",
        "        normalized_src_ip = src_ip_str\n",
        "        normalized_dst_ip = dst_ip_str\n",
        "        normalized_src_port = src_port\n",
        "        normalized_dst_port = dst_port\n",
        "\n",
        "    # The canonical 5-tuple key for the hash map\n",
        "    return (normalized_src_ip, normalized_dst_ip, normalized_src_port, normalized_dst_port, protocol_int)\n",
        "\n",
        "def process_packets_into_flows(packets, flow_timeout_us=120000000, idle_timeout_us=5000000):\n",
        "    \"\"\"\n",
        "    Processes a list of packets and groups them into flows, simulating FlowGenerator.java.\n",
        "    Args:\n",
        "        packets (list): A list of Packet namedtuples, derived from PCAP.\n",
        "        flow_timeout_us (int): Max flow duration in microseconds (120 seconds).\n",
        "        idle_timeout_us (int): Max idle time within a flow in microseconds (5 seconds).\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are unique flow identifiers (combining 5-tuple and a counter)\n",
        "              and values are Flow objects.\n",
        "    \"\"\"\n",
        "    active_flows = {} # {flow_key_5_tuple: Flow_object}\n",
        "    completed_flows = {} # {unique_completed_flow_id: Flow_object}\n",
        "\n",
        "    # Packets are assumed to be already sorted by timestamp when passed from PCAP reader\n",
        "\n",
        "    completed_flow_counter = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        flow_key = generate_flow_key(packet)\n",
        "        current_timestamp_us = int(packet.timestamp * 1_000_000) # Convert seconds to microseconds\n",
        "\n",
        "        # Check if this packet belongs to an existing active flow\n",
        "        if flow_key in active_flows:\n",
        "            flow = active_flows[flow_key]\n",
        "\n",
        "            # Check for IDLE timeout first (packet arrival AFTER idle period)\n",
        "            if (current_timestamp_us - flow.last_packet_time) > idle_timeout_us:\n",
        "                # Flow idle timed out, finish current flow and start new one\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove old flow from active\n",
        "\n",
        "                new_flow = Flow(flow_key, packet)\n",
        "                active_flows[flow_key] = new_flow\n",
        "\n",
        "            # Check for TOTAL flow timeout (flow duration)\n",
        "            elif (current_timestamp_us - flow.start_time) > flow_timeout_us:\n",
        "                # Flow timed out based on total duration, finish current flow and start new one\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove old flow from active\n",
        "\n",
        "                new_flow = Flow(flow_key, packet)\n",
        "                active_flows[flow_key] = new_flow\n",
        "\n",
        "            # Simulate TCP FIN flag termination\n",
        "            # Only apply if the protocol is TCP (6) and FIN flag is set.\n",
        "            # Add the FIN packet to the flow before deciding if it's finished.\n",
        "            elif packet.protocol == 6 and packet.has_fin_flag:\n",
        "                flow.add_packet(packet) # Add the FIN packet\n",
        "                completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "                completed_flow_counter += 1\n",
        "                active_flows.pop(flow_key) # Remove from active flows as it's finished\n",
        "\n",
        "            # Otherwise, add packet to existing active flow\n",
        "            else:\n",
        "                flow.add_packet(packet)\n",
        "                # No explicit idle time update needed in Flow object here,\n",
        "                # as it's checked upon next packet arrival.\n",
        "\n",
        "        else:\n",
        "            # New flow, or a flow that previously completed and was removed from active_flows\n",
        "            new_flow = Flow(flow_key, packet)\n",
        "            active_flows[flow_key] = new_flow\n",
        "\n",
        "    # After processing all packets, move any remaining active flows to completed flows\n",
        "    for flow_key, flow in list(active_flows.items()): # Iterate over a copy to allow modification\n",
        "        completed_flows[f\"{flow_key}_{completed_flow_counter}\"] = flow\n",
        "        completed_flow_counter += 1\n",
        "        # No need to pop from active_flows here, as loop is over.\n",
        "\n",
        "    return completed_flows\n",
        "\n",
        "def extract_packet_info_from_pcap(pcap_file_path):\n",
        "    \"\"\"\n",
        "    Reads a PCAP file using Scapy and extracts relevant information into Packet namedtuples.\n",
        "    Assigns a sequential index to each packet as it's read.\n",
        "    Broadened to include all IP packets, not just TCP/UDP.\n",
        "    \"\"\"\n",
        "    print(f\"Reading packets from {pcap_file_path}...\")\n",
        "    extracted_packets = []\n",
        "\n",
        "    try:\n",
        "        packets_scapy = rdpcap(pcap_file_path)\n",
        "        total_packets = len(packets_scapy)\n",
        "        print(f\"Successfully read {total_packets} packets from {pcap_file_path}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PCAP file not found at {pcap_file_path}.\")\n",
        "        return [] # Return empty list if file not found\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading PCAP file {pcap_file_path}: {e}\")\n",
        "        return [] # Return empty list on other errors\n",
        "\n",
        "\n",
        "    for i, pkt in enumerate(packets_scapy):\n",
        "        src_ip = None\n",
        "        dst_ip = None\n",
        "        src_port = 0 # Default to 0 for non-TCP/UDP protocols or if ports are missing\n",
        "        dst_port = 0 # Default to 0 for non-TCP/UDP protocols or if ports are missing\n",
        "        protocol = None\n",
        "        has_fin = False\n",
        "\n",
        "        # Ensure IP layer exists\n",
        "        if IP in pkt:\n",
        "            src_ip = pkt[IP].src\n",
        "            dst_ip = pkt[IP].dst\n",
        "            protocol = pkt[IP].proto # e.g., 6 for TCP, 17 for UDP, 1 for ICMP, 2 for IGMP, etc.\n",
        "\n",
        "            # Check for transport layer (TCP or UDP) to get ports and flags\n",
        "            if TCP in pkt:\n",
        "                src_port = pkt[TCP].sport\n",
        "                dst_port = pkt[TCP].dport\n",
        "                has_fin = bool(pkt[TCP].flags & 0x01) # FIN is bit 0 in TCP flags\n",
        "            elif UDP in pkt:\n",
        "                src_port = pkt[UDP].sport\n",
        "                dst_port = pkt[UDP].dport\n",
        "            # For other IP protocols (like ICMP, IGMP, etc.), src_port and dst_port remain 0.\n",
        "\n",
        "            # Only process packets with valid IP information\n",
        "            # This condition is now implicitly true for any packet with an IP layer,\n",
        "            # as src_ip, dst_ip, and protocol will be extracted.\n",
        "            extracted_packets.append(Packet(\n",
        "                index=i,\n",
        "                timestamp=pkt.time, # Scapy's pkt.time is already in seconds (float)\n",
        "                src_ip=src_ip,\n",
        "                dst_ip=dst_ip,\n",
        "                src_port=src_port,\n",
        "                dst_port=dst_port,\n",
        "                protocol=protocol,\n",
        "                length=len(pkt), # Total packet length\n",
        "                has_fin_flag=has_fin\n",
        "            ))\n",
        "\n",
        "    # Scapy's rdpcap usually returns packets in capture order (by timestamp),\n",
        "    # but explicit sorting ensures strict chronological processing as in CICFlowMeter.\n",
        "    extracted_packets.sort(key=lambda p: p.timestamp)\n",
        "    print(f\"Extracted {len(extracted_packets)} valid packets from PCAP.\")\n",
        "    return extracted_packets,total_packets\n",
        "\n",
        "def extract_flows_from_pcap(pcap_file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire process of extracting network flows from a PCAP file,\n",
        "    mimicking CICFlowMeter's logic, and returns the flows as a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        pcap_file_path (str): The path to the input PCAP file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row represents a discovered flow,\n",
        "                      including 'Packet Indices' and 'Packet Timestamps'.\n",
        "                      Returns an empty DataFrame if no valid packets are found or\n",
        "                      if PCAP file cannot be read.\n",
        "    \"\"\"\n",
        "    # Step 1: Extract packet information from the PCAP file\n",
        "    packets_from_pcap,total_packets = extract_packet_info_from_pcap(pcap_file_path)\n",
        "\n",
        "    if not packets_from_pcap:\n",
        "        print(\"No valid packets found or PCAP file could not be read. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    print(\"\\nProcessing packets into flows (simulating CICFlowMeter logic)...\")\n",
        "    # Step 2: Process the extracted packets into flows\n",
        "    # Default timeouts are 120 seconds (flow) and 5 seconds (idle) for CICFlowMeter\n",
        "    flows_data = process_packets_into_flows(packets_from_pcap,\n",
        "                                            flow_timeout_us=120_000_000,\n",
        "                                            idle_timeout_us=5_000_000)\n",
        "\n",
        "    print(f\"\\nDiscovered {len(flows_data)} flows.\")\n",
        "\n",
        "    # Step 3: Convert discovered flows to a list of dictionaries for DataFrame creation\n",
        "    flows_list_of_dicts = []\n",
        "    for flow_unique_id, flow_obj in flows_data.items():\n",
        "        flows_list_of_dicts.append(flow_obj.to_csv_row())\n",
        "\n",
        "    # Create DataFrame from the list of flow dictionaries\n",
        "    flows_df = pd.DataFrame(flows_list_of_dicts)\n",
        "\n",
        "    # Step 4: Display a summary of generated flows (for console output)\n",
        "    if not flows_df.empty:\n",
        "        print(\"\\nHead of the generated Flows DataFrame:\")\n",
        "        print(flows_df.head())\n",
        "        print(\"\\nColumns in the generated Flows DataFrame:\")\n",
        "        print(flows_df.columns.tolist())\n",
        "    else:\n",
        "        print(\"\\nNo flows generated to display.\")\n",
        "\n",
        "    return flows_df,total_packets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKIY85pTb6Hx",
        "outputId": "4e260232-99a0-4385-84e5-bf762c70a6fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/simulate_flow.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/utils.py\n",
        "# define function for dataframe preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def map_port(port):\n",
        "    if port == 21:\n",
        "        return 1  # FTP\n",
        "    elif port == 22:\n",
        "        return 2  # SSH\n",
        "    elif port == 53:\n",
        "        return 3  # DNS\n",
        "    elif port == 80:\n",
        "        return 4  # HTTP\n",
        "    elif port == 443:\n",
        "        return 5  # HTTPS\n",
        "    else:\n",
        "        return 6  # Other\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    original_indices = set(df.index)\n",
        "\n",
        "    # replace space in columns name with underscore\n",
        "    df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "    # drop objects type columns\n",
        "    columns_to_drop = [\n",
        "        'Flow_ID','Src_IP','Dst_IP','Src_Port','Protocol','Timestamp','Label'\n",
        "    ]\n",
        "\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # remove rows with missing and infinite values\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # map destination port to 1-6 numbers\n",
        "    df['Dst_Port'] = df['Dst_Port'].apply(map_port)\n",
        "\n",
        "    return df\n",
        "\n",
        "# preprocess_dataframe(df)\n",
        "\n",
        "def choose_label(labels):\n",
        "    if all(label == 'Benign' for label in labels):\n",
        "        return 'Benign'\n",
        "\n",
        "    # Count non-Benign labels\n",
        "    non_benign_labels = [label for label in labels if label != 'Benign']\n",
        "    label_counts = Counter(non_benign_labels)\n",
        "\n",
        "    # Return the most common non-Benign label\n",
        "    most_common_label, _ = label_counts.most_common(1)[0]\n",
        "    return most_common_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RarxpKcBPm",
        "outputId": "d4566161-f49c-4d5c-9952-3f820ea2092c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "2hRtjS45cHCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import requests\n",
        "import joblib\n",
        "import os\n",
        "import shutil\n",
        "import google.generativeai as genai\n",
        "import sys # Import sys for potential exit if truly needed (though st.stop() is preferred in Streamlit)\n",
        "\n",
        "# Assuming these are in your project directory (or adjust paths if they are elsewhere)\n",
        "from src.merge_flow import *\n",
        "from src.simulate_flow import *\n",
        "from src.utils import * # This is where map_port, preprocess_dataframe, and choose_label are now expected to be\n",
        "\n",
        "# Global constant for Gemini model\n",
        "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
        "\n",
        "# --- Initialize session state variables if they don't exist ---\n",
        "# This is the crucial part for robustness\n",
        "if 'initial_setup_completed' not in st.session_state:\n",
        "    st.session_state.initial_setup_completed = False\n",
        "if 'setup_failed' not in st.session_state:\n",
        "    st.session_state.setup_failed = False\n",
        "if 'show_setup_logs' not in st.session_state:\n",
        "    st.session_state.show_setup_logs = False\n",
        "if 'model_state' not in st.session_state: # Initialize model_state\n",
        "    st.session_state.model_state = None # Set to None initially\n",
        "\n",
        "# --- Your existing cached setup function ---\n",
        "@st.cache_data(show_spinner=False)\n",
        "def initial_setup_cached(cache_key_for_setup):\n",
        "    # These resets are crucial for the first run or after \"Analyze another file\"\n",
        "    # Keeping them for the first setup call, but the global initialization handles subsequent ones\n",
        "    st.session_state.initial_setup_completed = False\n",
        "    st.session_state.setup_failed = False\n",
        "\n",
        "    success = display_setup_logs()\n",
        "\n",
        "    st.session_state.initial_setup_completed = success\n",
        "    st.session_state.setup_failed = not success\n",
        "    st.session_state.show_setup_logs = True\n",
        "    return success\n",
        "\n",
        "# --- LLM Configuration Function ---\n",
        "@st.cache_resource(show_spinner=\"Connecting to Gemini...\")\n",
        "def configure_gemini(api_key):\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "        model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini (within cache_resource): {e}\") # Log to console\n",
        "        return None\n",
        "\n",
        "def display_setup_logs():\n",
        "    # CICFlowMeter setup\n",
        "    with st.status(\"Setting up CICFlowMeter-3.0...\", expanded=True, state=\"running\") as status:\n",
        "        try:\n",
        "            # install libpcap-dev library\n",
        "            st.write(\":arrow_down: Installing libpcap-dev...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"update\"], check=True, capture_output=True, text=True)\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"libpcap-dev\"], check=True, capture_output=True, text=True)\n",
        "            st.write(\":white_check_mark: libpcap-dev installed.\")\n",
        "\n",
        "            if not os.path.exists(\"CICFlowMeter-3.0\"):\n",
        "                st.write(\":arrow_down: Downloading CICFlowMeter-3.0.zip...\")\n",
        "                url = \"https://codeberg.org/iortega/TCPDUMP_and_CICFlowMeter/archive/master:CICFlowMeters/CICFlowMeter-3.0.zip\"\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(\"CICFlowMeter-3.0.zip\", \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                st.write(\":white_check_mark: CICFlowMeter-3.0.zip downloaded.\")\n",
        "\n",
        "                st.write(\":open_file_folder: Extracting CICFlowMeter-3.0...\")\n",
        "                subprocess.run([\"unzip\", \"-o\", \"CICFlowMeter-3.0.zip\", \"-d\", \"CICFlowMeter-3.0\"], check=True, capture_output=True, text=True)\n",
        "                st.write(\":white_check_mark: CICFlowMeter extracted.\")\n",
        "\n",
        "                st.write(\":wrench: Configuring executable permission...\")\n",
        "                subprocess.run([\"chmod\", \"+x\", \"CICFlowMeter-3.0/tcpdump_and_cicflowmeter/bin/CICFlowMeter\"], check=True, capture_output=True, text=True)\n",
        "                st.write(\":white_check_mark: Permission configured\")\n",
        "\n",
        "                st.write(\":wastebasket: Clearing .zip file...\")\n",
        "                subprocess.run([\"rm\", \"CICFlowMeter-3.0.zip\"], check=True, capture_output=True, text=True)\n",
        "                st.write(\":white_check_mark: CICFlowMeter-3.0.zip Cleared\")\n",
        "            else:\n",
        "                st.write(\":information_source: CICFlowMeter-3.0 existed. Skipping...\")\n",
        "\n",
        "            st.write(\":file_folder: Creating data/in data/out directories...\")\n",
        "            os.makedirs(\"data/in\", exist_ok=True)\n",
        "            os.makedirs(\"data/out\", exist_ok=True)\n",
        "            st.write(\":white_check_mark: Directories created.\")\n",
        "\n",
        "            status.update(label=\":white_check_mark: CICFlowMeter Setup Complete!\", state=\"complete\", expanded=False)\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            st.error(f\":x: Error during CICFlowMeter setup. Command '{e.cmd}' returned non-zero exit status {e.returncode}. Output: {e.stdout}\\nError: {e.stderr}\")\n",
        "            status.update(label=\":x: CICFlowMeter Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\":x: Error downloading CICFlowMeter: {e}\")\n",
        "            status.update(label=\":x: CICFlowMeter Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            st.error(f\":x: An unexpected error occurred during setup: {e}\")\n",
        "            status.update(label=\":x: CICFlowMeter Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "\n",
        "    # Classification Model setup\n",
        "    with st.status(\"Setting up ML Model...\", expanded=True, state=\"running\") as status:\n",
        "        try:\n",
        "            if not os.path.exists(\"RandomForest400IntPortCIC1718-2.pkl\"):\n",
        "                st.write(\":hugging_face: Downloading ML model...\")\n",
        "                model_url = \"https://huggingface.co/Mc4minta/RandomForest400IntPortCIC1718/resolve/main/RandomForest400IntPortCIC1718-2.pkl\"\n",
        "                response = requests.get(model_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(\"RandomForest400IntPortCIC1718-2.pkl\", \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                st.write(\":white_check_mark: ML Model downloaded.\")\n",
        "\n",
        "            status.update(label=\":white_check_mark: ML Model Setup Complete\", state=\"complete\", expanded=False)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\":x: Error downloading ML Model: {e}\")\n",
        "            status.update(label=\":x: ML Model Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            st.error(f\"An unexpected error occurred during ML Model setup: {e}\")\n",
        "            status.update(label=\":x: ML Model Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "\n",
        "    # Sample PCAP setup\n",
        "    with st.status(\"Downloading sample pcap...\", expanded=True, state=\"running\") as status:\n",
        "        try:\n",
        "            if not os.path.exists(\"sample_pcap.zip\"):\n",
        "                st.write(\":hugging_face: Downloading sample_pcap.zip...\")\n",
        "                zip_url = \"https://huggingface.co/Mc4minta/RandomForest400IntPortCIC1718/resolve/main/sample_pcap.zip\"\n",
        "                response = requests.get(zip_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(\"sample_pcap.zip\", \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                st.write(\":white_check_mark: sample_pcap.zip Downloaded.\")\n",
        "\n",
        "            if not os.path.exists(\"sample_pcap\") or not os.listdir(\"sample_pcap\"):\n",
        "                os.makedirs(\"sample_pcap\", exist_ok=True)\n",
        "\n",
        "                st.write(\":open_file_folder: Extracting sample_pcap...\")\n",
        "                subprocess.run([\"unzip\", \"-o\", \"sample_pcap.zip\", \"-d\", \".\"], check=True, capture_output=True, text=True)\n",
        "                st.write(\":white_check_mark: sample_pcap extracted.\")\n",
        "\n",
        "                if os.path.exists(\"sample_pcap\") and os.listdir(\"sample_pcap\"):\n",
        "                    pcap_files = [f for f in os.listdir(\"sample_pcap\") if f.endswith('.pcap')]\n",
        "                    st.write(f\":information_source: Found {len(pcap_files)} .pcap files in sample_pcap directory.\")\n",
        "                else:\n",
        "                    st.warning(\":warning: sample_pcap directory appears to be empty after extraction.\")\n",
        "            else:\n",
        "                pcap_files = [f for f in os.listdir(\"sample_pcap\") if f.endswith('.pcap')]\n",
        "                st.write(f\":information_source: Sample PCAP directory already exists with {len(pcap_files)} .pcap files. Skipping extraction.\")\n",
        "\n",
        "            status.update(label=\":white_check_mark: Sample Data Setup Complete\", state=\"complete\", expanded=False)\n",
        "            return True\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            st.error(f\":x: Error extracting sample data: Command '{e.cmd}' returned {e.returncode}. Output: {e.stdout}\\nError: {e.stderr}\")\n",
        "            status.update(label=\":x: Sample Data Extraction Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\":x: Error downloading sample data: {e}\")\n",
        "            status.update(label=\":x: Sample Data Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            st.error(f\":x: An unexpected error occurred during sample data setup: {e}\")\n",
        "            status.update(label=\":x: Sample Data Setup Failed\", state=\"error\", expanded=True)\n",
        "            st.session_state.setup_failed = True\n",
        "            st.session_state.initial_setup_completed = False\n",
        "            return False\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def initial_setup_cached():\n",
        "    if st.session_state.get('initial_setup_completed', False) and not st.session_state.get('setup_failed', False):\n",
        "        return True\n",
        "\n",
        "    st.session_state.initial_setup_completed = False\n",
        "    st.session_state.setup_failed = False\n",
        "\n",
        "    success = display_setup_logs()\n",
        "\n",
        "    st.session_state.initial_setup_completed = success\n",
        "    st.session_state.setup_failed = not success\n",
        "    st.session_state.show_setup_logs = True\n",
        "    return success\n",
        "\n",
        "def clear_uploaded_files():\n",
        "    # remove pcap files\n",
        "    if os.path.exists(\"data/in\"):\n",
        "        for filename in os.listdir(\"data/in\"):\n",
        "            filepath = os.path.join(\"data/in\", filename)\n",
        "            try:\n",
        "                if os.path.isfile(filepath):\n",
        "                    os.remove(filepath)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to delete {filename} from data/in: {e}\")\n",
        "    # remove csv files\n",
        "    if os.path.exists(\"data/out\"):\n",
        "        for filename in os.listdir(\"data/out\"):\n",
        "            filepath = os.path.join(\"data/out\", filename)\n",
        "            try:\n",
        "                if os.path.isfile(filepath):\n",
        "                    os.remove(filepath)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to delete {filename} from data/out: {e}\")\n",
        "\n",
        "# --- Prediction Pipeline (YOUR APPROACH) ---\n",
        "def run_prediction_pipeline(pcap_file_path, uploaded_pcap_name): # Removed 'model' parameter\n",
        "    # Load the ML model *here*, just before it's used for prediction\n",
        "    model_path = 'RandomForest400IntPortCIC1718-2.pkl'\n",
        "    model = None # Initialize model to None\n",
        "    try:\n",
        "        if not os.path.exists(model_path):\n",
        "            st.error(f\"ML model file not found at {model_path}. Please ensure setup completed successfully.\")\n",
        "            return None, 0, None # Indicate failure if file isn't there\n",
        "        st.status(\":robot_face: Loading ML model...\", state=\"running\") # Optional status update\n",
        "        model = joblib.load(model_path)\n",
        "        st.status(\":white_check_mark: ML Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading ML model: {e}. Cannot run prediction.\")\n",
        "        return None, 0, None # Indicate failure\n",
        "\n",
        "    # --- Rest of your run_prediction_pipeline function ---\n",
        "    data_in_dir = 'data/in/'\n",
        "    data_out_dir = 'data/out/'\n",
        "\n",
        "    file_name_without_ext = os.path.splitext(uploaded_pcap_name)[0]\n",
        "    index_file_path = f'{data_out_dir}{file_name_without_ext}_index.csv'\n",
        "    flow_file_path = f'{data_out_dir}{file_name_without_ext}_ISCX.csv'\n",
        "    prediction_file_path = f'{data_out_dir}{file_name_without_ext}_prediction.csv'\n",
        "    prompt_file_path = f'{data_out_dir}{file_name_without_ext}_prompt.csv' # This is the key file for LLM\n",
        "\n",
        "    try:\n",
        "        with st.status(\"Running analysis pipeline...\", expanded=True) as pipeline_status:\n",
        "            pipeline_status.write(\":mag: Starting analysis...\")\n",
        "\n",
        "            # Step 1: Run CICFlowMeter to generate flow features\n",
        "            pipeline_status.write(\":rocket: Running CICFlowMeter to generate flow features...\")\n",
        "            subprocess.run(\"CICFlowMeter-3.0/tcpdump_and_cicflowmeter/bin/CICFlowMeter\", check=True, capture_output=True, text=True)\n",
        "            pipeline_status.write(\":white_check_mark: CICFlowMeter finished.\")\n",
        "\n",
        "            # Step 2: Simulate flow with packet indices\n",
        "            pipeline_status.write(\":chart_with_upwards_trend: Extracting flows and simulating packet indices...\")\n",
        "            # Assuming extract_flows_from_pcap is defined elsewhere\n",
        "            simulated_flows_df, total_packets = extract_flows_from_pcap(pcap_file_path)\n",
        "            simulated_flows_df['Total_Packets'] = total_packets\n",
        "            simulated_flows_df.to_csv(index_file_path, index=False)\n",
        "            pipeline_status.write(f\":white_check_mark: Simulated flows extracted. Total packets: {total_packets}\")\n",
        "\n",
        "            # Step 3: Merge packet indices flow with original flow\n",
        "            pipeline_status.write(\":handshake: Merging simulated and original flow data...\")\n",
        "            # Assuming read_flows_to_dataframe and merge_flows_and_return_dataframe are defined elsewhere\n",
        "            simulated_df, _ = read_flows_to_dataframe(index_file_path, is_simulated_output=True)\n",
        "            original_df, _ = read_flows_to_dataframe(flow_file_path, is_simulated_output=False)\n",
        "\n",
        "            merged_df = merge_flows_and_return_dataframe(simulated_df, original_df)\n",
        "            merged_df['Total_Packets'] = total_packets\n",
        "            merged_df.to_csv(index_file_path, index=False)\n",
        "            pipeline_status.write(\":white_check_mark: Flow data merged successfully.\")\n",
        "\n",
        "            # --- START OF PREDICTION PIPELINE (integrated) ---\n",
        "            # 1. Extract Flow-level Metadata\n",
        "            flow_metadata_cols = [\n",
        "                'Simulated Packet Indices',\n",
        "                'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Protocol',\n",
        "                'Fwd Seg Size Min', 'Init Fwd Win Byts', 'Bwd Pkts/s', 'Flow IAT Max',\n",
        "                'Flow Duration', 'Pkt Len Mean', 'Flow Pkts/s', 'Fwd Header Len',\n",
        "                'TotLen Fwd Pkts', 'Pkt Size Avg', 'Init Bwd Win Byts', 'Flow IAT Mean',\n",
        "                'Subflow Fwd Byts', 'Bwd Pkt Len Mean', 'Bwd Header Len',\n",
        "                'Bwd Seg Size Avg', 'PSH Flag Cnt', 'Flow Byts/s', 'Fwd Pkts/s'\n",
        "            ]\n",
        "            flow_metadata_df = merged_df[flow_metadata_cols].copy()\n",
        "\n",
        "            # 2. Preprocess Data for Model Prediction\n",
        "            pipeline_status.write(\":gear: Preprocessing data for prediction...\")\n",
        "            # Assuming preprocess_dataframe is defined elsewhere\n",
        "            df = preprocess_dataframe(merged_df.copy())\n",
        "            total_packets = df['Total_Packets'].iloc[0] if 'Total_Packets' in df.columns and not df.empty else 0\n",
        "            if 'Total_Packets' in df.columns:\n",
        "                df = df.drop(columns=['Total_Packets'])\n",
        "\n",
        "            original_flow_indices = df.index\n",
        "\n",
        "            if 'Simulated_Packet_Indices' in df.columns:\n",
        "                df_for_prediction = df.drop(columns=['Simulated_Packet_Indices'])\n",
        "            else:\n",
        "                df_for_prediction = df.copy()\n",
        "            pipeline_status.write(\":white_check_mark: Data preprocessed.\")\n",
        "\n",
        "            # 3. Perform Prediction (per-flow prediction)\n",
        "            pipeline_status.write(\":robot_face: Performing flow-level predictions...\")\n",
        "            flow_predictions = model.predict(df_for_prediction) # Uses the 'model' loaded above\n",
        "            pipeline_status.write(\":white_check_mark: Predictions complete.\")\n",
        "\n",
        "            # 4. Link Predictions with Flow Metadata\n",
        "            df_flow_level_result = flow_metadata_df.loc[original_flow_indices].copy()\n",
        "            df_flow_level_result['Label'] = flow_predictions\n",
        "\n",
        "            # 5. Explode Packet Indices\n",
        "            df_prediction_expanded = df_flow_level_result.explode('Simulated Packet Indices').reset_index(drop=True)\n",
        "            df_prediction_expanded = df_prediction_expanded.rename(columns={'Simulated Packet Indices': 'Packet_Indices'})\n",
        "            df_prediction_expanded['Packet_Indices'] = df_prediction_expanded['Packet_Indices'].astype(int)\n",
        "\n",
        "            # 6. Group by Packet Indices and Aggregate Features\n",
        "            pipeline_status.write(\":clipboard: Aggregating features by packet index...\")\n",
        "            # Assuming choose_label is defined elsewhere\n",
        "            final_df_prediction = df_prediction_expanded.groupby('Packet_Indices').agg(\n",
        "                Label=('Label', lambda x: choose_label(list(x))),\n",
        "                Source_IP=('Src IP', 'first'),\n",
        "                Destination_IP=('Dst IP', 'first'),\n",
        "                Source_Port=('Src Port', 'first'),\n",
        "                Destination_Port=('Dst Port', 'first'),\n",
        "                Protocol=('Protocol', 'first'),\n",
        "                Fwd_Seg_Size_Min=('Fwd Seg Size Min', 'first'),\n",
        "                Init_Fwd_Win_Byts=('Init Fwd Win Byts', 'first'),\n",
        "                Bwd_Pkts_s=('Bwd Pkts/s', 'first'),\n",
        "                Flow_IAT_Max=('Flow IAT Max', 'first'),\n",
        "                Flow_Duration=('Flow Duration', 'first'),\n",
        "                Pkt_Len_Mean=('Pkt Len Mean', 'first'),\n",
        "                Flow_Pkts_s=('Flow Pkts/s', 'first'),\n",
        "                Fwd_Header_Len=('Fwd Header Len', 'first'),\n",
        "                TotLen_Fwd_Pkts=('TotLen Fwd Pkts', 'first'),\n",
        "                Pkt_Size_Avg=('Pkt Size Avg', 'first'),\n",
        "                Init_Bwd_Win_Byts=('Init Bwd Win Byts', 'first'),\n",
        "                Flow_IAT_Mean=('Flow IAT Mean', 'first'),\n",
        "                Subflow_Fwd_Byts=('Subflow Fwd Byts', 'first'),\n",
        "                Bwd_Pkt_Len_Mean=('Bwd Pkt Len Mean', 'first'),\n",
        "                Bwd_Header_Len=('Bwd Header Len', 'first'),\n",
        "                Bwd_Seg_Size_Avg=('Bwd Seg Size Avg', 'first'),\n",
        "                PSH_Flag_Cnt=('PSH Flag Cnt', 'first'),\n",
        "                Flow_Byts_s=('Flow Byts/s', 'first'),\n",
        "                Fwd_Pkts_s=('Fwd Pkts/s', 'first')\n",
        "            ).reset_index()\n",
        "            pipeline_status.write(\":white_check_mark: Aggregation complete.\")\n",
        "\n",
        "            for col in [\n",
        "                'Source_Port', 'Destination_Port', 'Protocol',\n",
        "                'Fwd_Seg_Size_Min', 'Init_Fwd_Win_Byts', 'Flow_Duration',\n",
        "                'Fwd_Header_Len', 'TotLen_Fwd_Pkts', 'Init_Bwd_Win_Byts',\n",
        "                'Subflow_Fwd_Byts', 'Bwd_Header_Len', 'PSH_Flag_Cnt'\n",
        "            ]:\n",
        "                final_df_prediction[col] = final_df_prediction[col].astype('Int64')\n",
        "\n",
        "            # 7. Adjust Packet Indices to be 1-based\n",
        "            final_df_prediction['Packet_Indices'] = final_df_prediction['Packet_Indices'] + 1\n",
        "            final_df_prediction['Packet_Indices'] = final_df_prediction['Packet_Indices'].astype(int)\n",
        "\n",
        "            # 8. Handle Missing Indices and Add Features\n",
        "            full_indices = set(range(1, total_packets + 1))\n",
        "            existing_indices = set(final_df_prediction['Packet_Indices'])\n",
        "            missing_indices = sorted(list(full_indices - existing_indices))\n",
        "\n",
        "            df_missing = pd.DataFrame({\n",
        "                'Packet_Indices': missing_indices,\n",
        "                'Label': ['Benign'] * len(missing_indices),\n",
        "                'Source_IP': [np.nan] * len(missing_indices),\n",
        "                'Destination_IP': [np.nan] * len(missing_indices),\n",
        "                'Source_Port': [pd.NA] * len(missing_indices),\n",
        "                'Destination_Port': [pd.NA] * len(missing_indices),\n",
        "                'Protocol': [pd.NA] * len(missing_indices),\n",
        "                'Fwd_Seg_Size_Min': [pd.NA] * len(missing_indices),\n",
        "                'Init_Fwd_Win_Byts': [pd.NA] * len(missing_indices),\n",
        "                'Bwd_Pkts_s': [np.nan] * len(missing_indices),\n",
        "                'Flow_IAT_Max': [np.nan] * len(missing_indices),\n",
        "                'Flow_Duration': [pd.NA] * len(missing_indices),\n",
        "                'Pkt_Len_Mean': [np.nan] * len(missing_indices),\n",
        "                'Flow_Pkts_s': [np.nan] * len(missing_indices),\n",
        "                'Fwd_Header_Len': [pd.NA] * len(missing_indices),\n",
        "                'TotLen_Fwd_Pkts': [pd.NA] * len(missing_indices),\n",
        "                'Pkt_Size_Avg': [np.nan] * len(missing_indices),\n",
        "                'Init_Bwd_Win_Byts': [pd.NA] * len(missing_indices),\n",
        "                'Flow_IAT_Mean': [np.nan] * len(missing_indices),\n",
        "                'Subflow_Fwd_Byts': [pd.NA] * len(missing_indices),\n",
        "                'Bwd_Pkt_Len_Mean': [np.nan] * len(missing_indices),\n",
        "                'Bwd_Header_Len': [pd.NA] * len(missing_indices),\n",
        "                'Bwd_Seg_Size_Avg': [np.nan] * len(missing_indices),\n",
        "                'PSH_Flag_Cnt': [pd.NA] * len(missing_indices),\n",
        "                'Flow_Byts_s': [np.nan] * len(missing_indices),\n",
        "                'Fwd_Pkts_s': [np.nan] * len(missing_indices),\n",
        "            })\n",
        "\n",
        "            for col in [\n",
        "                'Source_Port', 'Destination_Port', 'Protocol',\n",
        "                'Fwd_Seg_Size_Min', 'Init_Fwd_Win_Byts', 'Flow_Duration',\n",
        "                'Fwd_Header_Len', 'TotLen_Fwd_Pkts', 'Init_Bwd_Win_Byts',\n",
        "                'Subflow_Fwd_Byts', 'Bwd_Header_Len', 'PSH_Flag_Cnt'\n",
        "            ]:\n",
        "                df_missing[col] = df_missing[col].astype('Int64')\n",
        "            df_missing['Packet_Indices'] = df_missing['Packet_Indices'].astype(int)\n",
        "\n",
        "            # 9. Ensure Consistent Column Order\n",
        "            final_columns_order = [\n",
        "                'Packet_Indices', 'Label', 'Source_IP', 'Destination_IP',\n",
        "                'Source_Port', 'Destination_Port', 'Protocol',\n",
        "                'Fwd_Seg_Size_Min', 'Init_Fwd_Win_Byts', 'Bwd_Pkts_s', 'Flow_IAT_Max',\n",
        "                'Flow_Duration', 'Pkt_Len_Mean', 'Flow_Pkts_s', 'Fwd_Header_Len',\n",
        "                'TotLen_Fwd_Pkts', 'Pkt_Size_Avg', 'Init_Bwd_Win_Byts', 'Flow_IAT_Mean',\n",
        "                'Subflow_Fwd_Byts', 'Bwd_Pkt_Len_Mean', 'Bwd_Header_Len',\n",
        "                'Bwd_Seg_Size_Avg', 'PSH_Flag_Cnt', 'Flow_Byts_s', 'Fwd_Pkts_s'\n",
        "            ]\n",
        "            final_df_prediction = final_df_prediction.reindex(columns=final_columns_order)\n",
        "            df_missing = df_missing.reindex(columns=final_columns_order)\n",
        "\n",
        "            # 10. Concatenate and Sort Final DataFrame\n",
        "            final_df_prediction = pd.concat([final_df_prediction, df_missing], ignore_index=True)\n",
        "            final_df_prediction.sort_values(by='Packet_Indices', inplace=True)\n",
        "            final_df_prediction.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            final_df_prediction.to_csv(prompt_file_path, index=False) # This is the file with all details for LLM\n",
        "\n",
        "            columns_to_keep_for_display = [\n",
        "                'Packet_Indices',\n",
        "                'Label',\n",
        "                'Source_IP',\n",
        "                'Destination_IP',\n",
        "                'Source_Port',\n",
        "                'Destination_Port',\n",
        "                'Protocol',\n",
        "            ]\n",
        "            display_df = final_df_prediction[columns_to_keep_for_display].copy()\n",
        "            display_df.to_csv(prediction_file_path, index=False)\n",
        "\n",
        "            pipeline_status.update(label=\":white_check_mark: Analysis Complete!\", state=\"complete\", expanded=False)\n",
        "            return display_df, total_packets, prompt_file_path # Return prompt_file_path\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        st.error(f\":x: Pipeline Error during command execution: {e.cmd} returned {e.returncode}. Output: {e.stdout}\\nError: {e.stderr}\")\n",
        "        return None, 0, None\n",
        "    except Exception as e:\n",
        "        st.error(f\":x: An unexpected error occurred during the analysis pipeline: {e}\")\n",
        "        return None, 0, None\n",
        "\n",
        "\n",
        "# --- Function to generate the initial prompt for a single flow (for LLM) ---\n",
        "def create_gemini_prompt_for_streamlit(df_row):\n",
        "    prompt = f\"\"\"\n",
        "I have a network flow extracted from a .pcap file using CICFlowMeter, predicted by a machine learning model.\n",
        "This flow was classified as: **{df_row['Label']}**.\n",
        "\n",
        "Here are the flow details:\n",
        "- **Packet Index**: {df_row['Packet_Indices']}\n",
        "- **Source IP**: {df_row['Source_IP']}\n",
        "- **Destination IP**: {df_row['Destination_IP']}\n",
        "- **Destination Port**: {df_row['Destination_Port']}\n",
        "- **Protocol**: {df_row['Protocol']}\n",
        "- **Minimum segment size observed in the forward direction**: {df_row['Fwd_Seg_Size_Min']} bytes\n",
        "- **Total initial window bytes in forward direction**: {df_row['Init_Fwd_Win_Byts']} bytes\n",
        "- **Backward packets per second**: {df_row['Bwd_Pkts_s']}\n",
        "- **Maximum time between two packets (Flow IAT Max)**: {df_row['Flow_IAT_Max']} ms\n",
        "- **Flow Duration**: {df_row['Flow_Duration']} ms\n",
        "- **Mean packet length**: {df_row['Pkt_Len_Mean']} bytes\n",
        "- **Flow packets per second**: {df_row['Flow_Pkts_s']}\n",
        "- **Forward header length**: {df_row['Fwd_Header_Len']} bytes\n",
        "- **Total length of forward packets**: {df_row['TotLen_Fwd_Pkts']} bytes\n",
        "- **Average packet size**: {df_row['Pkt_Size_Avg']} bytes\n",
        "- **Total initial window bytes in backward direction**: {df_row['Init_Bwd_Win_Byts']} bytes\n",
        "- **Mean time between two packets (Flow IAT Mean)**: {df_row['Flow_IAT_Mean']} ms\n",
        "- **Average forward subflow bytes**: {df_row['Subflow_Fwd_Byts']} bytes\n",
        "- **Mean backward packet length**: {df_row['Bwd_Pkt_Len_Mean']} bytes\n",
        "- **Backward header length**: {df_row['Bwd_Header_Len']} bytes\n",
        "- **Average segment size in backward direction**: {df_row['Bwd_Seg_Size_Avg']} bytes\n",
        "- **PSH Flag Count**: {df_row['PSH_Flag_Cnt']}\n",
        "- **Flow bytes per second**: {df_row['Flow_Byts_s']}\n",
        "- **Forward packets per second**: {df_row['Fwd_Pkts_s']}\n",
        "\n",
        "Given these details, please explain:\n",
        "1.  **Why** might the model classify this flow as **{df_row['Label']}**? Elaborate on the features that strongly suggest this classification.\n",
        "2.  What **suspicious behaviors** or **flow characteristics** directly support this classification, if any?\n",
        "3.  What **insights** does this specific prediction provide about the network activity?\n",
        "4.  What parts of the original **.pcap file** (e.g., specific filters to apply in Wireshark/tcpdump, packet types) should I examine further to confirm or understand this flow better?\n",
        "5.  Based on this classification, what are the **immediate next steps** for investigation or mitigation from a cybersecurity perspective?\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "# --- Streamlit Main Function ---\n",
        "def main():\n",
        "    # 1. Set page config FIRST\n",
        "    st.set_page_config(\n",
        "        page_title=\"Malicious .PCAP File Classifier\",\n",
        "        page_icon=\":peacock:\",\n",
        "        layout=\"centered\",\n",
        "        initial_sidebar_state=\"expanded\",\n",
        "    )\n",
        "\n",
        "    # 2. Then perform API key check and Gemini model initialization\n",
        "    # It's better to fetch the API key here\n",
        "\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        st.error(\"🚨 GOOGLE_API_KEY environment variable not set.\")\n",
        "        st.warning(\"Please set it before running the app. E.g.:\")\n",
        "        st.code(\"export GOOGLE_API_KEY='YOUR_API_KEY'  # For Linux terminal\")\n",
        "        st.code(\"os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY' # For Google Colab cell\")\n",
        "        st.info(\"The application cannot proceed without the API key.\")\n",
        "        st.stop() # Halt execution if API key is missing\n",
        "\n",
        "    # Initialize Gemini model (this will be called once per session due to st.cache_resource)\n",
        "    # Pass the fetched api_key\n",
        "    gemini_model = configure_gemini(api_key)\n",
        "    if gemini_model is None:\n",
        "        # configure_gemini already displayed error, so just stop\n",
        "        st.info(\"Gemini model could not be initialized. Please check your API key and connection.\")\n",
        "        st.stop() # Halt execution if Gemini model couldn't be configured\n",
        "\n",
        "    # Initialize session states (these can be after set_page_config)\n",
        "    if 'initial_setup_completed' not in st.session_state:\n",
        "        st.session_state.initial_setup_completed = False\n",
        "    if 'setup_failed' not in st.session_state:\n",
        "        st.session_state.setup_failed = False\n",
        "    if 'show_setup_logs' not in st.session_state:\n",
        "        st.session_state.show_setup_logs = False\n",
        "    if 'proceed_clicked' not in st.session_state:\n",
        "        st.session_state.proceed_clicked = False\n",
        "    if 'show_results' not in st.session_state:\n",
        "        st.session_state.show_results = False\n",
        "    if 'file_selected_successfully' not in st.session_state:\n",
        "        st.session_state.file_selected_successfully = False\n",
        "    if 'selected_filename' not in st.session_state:\n",
        "        st.session_state.selected_filename = None\n",
        "    if 'prediction_results_df' not in st.session_state:\n",
        "        st.session_state.prediction_results_df = None\n",
        "    if 'total_packets' not in st.session_state:\n",
        "        st.session_state.total_packets = 0\n",
        "    if 'prompt_csv_path' not in st.session_state: # New: Store path to the full data for LLM\n",
        "        st.session_state.prompt_csv_path = None\n",
        "\n",
        "    # LLM specific states\n",
        "    if 'llm_chat_history' not in st.session_state:\n",
        "        st.session_state.llm_chat_history = {} # Stores chat history per Packet_Indices\n",
        "    if 'current_llm_flow_index' not in st.session_state:\n",
        "        st.session_state.current_llm_flow_index = None\n",
        "    if 'show_llm_chat' not in st.session_state:\n",
        "        st.session_state.show_llm_chat = False\n",
        "\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "        <h1 style='text-align: center; color: #b5213b;'>\n",
        "            AI Builders 2025\n",
        "        </h1>\n",
        "        <h1 style='text-align: center;'>\n",
        "            Malicious <span style='color: #074ec0;'>.pcap</span> File Classifier\n",
        "        </h1>\n",
        "        <h3 style='text-align: center;'><span style='color: #1abc9c;'>\n",
        "            By: MiN - Vibrant Peacock</span> 🦚\n",
        "        </h3>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "        <style>\n",
        "        div.stButton > button {\n",
        "            display: block;\n",
        "            margin: 0 auto;\n",
        "        }\n",
        "        </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # --- Setup Logic ---\n",
        "    # The rest of your main function logic can remain largely the same from here\n",
        "    if not st.session_state.initial_setup_completed and not st.session_state.setup_failed:\n",
        "        if st.button(\"Start Setup\"):\n",
        "            initial_setup_cached()\n",
        "            st.rerun()\n",
        "\n",
        "    elif st.session_state.setup_failed:\n",
        "        st.warning(\"Setup failed. Please try again.\")\n",
        "        if st.button(\"Start Setup\"):\n",
        "            initial_setup_cached()\n",
        "            st.rerun()\n",
        "\n",
        "    if st.session_state.initial_setup_completed and not st.session_state.proceed_clicked:\n",
        "        st.success(\":tada: Setup Completed\")\n",
        "        if st.button(\"Proceed\"):\n",
        "            st.session_state.show_setup_logs = False\n",
        "            st.session_state.proceed_clicked = True\n",
        "            st.rerun()\n",
        "\n",
        "    # --- Main Application Logic (after setup and proceed) ---\n",
        "    if st.session_state.initial_setup_completed and st.session_state.proceed_clicked:\n",
        "        if not st.session_state.show_results:\n",
        "            st.info(\":file_folder: Choose a file for analysis.\")\n",
        "\n",
        "            # --- File Selection Options ---\n",
        "            selection_method = st.radio(\n",
        "                \"How would you like to provide the PCAP file?\",\n",
        "                (\"Upload a PCAP file\", \"Choose from Sample Data\"),\n",
        "                key=\"selection_method\"\n",
        "            )\n",
        "\n",
        "            pcap_to_analyze_bytes = None # Renamed to clearly indicate bytes data\n",
        "            pcap_filename_for_analysis = None # Renamed to clearly indicate filename for the pipeline\n",
        "\n",
        "            if selection_method == \"Upload a PCAP file\":\n",
        "                uploaded_file = st.file_uploader(\n",
        "                    \"Choose a PCAP file\", accept_multiple_files=False, type=[\"pcap\"]\n",
        "                )\n",
        "                if uploaded_file is not None:\n",
        "                    pcap_to_analyze_bytes = uploaded_file.read()\n",
        "                    pcap_filename_for_analysis = uploaded_file.name\n",
        "\n",
        "            elif selection_method == \"Choose from Sample Data\":\n",
        "                sample_data_dir = \"sample_pcap\"\n",
        "                if os.path.exists(sample_data_dir):\n",
        "                    pcap_files = [f for f in os.listdir(sample_data_dir) if f.endswith('.pcap')]\n",
        "                    if pcap_files:\n",
        "                        selected_sample_file = st.selectbox(\n",
        "                            \"Select a sample PCAP file:\",\n",
        "                            [\"-- Select a file --\"] + sorted(pcap_files),\n",
        "                            key=\"selected_sample_file\"\n",
        "                        )\n",
        "                        if selected_sample_file != \"-- Select a file --\":\n",
        "                            pcap_filename_for_analysis = selected_sample_file\n",
        "                            sample_file_path = os.path.join(sample_data_dir, pcap_filename_for_analysis)\n",
        "                            with open(sample_file_path, \"rb\") as f:\n",
        "                                pcap_to_analyze_bytes = f.read()\n",
        "                    else:\n",
        "                        st.warning(\"No .pcap files found in the 'sample_pcap' folder.\")\n",
        "                else:\n",
        "                    st.error(\"The 'sample_pcap' folder does not exist. Please create it and add .pcap files.\")\n",
        "\n",
        "            # --- Analysis Button ---\n",
        "            if pcap_to_analyze_bytes is not None and pcap_filename_for_analysis is not None:\n",
        "                if st.button(\"Start Analysis\", key=\"start_analysis_button\"):\n",
        "                    # Save the chosen PCAP (uploaded or sample) to data/in\n",
        "                    os.makedirs('data/in', exist_ok=True)\n",
        "                    target_pcap_path = os.path.join(\"data/in\", pcap_filename_for_analysis)\n",
        "                    try:\n",
        "                        with open(target_pcap_path, \"wb\") as f:\n",
        "                            f.write(pcap_to_analyze_bytes) # Use the bytes data here\n",
        "                        mb_size = len(pcap_to_analyze_bytes) / (1024 * 1024)\n",
        "                        st.session_state.selected_filename = pcap_filename_for_analysis\n",
        "                        st.success(f\":file_folder: '{pcap_filename_for_analysis}' size {mb_size:.2f} MB selected. Starting analysis...\")\n",
        "                        st.session_state.file_selected_successfully = True\n",
        "\n",
        "                        results_df, total_p, prompt_csv_path = run_prediction_pipeline(target_pcap_path, pcap_filename_for_analysis)\n",
        "                        if results_df is not None:\n",
        "                            st.session_state.prediction_results_df = results_df\n",
        "                            st.session_state.total_packets = total_p\n",
        "                            st.session_state.prompt_csv_path = prompt_csv_path\n",
        "                            st.session_state.show_results = True\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.error(\"Analysis failed. Please check the logs above.\")\n",
        "                            st.session_state.file_selected_successfully = False\n",
        "                            st.session_state.show_results = False\n",
        "                            clear_uploaded_files()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error during file processing or analysis: {e}\")\n",
        "                        st.session_state.file_selected_successfully = False\n",
        "                        st.session_state.show_results = False\n",
        "                        clear_uploaded_files()\n",
        "\n",
        "        # Display results once available\n",
        "        if st.session_state.show_results and st.session_state.prediction_results_df is not None:\n",
        "            selected_filename = st.session_state.selected_filename\n",
        "            total_packets = st.session_state.total_packets\n",
        "            st.subheader(f\"Analysis Results for '{selected_filename}'\")\n",
        "            st.info(f\"Total packets analyzed: **{total_packets}**\")\n",
        "\n",
        "            prediction_df = st.session_state.prediction_results_df\n",
        "\n",
        "            if \"show_df\" not in st.session_state:\n",
        "                st.session_state.show_df = False\n",
        "            if \"show_predictions\" not in st.session_state:\n",
        "                st.session_state.show_predictions = True\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                if st.button(\"Show Packet-Level Details\"):\n",
        "                    st.session_state.show_df = True\n",
        "                    st.session_state.show_predictions = False\n",
        "                    st.session_state.show_llm_chat = False # Hide LLM when showing details\n",
        "            with col2:\n",
        "                if st.button(\"Show Prediction Summary\"):\n",
        "                    st.session_state.show_df = False\n",
        "                    st.session_state.show_predictions = True\n",
        "                    st.session_state.show_llm_chat = False # Hide LLM when showing summary\n",
        "\n",
        "            if st.session_state.show_predictions:\n",
        "                st.write(\"### Prediction Summary (Packet Count by Label)\")\n",
        "                if not prediction_df.empty:\n",
        "                    prediction_counts = prediction_df['Label'].value_counts().sort_index()\n",
        "                    st.bar_chart(prediction_counts)\n",
        "                    st.dataframe(prediction_counts.reset_index().rename(columns={'index': 'Label', 'Label': 'Count'}), use_container_width=True)\n",
        "                else:\n",
        "                    st.warning(\"No predictions to display.\")\n",
        "\n",
        "            if st.session_state.show_df:\n",
        "                st.write(\"### Packet-Level Prediction Details\")\n",
        "                st.dataframe(prediction_df, use_container_width=True)\n",
        "\n",
        "            output_prediction_file = os.path.join('data', 'out', os.path.splitext(selected_filename)[0] + '_prediction.csv')\n",
        "            if os.path.exists(output_prediction_file):\n",
        "                with open(output_prediction_file, \"rb\") as file:\n",
        "                    btn = st.download_button(\n",
        "                        label=\"Download Packet Predictions (CSV)\",\n",
        "                        data=file,\n",
        "                        file_name=os.path.basename(output_prediction_file),\n",
        "                        mime=\"text/csv\",\n",
        "                        help=\"Download the CSV file containing packet-level predictions.\"\n",
        "                    )\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"🤖 LLM-Powered Flow Analysis\")\n",
        "\n",
        "            # Load the full prompt data for LLM if not already loaded\n",
        "            if 'full_prompt_df' not in st.session_state and st.session_state.prompt_csv_path:\n",
        "                try:\n",
        "                    st.session_state.full_prompt_df = pd.read_csv(st.session_state.prompt_csv_path)\n",
        "                except FileNotFoundError:\n",
        "                    st.error(f\"LLM data file not found: {st.session_state.prompt_csv_path}\")\n",
        "                    st.session_state.full_prompt_df = pd.DataFrame()\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error loading LLM data: {e}\")\n",
        "                    st.session_state.full_prompt_df = pd.DataFrame()\n",
        "\n",
        "            if not st.session_state.get('full_prompt_df', pd.DataFrame()).empty:\n",
        "                unique_labels = st.session_state.full_prompt_df['Label'].unique().tolist()\n",
        "                label_filter = st.multiselect(\n",
        "                    \"Filter flows by Label for LLM analysis:\",\n",
        "                    options=[\"All\"] + sorted(unique_labels),\n",
        "                    default=[\"All\"],\n",
        "                    key=\"llm_label_filter\"\n",
        "                )\n",
        "\n",
        "                filtered_llm_flows = st.session_state.full_prompt_df.copy()\n",
        "                if \"All\" not in label_filter:\n",
        "                    filtered_llm_flows = filtered_llm_flows[filtered_llm_flows['Label'].isin(label_filter)]\n",
        "\n",
        "                # Exclude Benign from default selection if not explicitly chosen\n",
        "                if \"All\" in label_filter:\n",
        "                    filtered_llm_flows_options = filtered_llm_flows\n",
        "                else:\n",
        "                    filtered_llm_flows_options = filtered_llm_flows[filtered_llm_flows['Label'] != 'Benign']\n",
        "\n",
        "                # Display a dropdown for Packet_Indices of non-benign flows\n",
        "                if not filtered_llm_flows_options.empty:\n",
        "                    # Create options with more descriptive labels\n",
        "                    llm_flow_options = [\n",
        "                        f\"Packet {row['Packet_Indices']} (Label: {row['Label']}, Src: {row['Source_IP']}, Dst: {row['Destination_IP']}:{row['Destination_Port']})\"\n",
        "                        for index, row in filtered_llm_flows_options.sort_values(by='Packet_Indices').iterrows()\n",
        "                    ]\n",
        "\n",
        "                    selected_flow_option = st.selectbox(\n",
        "                        \"Select a flow to get LLM insights:\",\n",
        "                        options=[\"-- Select a flow --\"] + llm_flow_options,\n",
        "                        key=\"llm_flow_selection\"\n",
        "                    )\n",
        "\n",
        "                    # Extract Packet_Indices from the selected option\n",
        "                    selected_packet_index = None\n",
        "                    if selected_flow_option != \"-- Select a flow --\":\n",
        "                        selected_packet_index = int(selected_flow_option.split(\" (Label:\")[0].replace(\"Packet \", \"\"))\n",
        "\n",
        "                    if selected_packet_index:\n",
        "                        # Get the row corresponding to the selected packet index\n",
        "                        selected_flow_row = st.session_state.full_prompt_df[\n",
        "                            st.session_state.full_prompt_df['Packet_Indices'] == selected_packet_index\n",
        "                        ].iloc[0]\n",
        "\n",
        "                        # Ensure the chat history for this flow is initialized\n",
        "                        if selected_packet_index not in st.session_state.llm_chat_history:\n",
        "                            st.session_state.llm_chat_history[selected_packet_index] = []\n",
        "                            # Automatically generate initial explanation if not already done\n",
        "                            with st.spinner(f\"Generating initial analysis for Packet {selected_packet_index}...\"):\n",
        "                                initial_prompt_text = create_gemini_prompt_for_streamlit(selected_flow_row)\n",
        "                                try:\n",
        "                                    # Use the gemini_model object from main's scope\n",
        "                                    response = gemini_model.generate_content(initial_prompt_text)\n",
        "                                    st.session_state.llm_chat_history[selected_packet_index].append({\"role\": \"user\", \"content\": initial_prompt_text})\n",
        "                                    st.session_state.llm_chat_history[selected_packet_index].append({\"role\": \"model\", \"content\": response.text})\n",
        "                                    st.session_state.current_llm_flow_index = selected_packet_index\n",
        "                                    st.session_state.show_llm_chat = True\n",
        "                                    # Rerun to show the initial chat immediately\n",
        "                                    st.rerun()\n",
        "                                except Exception as e:\n",
        "                                    st.error(f\"Error generating initial LLM response: {e}\")\n",
        "                                    st.session_state.show_llm_chat = False\n",
        "\n",
        "                        st.session_state.current_llm_flow_index = selected_packet_index\n",
        "                        st.session_state.show_llm_chat = True\n",
        "\n",
        "                        if st.session_state.show_llm_chat and st.session_state.current_llm_flow_index == selected_packet_index:\n",
        "                            st.write(f\"### Chat for Packet {selected_packet_index} ({selected_flow_row['Label']} traffic)\")\n",
        "\n",
        "                            # Display chat messages from history\n",
        "                            for message in st.session_state.llm_chat_history[selected_packet_index]:\n",
        "                                with st.chat_message(message[\"role\"]):\n",
        "                                    st.markdown(message[\"content\"])\n",
        "\n",
        "                            # Chat input for follow-up questions\n",
        "                            prompt_chat_input = st.chat_input(\"Ask a follow-up question about this flow:\")\n",
        "                            if prompt_chat_input:\n",
        "                                st.session_state.llm_chat_history[selected_packet_index].append({\"role\": \"user\", \"content\": prompt_chat_input})\n",
        "                                with st.chat_message(\"user\"):\n",
        "                                    st.markdown(prompt_chat_input)\n",
        "\n",
        "                                with st.chat_message(\"model\"):\n",
        "                                    with st.spinner(\"Thinking...\"):\n",
        "                                        try:\n",
        "                                            # Reconstruct history for current chat session\n",
        "                                            current_chat_history = [\n",
        "                                                {\"role\": \"user\", \"parts\": [msg[\"content\"]]} if msg[\"role\"] == \"user\" else {\"role\": \"model\", \"parts\": [msg[\"content\"]]}\n",
        "                                                for msg in st.session_state.llm_chat_history[selected_packet_index] if msg[\"role\"] in [\"user\", \"model\"]\n",
        "                                            ]\n",
        "\n",
        "                                            # Start a new chat with the current history\n",
        "                                            chat_session = gemini_model.start_chat(history=current_chat_history[:-1]) # Exclude the latest user message from history for send_message\n",
        "\n",
        "                                            full_response = chat_session.send_message(prompt_chat_input)\n",
        "                                            st.markdown(full_response.text)\n",
        "                                            st.session_state.llm_chat_history[selected_packet_index].append({\"role\": \"model\", \"content\": full_response.text})\n",
        "\n",
        "                                        except Exception as e:\n",
        "                                            st.error(f\"Error communicating with LLM: {e}\")\n",
        "                                            st.session_state.llm_chat_history[selected_packet_index].append({\"role\": \"model\", \"content\": f\"Error: {e}\"}) # Add error to history\n",
        "                                st.rerun() # Rerun to display new message\n",
        "                    else:\n",
        "                        st.session_state.show_llm_chat = False # Hide chat if no flow is selected\n",
        "                        st.session_state.current_llm_flow_index = None\n",
        "                else:\n",
        "                    st.info(\"No flows matching the selected labels to analyze with LLM.\")\n",
        "            else:\n",
        "                st.warning(\"No flow data available for LLM analysis. Please ensure a PCAP file was analyzed successfully.\")\n",
        "\n",
        "            # --- Bottom navigation buttons ---\n",
        "            st.markdown(\"---\")\n",
        "            if st.button(\"Analyze another file\", key=\"analyze_another_file_button\"):\n",
        "                clear_uploaded_files()\n",
        "                # Clear all relevant session states for a fresh start\n",
        "                for key in list(st.session_state.keys()):\n",
        "                    if key.startswith(('show_', 'file_selected', 'selected_filename',\n",
        "                                       'prediction_results_df', 'total_packets', 'prompt_csv_path',\n",
        "                                       'llm_')): # Clear LLM related states too\n",
        "                        del st.session_state[key]\n",
        "                st.session_state.proceed_clicked = True # Keep this true if we want to immediately go to file selection\n",
        "                st.rerun()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjC1UE_jcIJy",
        "outputId": "b3a37bc1-51c7-43e5-a9f2-8e947a6c4533"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 Demo Link below this cell (Localtunnel)"
      ],
      "metadata": {
        "id": "ap53mOvfj-Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets\n",
        "try:\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if api_key is not None:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "        print(\"Google API Key loaded from Colab secrets successfully.\")\n",
        "    else:\n",
        "        print(\"Warning: GOOGLE_API_KEY not found in Colab secrets. Please ensure it's set and 'Notebook access' is enabled.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key from Colab secrets: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB_zDv0skUfT",
        "outputId": "c2530eaa-3871-4796-f9a8-4173f6de5e8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key loaded from Colab secrets successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the public IP address\n",
        "import subprocess\n",
        "\n",
        "def get_public_ip():\n",
        "    try:\n",
        "        result = subprocess.run(['curl', 'https://ipinfo.io/ip'], capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error getting IP: {e}\"\n",
        "\n",
        "public_ip = get_public_ip()\n",
        "print(f\"Your public IP address is: {public_ip}\")\n",
        "\n",
        "\n",
        "\n",
        "!npm install localtunnel\n",
        "!streamlit run main.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ztmpoInkPGO",
        "outputId": "292c50a6-fec4-4bb8-f335-d5a5668218d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your public IP address is: 34.19.43.136\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://shaky-pianos-chew.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Content"
      ],
      "metadata": {
        "id": "NJBqdQjLbJIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hosting using local tunnel\n",
        "```\n",
        "# Get the public IP address\n",
        "import subprocess\n",
        "\n",
        "def get_public_ip():\n",
        "    try:\n",
        "        result = subprocess.run(['curl', 'https://ipinfo.io/ip'], capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error getting IP: {e}\"\n",
        "\n",
        "public_ip = get_public_ip()\n",
        "print(f\"Your public IP address is: {public_ip}\")\n",
        "\n",
        "\n",
        "\n",
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "```"
      ],
      "metadata": {
        "id": "oxTcJQMGbUCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hosting using local tunnel\n",
        "```\n",
        "# Get the public IP address\n",
        "import subprocess\n",
        "\n",
        "def get_public_ip():\n",
        "    try:\n",
        "        result = subprocess.run(['curl', 'https://ipinfo.io/ip'], capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error getting IP: {e}\"\n",
        "\n",
        "public_ip = get_public_ip()\n",
        "print(f\"Your public IP address is: {public_ip}\")\n",
        "\n",
        "\n",
        "\n",
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "```"
      ],
      "metadata": {
        "id": "y3peH-0OhRD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hosting using ngrok\n",
        "```\n",
        "# For ngrok\n",
        "!pip install pyngrok -qq\n",
        "\n",
        "# Import ngrok and auth key\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Set up a new ngrok tunnel\n",
        "# You might need to authenticate ngrok if you haven't already.\n",
        "# Get your auth token from https://ngrok.com/signup\n",
        "\n",
        "ngrok_auth_token = userdata.get('ngrok_auth_token')\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "# Run Streamlit in the background and expose it via ngrok\n",
        "# The Streamlit app runs on port 8501 by default\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Open a ngrok tunnel to the Streamlit port\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(\"Your Streamlit app is live at:\", public_url)\n",
        "\n",
        "# You can access the URL directly from the printed output.\n",
        "# You can also use public_url.url to get the string if you want to embed it.\n",
        "```"
      ],
      "metadata": {
        "id": "2c0qgnXghlh6"
      }
    }
  ]
}