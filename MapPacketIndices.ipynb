{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONqQXc73RR9t2irVZSqfE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mc4minta/AIB5-PcapAttackClassifier/blob/main/MapPacketIndices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DwJf05EpI8t",
        "outputId": "139be151-3517-42dc-f4ca-b571422eca81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get attak pcap"
      ],
      "metadata": {
        "id": "UkzLkv8WyBju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "pcap_dir = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/TestsetForBaseline/'\n",
        "os.listdir(pcap_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdcbr8A1pQG-",
        "outputId": "309bbcf8-3d2b-453f-fe44-404b38820d30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DoS.zip',\n",
              " 'DDoS.zip',\n",
              " 'PortScan.zip',\n",
              " 'Benign.zip',\n",
              " 'DoS-HTTP-Flood.zip',\n",
              " 'DoS-SlowRate.zip',\n",
              " 'Benign2.zip',\n",
              " 'DoS-overall.zip',\n",
              " 'DoS-Layer3and4.zip',\n",
              " 'FTP-Bruteforce.zip',\n",
              " 'SSH-Bruteforce.zip']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "def extract_zip_file(zip_name):\n",
        "  dst_dir = '/content/'\n",
        "  with zipfile.ZipFile(pcap_dir+zip_name, 'r') as zip_ref:\n",
        "      zip_ref.extractall(dst_dir)\n",
        "\n",
        "zip_name = 'FTP-Bruteforce.zip'\n",
        "extract_zip_file('FTP-Bruteforce.zip')\n",
        "extract_zip_file('SSH-Bruteforce.zip')"
      ],
      "metadata": {
        "id": "XufYVemDqBXA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CICFlowmeter seup"
      ],
      "metadata": {
        "id": "SyaKuBCuyFIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup cicflowmeter\n",
        "!sudo apt-get install libpcap-dev > /content/log.txt # install libpcap-dev\n",
        "!wget https://codeberg.org/iortega/TCPDUMP_and_CICFlowMeter/archive/master:CICFlowMeters/CICFlowMeter-3.0.zip # download CICFlowMeter3.0.zip\n",
        "!unzip CICFlowMeter-3.0.zip -d CICFlowMeter-3.0 > /content/log.txt # Extract CICFlowMeter3.0.zip\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the directory paths\n",
        "in_directory = 'data/in'\n",
        "out_directory = 'data/out'\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(in_directory, exist_ok=True)\n",
        "os.makedirs(out_directory, exist_ok=True)\n",
        "\n",
        "print(f\"Directories '{in_directory}' and '{out_directory}' created (if they didn't exist).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEDmDnGLquz0",
        "outputId": "da25127f-6da7-446e-cb30-8c7fc451cdc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "--2025-06-06 13:54:23--  https://codeberg.org/iortega/TCPDUMP_and_CICFlowMeter/archive/master:CICFlowMeters/CICFlowMeter-3.0.zip\n",
            "Resolving codeberg.org (codeberg.org)... 217.197.84.140, 2a0a:4580:103f:c0de::1\n",
            "Connecting to codeberg.org (codeberg.org)|217.197.84.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15039922 (14M) [application/octet-stream]\n",
            "Saving to: ‘CICFlowMeter-3.0.zip’\n",
            "\n",
            "CICFlowMeter-3.0.zi 100%[===================>]  14.34M  10.8MB/s    in 1.3s    \n",
            "\n",
            "2025-06-06 13:54:25 (10.8 MB/s) - ‘CICFlowMeter-3.0.zip’ saved [15039922/15039922]\n",
            "\n",
            "Directories 'data/in' and 'data/out' created (if they didn't exist).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CICFlowmter Running"
      ],
      "metadata": {
        "id": "du4Kbw6bsY3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'ftpbrute-ubuntu'"
      ],
      "metadata": {
        "id": "G1bcgSdMh8OK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./CICFlowMeter-3.0/tcpdump_and_cicflowmeter/bin/CICFlowMeter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UBNGCFVCsbot",
        "outputId": "699236c4-841a-4778-a03e-864551151605"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run at:   /content\n",
            "app at:   /content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter\n",
            "SAVED:    /content\n",
            "APP_HOME: /content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter\n",
            "CLASSPATH: /content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/animal-sniffer-annotations-1.14.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/checker-compat-qual-2.0.0.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/CICFlowMeter-3.0.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/commons-io-2.5.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/commons-lang3-3.6.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/commons-math3-3.5.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/error_prone_annotations-2.1.3.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/guava-23.6-jre.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/hamcrest-core-1.3.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/j2objc-annotations-1.1.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/java-cup-0.11a.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/jfreechart-1.5.0.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/jnetpcap-1.4.r1425-1g.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/jsr305-1.3.9.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/junit-4.12.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/log4j-1.2.17.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/slf4j-api-1.7.25.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/slf4j-log4j12-1.7.25.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/tika-core-1.17.jar:/content/CICFlowMeter-3.0/tcpdump_and_cicflowmeter/lib/weka-stable-3.6.14.jar\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \n",
            "cic.cs.unb.ca.ifm.CICFlowMeter CICFlowMeter-V3 found: 1 Files.\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \n",
            "cic.cs.unb.ca.ifm.CICFlowMeter Working on... /content/data/in/ftpbrute-kali.pcap \n",
            "cic.cs.unb.ca.ifm.CICFlowMeter Done! in 0 seconds\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \t Total packets: 13910\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \t Valid packets: 13894\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \t Ignored packets:16 16 \n",
            "cic.cs.unb.ca.ifm.CICFlowMeter PCAP duration 249 seconds\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter ----------------------------------------------------------------------------\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter \n",
            "\n",
            "----------------------------------------------------------------------------\n",
            " TOTAL FLOWS GENERATED: 1217\n",
            "cic.cs.unb.ca.ifm.CICFlowMeter ----------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(f'/content/data/out/{name}_ISCX.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "b6xqEjoWhq7Q",
        "outputId": "0e83c051-8b4e-4078-9592-1c64ecd1324b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Flow ID           Src IP  Src Port  \\\n",
              "0     192.168.159.159-192.168.159.160-21-60050-6  192.168.159.159        21   \n",
              "1     192.168.159.159-192.168.159.160-21-60056-6  192.168.159.159        21   \n",
              "2     192.168.159.159-192.168.159.160-21-60066-6  192.168.159.159        21   \n",
              "3     192.168.159.159-192.168.159.160-21-60070-6  192.168.159.159        21   \n",
              "4     192.168.159.159-192.168.159.160-21-60050-6  192.168.159.160     60050   \n",
              "...                                          ...              ...       ...   \n",
              "1212  192.168.159.159-192.168.159.160-21-60070-6  192.168.159.159        21   \n",
              "1213  192.168.159.159-192.168.159.160-21-50126-6  192.168.159.159        21   \n",
              "1214  192.168.159.159-192.168.159.160-21-43364-6  192.168.159.160     43364   \n",
              "1215  192.168.159.159-192.168.159.160-21-51154-6  192.168.159.159        21   \n",
              "1216  192.168.159.159-192.168.159.160-21-45586-6  192.168.159.159        21   \n",
              "\n",
              "               Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
              "0     192.168.159.160     60050         6  29/05/2025 06:25:45            129   \n",
              "1     192.168.159.160     60056         6  29/05/2025 06:25:45            439   \n",
              "2     192.168.159.160     60066         6  29/05/2025 06:25:45              1   \n",
              "3     192.168.159.160     60070         6  29/05/2025 06:25:45              0   \n",
              "4     192.168.159.159        21         6  29/05/2025 06:25:45          61638   \n",
              "...               ...       ...       ...                  ...            ...   \n",
              "1212  192.168.159.160     60070         6  29/05/2025 06:25:45              1   \n",
              "1213  192.168.159.160     50126         6  29/05/2025 06:29:30              1   \n",
              "1214  192.168.159.159        21         6  29/05/2025 06:29:52            223   \n",
              "1215  192.168.159.160     51154         6  29/05/2025 06:28:01            233   \n",
              "1216  192.168.159.160     45586         6  29/05/2025 06:28:22              1   \n",
              "\n",
              "      Tot Fwd Pkts  Tot Bwd Pkts  ...  Fwd Seg Size Min  Active Mean  \\\n",
              "0                2             1  ...                32          0.0   \n",
              "1                2             1  ...                32          0.0   \n",
              "2                2             0  ...                32          0.0   \n",
              "3                2             0  ...                32          0.0   \n",
              "4                3             0  ...                32          0.0   \n",
              "...            ...           ...  ...               ...          ...   \n",
              "1212             2             0  ...                20          0.0   \n",
              "1213             2             0  ...                20          0.0   \n",
              "1214             2             0  ...                20          0.0   \n",
              "1215             2             0  ...                20          0.0   \n",
              "1216             2             0  ...                20          0.0   \n",
              "\n",
              "      Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  \\\n",
              "0            0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "1            0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "2            0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "3            0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "4            0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "...          ...         ...         ...        ...       ...       ...   \n",
              "1212         0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "1213         0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "1214         0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "1215         0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "1216         0.0         0.0         0.0        0.0       0.0       0.0   \n",
              "\n",
              "      Idle Min     Label  \n",
              "0          0.0  No Label  \n",
              "1          0.0  No Label  \n",
              "2          0.0  No Label  \n",
              "3          0.0  No Label  \n",
              "4          0.0  No Label  \n",
              "...        ...       ...  \n",
              "1212       0.0  No Label  \n",
              "1213       0.0  No Label  \n",
              "1214       0.0  No Label  \n",
              "1215       0.0  No Label  \n",
              "1216       0.0  No Label  \n",
              "\n",
              "[1217 rows x 84 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f61acf21-caf7-4cfd-9d54-9f3cb14bed69\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow ID</th>\n",
              "      <th>Src IP</th>\n",
              "      <th>Src Port</th>\n",
              "      <th>Dst IP</th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>...</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60050-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60050</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>129</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60056-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60056</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>439</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60066-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60066</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60070-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60070</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60050-6</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60050</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>61638</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1212</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-60070-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>60070</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:25:45</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-50126-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>50126</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:29:30</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-43364-6</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>43364</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:29:52</td>\n",
              "      <td>223</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-51154-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>51154</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:28:01</td>\n",
              "      <td>233</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-45586-6</td>\n",
              "      <td>192.168.159.159</td>\n",
              "      <td>21</td>\n",
              "      <td>192.168.159.160</td>\n",
              "      <td>45586</td>\n",
              "      <td>6</td>\n",
              "      <td>29/05/2025 06:28:22</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1217 rows × 84 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f61acf21-caf7-4cfd-9d54-9f3cb14bed69')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f61acf21-caf7-4cfd-9d54-9f3cb14bed69 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f61acf21-caf7-4cfd-9d54-9f3cb14bed69');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6c35bd0e-6817-40aa-839c-5c42ec7774ff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c35bd0e-6817-40aa-839c-5c42ec7774ff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6c35bd0e-6817-40aa-839c-5c42ec7774ff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bd8d8124-7ff5-4311-ae36-15d1e9134932\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bd8d8124-7ff5-4311-ae36-15d1e9134932 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (84) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Success Mapping"
      ],
      "metadata": {
        "id": "pmcLOIajq4K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'ftpbrute-kali'\n",
        "attack = 'FTPBruteforce'"
      ],
      "metadata": {
        "id": "5Ll4S79lvhic"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Google Colab Setup and Imports ---\n",
        "# Install Scapy (if not already installed in your Colab environment)\n",
        "!pip install scapy pandas numpy > /content/log.txt\n",
        "\n",
        "# Import necessary libraries\n",
        "from scapy.all import PcapReader, IP, TCP, UDP, ICMP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipaddress\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import struct\n",
        "\n",
        "# Define constants based on CICFlowMeter's declaration\n",
        "# These are in microseconds based on the provided Java code\n",
        "FLOW_TIMEOUT = 120_000_000 # 120 seconds = 2 minutes\n",
        "FLOW_ACTIVITY_TIMEOUT = 5_000_000 # 5 seconds\n",
        "\n",
        "# Placeholder for your PCAP file and CICFlowMeter output.\n",
        "PCAP_FILE_PATH = f\"{attack}/{name}.pcap\"\n",
        "CICFLOWMETER_OUTPUT_PATH = f\"{attack}/{name}_ISCX.csv\"\n",
        "OUTPUT_MERGED_CSV_PATH = f\"{attack}/{name}_PktIdx.csv\"\n",
        "\n",
        "print(\"Setup complete. Starting script...\")\n",
        "\n",
        "# --- Step 2: `PacketInfo` Class ---\n",
        "class PacketInfo:\n",
        "    def __init__(self, packet_index, timestamp_us, src_ip, dst_ip, src_port, dst_port, protocol, tcp_flags=0):\n",
        "        self.packet_index = packet_index\n",
        "        self.timestamp_us = timestamp_us # Timestamp in microseconds\n",
        "        self.src_ip = src_ip\n",
        "        self.dst_ip = dst_ip\n",
        "        self.src_port = src_port\n",
        "        self.dst_port = dst_port\n",
        "        self.protocol = protocol # e.g., 6 for TCP, 17 for UDP, 1 for ICMP\n",
        "        self.tcp_flags = tcp_flags\n",
        "\n",
        "        # Convert IP strings to byte arrays for comparison logic mirroring Java\n",
        "        self.src_ip_bytes = self._ip_to_bytes(src_ip)\n",
        "        self.dst_ip_bytes = self._ip_to_bytes(dst_ip)\n",
        "\n",
        "        # Store the generated Flow ID immediately upon creation\n",
        "        self.flow_id = self.generate_flow_id()\n",
        "\n",
        "    def _ip_to_bytes(self, ip_str):\n",
        "        \"\"\"Converts an IP address string to a byte array.\"\"\"\n",
        "        try:\n",
        "            return bytes(ipaddress.ip_address(ip_str).packed)\n",
        "        except ValueError:\n",
        "            # Handle cases where IP might be invalid or not fully parsed\n",
        "            # For simplicity, return empty bytes. A more robust solution might log this.\n",
        "            return b''\n",
        "\n",
        "\n",
        "    def has_flag_fin(self):\n",
        "        \"\"\"Check if TCP FIN flag is set.\"\"\"\n",
        "        return (self.protocol == 6) and (self.tcp_flags & 0x01) # FIN flag is 0x01\n",
        "\n",
        "    def generate_flow_id(self):\n",
        "        \"\"\"\n",
        "        Replicates CICFlowMeter's generateFlowId logic based on 5-tuple.\n",
        "        This ensures bidirectional flows have a canonical ID.\n",
        "        \"\"\"\n",
        "        forward = True\n",
        "        src_bytes = self.src_ip_bytes\n",
        "        dst_bytes = self.dst_ip_bytes\n",
        "\n",
        "        # Only proceed with byte comparison if both IP byte arrays are valid\n",
        "        if src_bytes and dst_bytes and len(src_bytes) == len(dst_bytes):\n",
        "            for i in range(len(src_bytes)):\n",
        "                if src_bytes[i] != dst_bytes[i]:\n",
        "                    if src_bytes[i] > dst_bytes[i]:\n",
        "                        forward = False\n",
        "                    break # Exit loop once the first differing byte is found\n",
        "        else:\n",
        "            # If IP bytes are not comparable (e.g., one is missing or malformed),\n",
        "            # default to forward based on string comparison or some other heuristic.\n",
        "            pass\n",
        "\n",
        "\n",
        "        # Construct the Flow ID string\n",
        "        if forward:\n",
        "            flow_id_str = f\"{self.src_ip}-{self.dst_ip}-{self.src_port}-{self.dst_port}-{self.protocol}\"\n",
        "        else:\n",
        "            flow_id_str = f\"{self.dst_ip}-{self.src_ip}-{self.dst_port}-{self.src_port}-{self.protocol}\"\n",
        "        return flow_id_str\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"PacketInfo(idx={self.packet_index}, ts={self.timestamp_us}, \"\n",
        "                f\"flow_id='{self.flow_id}', proto={self.protocol}, \"\n",
        "                f\"src={self.src_ip}:{self.src_port}, dst={self.dst_ip}:{self.dst_port})\")\n",
        "\n",
        "\n",
        "# --- Step 3: `SimulatedFlow` Class ---\n",
        "class SimulatedFlow:\n",
        "    def __init__(self, first_packet: PacketInfo):\n",
        "        self.flow_id_string = first_packet.flow_id # The 5-tuple string\n",
        "        self.flow_start_time_us = first_packet.timestamp_us\n",
        "        self.flow_last_seen_us = first_packet.timestamp_us\n",
        "        self.packet_indices = [] # Stores the packet_index of all packets in this flow\n",
        "\n",
        "        # Store the original src/dst IP bytes from the *first* packet to determine direction\n",
        "        self.initial_src_ip_bytes = first_packet.src_ip_bytes\n",
        "\n",
        "        # Add the first packet's index\n",
        "        self.add_packet_index(first_packet.packet_index)\n",
        "\n",
        "        self.start_active_time_us = first_packet.timestamp_us\n",
        "        self.end_active_time_us = first_packet.timestamp_us\n",
        "\n",
        "    def add_packet_index(self, packet_index: int):\n",
        "        \"\"\"Adds a packet index to the flow.\"\"\"\n",
        "        self.packet_indices.append(packet_index)\n",
        "\n",
        "    def update_flow_timestamps(self, current_timestamp_us: int):\n",
        "        \"\"\"Updates the flow's overall last seen timestamp.\"\"\"\n",
        "        self.flow_last_seen_us = current_timestamp_us\n",
        "\n",
        "    def update_active_idle_time(self, current_timestamp_us: int, threshold_us: int):\n",
        "        \"\"\"\n",
        "        Simulates BasicFlow's updateActiveIdleTime for feature tracking.\n",
        "        Does not cause flow termination in CICFlowMeter's core logic.\n",
        "        \"\"\"\n",
        "        if current_timestamp_us - self.end_active_time_us > threshold_us:\n",
        "            self.start_active_time_us = current_timestamp_us\n",
        "            self.end_active_time_us = current_timestamp_us\n",
        "        else:\n",
        "            self.end_active_time_us = current_timestamp_us\n",
        "\n",
        "    def get_duration_us(self):\n",
        "        \"\"\"Calculates the total duration of the flow in microseconds.\"\"\"\n",
        "        return self.flow_last_seen_us - self.flow_start_time_us\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"SimulatedFlow(id='{self.flow_id_string}', \"\n",
        "                f\"start_ts={self.flow_start_time_us}, end_ts={self.flow_last_seen_us}, \"\n",
        "                f\"packets={len(self.packet_indices)}, \"\n",
        "                f\"indices={self.packet_indices[:5]}...{self.packet_indices[-5:] if len(self.packet_indices) > 5 else ''})\")\n",
        "\n",
        "\n",
        "# --- Step 4: `FlowAggregator` Class ---\n",
        "class FlowAggregator:\n",
        "    def __init__(self, flow_timeout_us, flow_activity_timeout_us):\n",
        "        self.current_flows = {} # {flow_id_string: SimulatedFlow_object}\n",
        "        self.finished_flows = {} # {flow_id_string: SimulatedFlow_object} (using flow_id_string as key for direct lookup)\n",
        "        self.flow_timeout_us = flow_timeout_us\n",
        "        self.flow_activity_timeout_us = flow_activity_timeout_us\n",
        "\n",
        "    def add_packet(self, packet: PacketInfo):\n",
        "        if not packet:\n",
        "            return\n",
        "\n",
        "        current_timestamp = packet.timestamp_us\n",
        "        flow_id_string = packet.flow_id # The 5-tuple string\n",
        "\n",
        "        if flow_id_string in self.current_flows:\n",
        "            flow = self.current_flows[flow_id_string]\n",
        "\n",
        "            # --- CICFlowMeter's Flow Termination Logic (from FlowGenerator.addPacket) ---\n",
        "            # 1. Absolute Flow Timeout Check\n",
        "            if current_timestamp - flow.flow_start_time_us > self.flow_timeout_us:\n",
        "                # If flow has more than 1 packet, consider it finished and store it\n",
        "                if len(flow.packet_indices) > 1:\n",
        "                    self.finished_flows[flow_id_string] = flow # Store by flow_id_string\n",
        "\n",
        "                # Remove old flow and start a new one with the current packet\n",
        "                del self.current_flows[flow_id_string]\n",
        "                self.current_flows[flow_id_string] = SimulatedFlow(packet)\n",
        "\n",
        "            # 2. TCP FIN Flag Termination\n",
        "            elif packet.has_flag_fin():\n",
        "                # Add the FIN packet to the flow before terminating\n",
        "                flow.add_packet_index(packet.packet_index)\n",
        "                flow.update_flow_timestamps(current_timestamp)\n",
        "\n",
        "                # Finish the flow if it has more than 1 packet\n",
        "                if len(flow.packet_indices) > 1:\n",
        "                    self.finished_flows[flow_id_string] = flow # Store by flow_id_string\n",
        "                del self.current_flows[flow_id_string]\n",
        "\n",
        "            # 3. Normal Packet Addition to Existing Flow\n",
        "            else:\n",
        "                flow.update_active_idle_time(current_timestamp, self.flow_activity_timeout_us)\n",
        "                flow.add_packet_index(packet.packet_index)\n",
        "                flow.update_flow_timestamps(current_timestamp)\n",
        "\n",
        "        else:\n",
        "            # First packet of a new flow\n",
        "            self.current_flows[flow_id_string] = SimulatedFlow(packet)\n",
        "\n",
        "\n",
        "    def finish_all_remaining_flows(self):\n",
        "        \"\"\"\n",
        "        Moves all flows still in current_flows to finished_flows at the end of processing.\n",
        "        \"\"\"\n",
        "        for flow_id_string, flow in list(self.current_flows.items()):\n",
        "            # Only dump/finish if more than 1 packet, mirroring CICFlowMeter's behavior\n",
        "            if len(flow.packet_indices) > 1:\n",
        "                self.finished_flows[flow_id_string] = flow\n",
        "            del self.current_flows[flow_id_string]\n",
        "\n",
        "    def get_simulated_flow_data(self):\n",
        "        \"\"\"\n",
        "        Returns a dictionary mapping CICFlowMeter's Flow ID string to list of packet indices.\n",
        "        This is the data we'll use for merging.\n",
        "        \"\"\"\n",
        "        # Combine finished and any remaining current flows for the final output\n",
        "        # If a flow was already finished, it takes precedence.\n",
        "        all_simulated_flows = {**self.finished_flows, **self.current_flows}\n",
        "\n",
        "        # Create a dictionary to hold the flow_id_string to packet_indices mapping\n",
        "        mapping_dict = {}\n",
        "        for flow_id_str, flow_obj in all_simulated_flows.items():\n",
        "            # Only include flows with more than 1 packet, as CICFlowMeter does for output.\n",
        "            if len(flow_obj.packet_indices) > 1:\n",
        "                mapping_dict[flow_id_str] = flow_obj.packet_indices\n",
        "        return mapping_dict\n",
        "\n",
        "# --- Step 5: PCAP Processing and Flow Aggregation ---\n",
        "flow_aggregator = FlowAggregator(FLOW_TIMEOUT, FLOW_ACTIVITY_TIMEOUT)\n",
        "\n",
        "print(f\"Starting PCAP processing: {PCAP_FILE_PATH}\")\n",
        "packet_count = 0\n",
        "try:\n",
        "    for pkt in PcapReader(PCAP_FILE_PATH):\n",
        "        packet_count += 1\n",
        "\n",
        "        src_ip = None\n",
        "        dst_ip = None\n",
        "        src_port = 0\n",
        "        dst_port = 0\n",
        "        protocol = None\n",
        "        tcp_flags = 0\n",
        "\n",
        "        if IP in pkt:\n",
        "            src_ip = pkt[IP].src\n",
        "            dst_ip = pkt[IP].dst\n",
        "            protocol = pkt[IP].proto\n",
        "\n",
        "            if TCP in pkt:\n",
        "                src_port = pkt[TCP].sport\n",
        "                dst_port = pkt[TCP].dport\n",
        "                tcp_flags = pkt[TCP].flags\n",
        "            elif UDP in pkt:\n",
        "                src_port = pkt[UDP].sport\n",
        "                dst_port = pkt[UDP].dport\n",
        "            elif ICMP in pkt:\n",
        "                pass\n",
        "\n",
        "            timestamp_us = int(pkt.time * 1_000_000)\n",
        "\n",
        "            if src_ip and dst_ip and protocol is not None:\n",
        "                p_info = PacketInfo(\n",
        "                    packet_index=packet_count,\n",
        "                    timestamp_us=timestamp_us,\n",
        "                    src_ip=src_ip,\n",
        "                    dst_ip=dst_ip,\n",
        "                    src_port=src_port,\n",
        "                    dst_port=dst_port,\n",
        "                    protocol=protocol,\n",
        "                    tcp_flags=tcp_flags\n",
        "                )\n",
        "                flow_aggregator.add_packet(p_info)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: PCAP file not found at {PCAP_FILE_PATH}. Please upload it or check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during PCAP processing: {e}\")\n",
        "    # Optional: Log the packet number where the error occurred\n",
        "    print(f\"Error occurred at packet_count: {packet_count}\")\n",
        "\n",
        "\n",
        "# After processing all packets, finish any remaining active flows\n",
        "flow_aggregator.finish_all_remaining_flows()\n",
        "\n",
        "simulated_flow_data = flow_aggregator.get_simulated_flow_data()\n",
        "\n",
        "print(f\"\\nPCAP processing complete. Processed {packet_count} packets.\")\n",
        "print(f\"Simulated {len(simulated_flow_data)} flows with more than 1 packet.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Map with CICFlowMeter Output ---\n",
        "print(f\"\\nLoading CICFlowMeter output from: {CICFLOWMETER_OUTPUT_PATH}\")\n",
        "try:\n",
        "    cic_df = pd.read_csv(CICFLOWMETER_OUTPUT_PATH)\n",
        "\n",
        "    # Standardize the 'Flow ID' column name and type for merging\n",
        "    if 'Flow ID' in cic_df.columns:\n",
        "        cic_df.rename(columns={'Flow ID': 'Flow ID'}, inplace=True) # Ensure consistent name if multiple possibilities\n",
        "    elif 'FlowId' in cic_df.columns: # Sometimes it's 'FlowId'\n",
        "        cic_df.rename(columns={'FlowId': 'Flow ID'}, inplace=True)\n",
        "    else:\n",
        "        print(\"Error: 'Flow ID' or 'FlowId' column not found in CICFlowMeter CSV.\")\n",
        "        # If you know the column name, you can force it here, e.g., cic_df.columns[0]\n",
        "        # For now, we'll exit or raise an error if the key column isn't found.\n",
        "        raise ValueError(\"Missing 'Flow ID' column in CICFlowMeter output.\")\n",
        "\n",
        "    # Ensure 'Flow ID' in CICFlowMeter output is string type for matching\n",
        "    cic_df['Flow ID'] = cic_df['Flow ID'].astype(str)\n",
        "\n",
        "    # Convert the simulated_flow_data dictionary to a Pandas Series for easy mapping\n",
        "    # The index of this Series will be the 'Flow ID' string, and values will be the packet indices list.\n",
        "    packet_indices_series = pd.Series(simulated_flow_data)\n",
        "\n",
        "    # Add the new column to your CICFlowMeter DataFrame\n",
        "    # Use .get() with a default empty list if a flow_id from the CSV doesn't exist in our map\n",
        "    # This can happen if CICFlowMeter generated flows with only 1 packet and then discarded them,\n",
        "    # or if there are slight differences in flow ID generation due to floating point nuances etc.\n",
        "    cic_df['Packet Indices Mapped'] = cic_df['Flow ID'].apply(lambda x: packet_indices_series.get(x, []))\n",
        "\n",
        "    # Format the list of packet indices into a string as per your request: \"[idx1,idx2,idx3]\"\n",
        "    cic_df['Packet Indices Mapped'] = cic_df['Packet Indices Mapped'].apply(lambda x: f\"[{','.join(map(str, x))}]\")\n",
        "\n",
        "    print(\"\\nMerged DataFrame head (showing Flow ID, Packet Count, and Mapped Indices):\")\n",
        "    # Try to show original 'Packet Count' if it exists for comparison\n",
        "    if 'Packet Count' in cic_df.columns:\n",
        "        print(cic_df[['Flow ID', 'Packet Count', 'Packet Indices Mapped']].head())\n",
        "    else:\n",
        "        print(cic_df[['Flow ID', 'Packet Indices Mapped']].head())\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    cic_df.to_csv(OUTPUT_MERGED_CSV_PATH, index=False)\n",
        "    print(f\"\\nUpdated CICFlowMeter output with packet indices saved to: {OUTPUT_MERGED_CSV_PATH}\")\n",
        "\n",
        "    # --- Verification ---\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    total_cic_flows = len(cic_df)\n",
        "    mapped_flows_count = cic_df[cic_df['Packet Indices Mapped'] != '[]'].shape[0]\n",
        "    unmapped_flows_count = total_cic_flows - mapped_flows_count\n",
        "\n",
        "    print(f\"Total flows in CICFlowMeter output: {total_cic_flows}\")\n",
        "    print(f\"Flows successfully mapped to packet indices: {mapped_flows_count}\")\n",
        "    print(f\"Flows in CICFlowMeter output that could NOT be mapped (e.g., 1-packet flows, minor ID differences): {unmapped_flows_count}\")\n",
        "\n",
        "    # You can further analyze unmapped flows by filtering:\n",
        "    # unmapped_df = cic_df[cic_df['Packet Indices Mapped'] == '[]']\n",
        "    # print(\"\\nSample of Unmapped Flows:\")\n",
        "    # print(unmapped_df[['Flow ID', 'Packet Count']].head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CICFlowMeter output CSV not found at {CICFLOWMETER_OUTPUT_PATH}. Please upload it or check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during merging with CICFlowMeter output: {e}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve07dINHvW33",
        "outputId": "a1a7244e-d258-4580-8fcc-c80851a1412b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Starting script...\n",
            "Starting PCAP processing: FTPBruteforce/ftpbrute-ubuntu.pcap\n",
            "\n",
            "PCAP processing complete. Processed 20340 packets.\n",
            "Simulated 536 flows with more than 1 packet.\n",
            "\n",
            "Loading CICFlowMeter output from: FTPBruteforce/ftpbrute-ubuntu_ISCX.csv\n",
            "\n",
            "Merged DataFrame head (showing Flow ID, Packet Count, and Mapped Indices):\n",
            "                                      Flow ID Packet Indices Mapped\n",
            "0  192.168.159.159-192.168.159.160-21-44328-6             [263,264]\n",
            "1  192.168.159.159-192.168.159.160-21-44328-6             [263,264]\n",
            "2  192.168.159.159-192.168.159.160-21-44176-6     [278,291,315,316]\n",
            "3  192.168.159.159-192.168.159.160-21-44192-6             [329,330]\n",
            "4  192.168.159.159-192.168.159.160-21-44190-6             [347,348]\n",
            "\n",
            "Updated CICFlowMeter output with packet indices saved to: FTPBruteforce/ftpbrute-ubuntu_PktIdx.csv\n",
            "\n",
            "--- Verification ---\n",
            "Total flows in CICFlowMeter output: 1462\n",
            "Flows successfully mapped to packet indices: 1447\n",
            "Flows in CICFlowMeter output that could NOT be mapped (e.g., 1-packet flows, minor ID differences): 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup and Imports (Run this cell first in Colab) ---\n",
        "# Install Scapy and Pandas (output redirected to avoid clutter)\n",
        "!pip install scapy pandas numpy > /content/log.txt\n",
        "\n",
        "# Import necessary libraries\n",
        "from scapy.all import PcapReader, IP, TCP, UDP, ICMP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipaddress\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import struct\n",
        "import traceback # For detailed error logging\n",
        "\n",
        "# Define constants based on CICFlowMeter's declaration\n",
        "# These are in microseconds based on the provided Java code\n",
        "FLOW_TIMEOUT = 120_000_000 # 120 seconds = 2 minutes\n",
        "FLOW_ACTIVITY_TIMEOUT = 5_000_000 # 5 seconds\n",
        "\n",
        "print(\"Environment setup complete. Libraries installed and imported.\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Helper Classes (PacketInfo, SimulatedFlow, FlowAggregator) ---\n",
        "# These classes encapsulate the core logic for packet and flow handling.\n",
        "# They are defined globally so they can be reused.\n",
        "\n",
        "class PacketInfo:\n",
        "    def __init__(self, packet_index, timestamp_us, src_ip, dst_ip, src_port, dst_port, protocol, tcp_flags=0):\n",
        "        self.packet_index = packet_index\n",
        "        self.timestamp_us = timestamp_us # Timestamp in microseconds\n",
        "        self.src_ip = src_ip\n",
        "        self.dst_ip = dst_ip\n",
        "        self.src_port = src_port\n",
        "        self.dst_port = dst_port\n",
        "        self.protocol = protocol # e.g., 6 for TCP, 17 for UDP, 1 for ICMP\n",
        "        self.tcp_flags = tcp_flags\n",
        "\n",
        "        self.src_ip_bytes = self._ip_to_bytes(src_ip)\n",
        "        self.dst_ip_bytes = self._ip_to_bytes(dst_ip)\n",
        "\n",
        "        self.flow_id = self.generate_flow_id()\n",
        "\n",
        "    def _ip_to_bytes(self, ip_str):\n",
        "        \"\"\"Converts an IP address string to a byte array.\"\"\"\n",
        "        try:\n",
        "            return bytes(ipaddress.ip_address(ip_str).packed)\n",
        "        except ValueError:\n",
        "            return b''\n",
        "\n",
        "    def has_flag_fin(self):\n",
        "        \"\"\"Check if TCP FIN flag is set.\"\"\"\n",
        "        return (self.protocol == 6) and (self.tcp_flags & 0x01) # FIN flag is 0x01\n",
        "\n",
        "    def generate_flow_id(self):\n",
        "        \"\"\"\n",
        "        Replicates CICFlowMeter's generateFlowId logic based on 5-tuple.\n",
        "        This ensures bidirectional flows have a canonical ID.\n",
        "        \"\"\"\n",
        "        forward = True\n",
        "        src_bytes = self.src_ip_bytes\n",
        "        dst_bytes = self.dst_ip_bytes\n",
        "\n",
        "        if src_bytes and dst_bytes and len(src_bytes) == len(dst_bytes):\n",
        "            for i in range(len(src_bytes)):\n",
        "                if src_bytes[i] != dst_bytes[i]:\n",
        "                    if src_bytes[i] > dst_bytes[i]:\n",
        "                        forward = False\n",
        "                    break\n",
        "\n",
        "        if forward:\n",
        "            flow_id_str = f\"{self.src_ip}-{self.dst_ip}-{self.src_port}-{self.dst_port}-{self.protocol}\"\n",
        "        else:\n",
        "            flow_id_str = f\"{self.dst_ip}-{self.src_ip}-{self.dst_port}-{self.src_port}-{self.protocol}\"\n",
        "        return flow_id_str\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"PacketInfo(idx={self.packet_index}, ts={self.timestamp_us}, \"\n",
        "                f\"flow_id='{self.flow_id}', proto={self.protocol}, \"\n",
        "                f\"src={self.src_ip}:{self.src_port}, dst={self.dst_ip}:{self.dst_port})\")\n",
        "\n",
        "class SimulatedFlow:\n",
        "    def __init__(self, first_packet: PacketInfo):\n",
        "        self.flow_id_string = first_packet.flow_id\n",
        "        self.flow_start_time_us = first_packet.timestamp_us\n",
        "        self.flow_last_seen_us = first_packet.timestamp_us\n",
        "        self.packet_indices = []\n",
        "\n",
        "        self.initial_src_ip_bytes = first_packet.src_ip_bytes\n",
        "\n",
        "        self.add_packet_index(first_packet.packet_index)\n",
        "\n",
        "        self.start_active_time_us = first_packet.timestamp_us\n",
        "        self.end_active_time_us = first_packet.timestamp_us\n",
        "\n",
        "    def add_packet_index(self, packet_index: int):\n",
        "        \"\"\"Adds a packet index to the flow.\"\"\"\n",
        "        self.packet_indices.append(packet_index)\n",
        "\n",
        "    def update_flow_timestamps(self, current_timestamp_us: int):\n",
        "        \"\"\"Updates the flow's overall last seen timestamp.\"\"\"\n",
        "        self.flow_last_seen_us = current_timestamp_us\n",
        "\n",
        "    def update_active_idle_time(self, current_timestamp_us: int, threshold_us: int):\n",
        "        \"\"\"Simulates BasicFlow's updateActiveIdleTime for feature tracking.\"\"\"\n",
        "        if current_timestamp_us - self.end_active_time_us > threshold_us:\n",
        "            self.start_active_time_us = current_timestamp_us\n",
        "            self.end_active_time_us = current_timestamp_us\n",
        "        else:\n",
        "            self.end_active_time_us = current_timestamp_us\n",
        "\n",
        "    def get_duration_us(self):\n",
        "        \"\"\"Calculates the total duration of the flow in microseconds.\"\"\"\n",
        "        return self.flow_last_seen_us - self.flow_start_time_us\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"SimulatedFlow(id='{self.flow_id_string}', \"\n",
        "                f\"start_ts={self.flow_start_time_us}, end_ts={self.flow_last_seen_us}, \"\n",
        "                f\"packets={len(self.packet_indices)}, \"\n",
        "                f\"indices={self.packet_indices[:5]}...{self.packet_indices[-5:] if len(self.packet_indices) > 5 else ''})\")\n",
        "\n",
        "class FlowAggregator:\n",
        "    def __init__(self, flow_timeout_us, flow_activity_timeout_us):\n",
        "        self.current_flows = {} # {flow_id_string: SimulatedFlow_object}\n",
        "        self.finished_flows = {} # {flow_id_string: SimulatedFlow_object} (using flow_id_string as key for direct lookup)\n",
        "        self.flow_timeout_us = flow_timeout_us\n",
        "        self.flow_activity_timeout_us = flow_activity_timeout_us\n",
        "\n",
        "    def add_packet(self, packet: PacketInfo):\n",
        "        if not packet:\n",
        "            return\n",
        "\n",
        "        current_timestamp = packet.timestamp_us\n",
        "        flow_id_string = packet.flow_id\n",
        "\n",
        "        if flow_id_string in self.current_flows:\n",
        "            flow = self.current_flows[flow_id_string]\n",
        "\n",
        "            # --- CICFlowMeter's Flow Termination Logic ---\n",
        "            if current_timestamp - flow.flow_start_time_us > self.flow_timeout_us:\n",
        "                if len(flow.packet_indices) > 1:\n",
        "                    self.finished_flows[flow_id_string] = flow\n",
        "                del self.current_flows[flow_id_string]\n",
        "                self.current_flows[flow_id_string] = SimulatedFlow(packet)\n",
        "\n",
        "            elif packet.has_flag_fin():\n",
        "                flow.add_packet_index(packet.packet_index)\n",
        "                flow.update_flow_timestamps(current_timestamp)\n",
        "                if len(flow.packet_indices) > 1:\n",
        "                    self.finished_flows[flow_id_string] = flow\n",
        "                del self.current_flows[flow_id_string]\n",
        "\n",
        "            else:\n",
        "                flow.update_active_idle_time(current_timestamp, self.flow_activity_timeout_us)\n",
        "                flow.add_packet_index(packet.packet_index)\n",
        "                flow.update_flow_timestamps(current_timestamp)\n",
        "\n",
        "        else:\n",
        "            self.current_flows[flow_id_string] = SimulatedFlow(packet)\n",
        "\n",
        "    def finish_all_remaining_flows(self):\n",
        "        \"\"\"Moves all flows still in current_flows to finished_flows.\"\"\"\n",
        "        for flow_id_string, flow in list(self.current_flows.items()):\n",
        "            if len(flow.packet_indices) > 1:\n",
        "                self.finished_flows[flow_id_string] = flow\n",
        "            del self.current_flows[flow_id_string]\n",
        "\n",
        "    def get_simulated_flow_data(self):\n",
        "        \"\"\"\n",
        "        Returns a dictionary mapping CICFlowMeter's Flow ID string to list of packet indices.\n",
        "        \"\"\"\n",
        "        all_simulated_flows = {**self.finished_flows, **self.current_flows}\n",
        "\n",
        "        mapping_dict = {}\n",
        "        for flow_id_str, flow_obj in all_simulated_flows.items():\n",
        "            if len(flow_obj.packet_indices) > 1: # Only include flows with > 1 packet\n",
        "                mapping_dict[flow_id_str] = flow_obj.packet_indices\n",
        "        return mapping_dict\n",
        "\n",
        "\n",
        "\n",
        "### **Main Function: `process_pcap_and_map_flows`**\n",
        "\n",
        "def process_pcap_and_map_flows(attack: str, name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes a PCAP file, simulates CICFlowMeter's flow generation,\n",
        "    and merges the resulting packet-to-flow mapping with an existing\n",
        "    CICFlowMeter output CSV.\n",
        "\n",
        "    Args:\n",
        "        attack (str): The name of the attack directory (e.g., \"FTPBruteforce\").\n",
        "        name (str): The base name of the PCAP and CICFlowMeter output files\n",
        "                    (e.g., \"ftpbrute-ubuntu\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The merged DataFrame with a 'Packet Indices Mapped' column,\n",
        "                      or None if an error occurs.\n",
        "    \"\"\"\n",
        "    # Construct file paths\n",
        "    pcap_file_path = f\"{attack}/{name}.pcap\"\n",
        "    cicflowmeter_output_path = f\"{attack}/{name}_ISCX.csv\"\n",
        "    output_merged_csv_path = f\"{attack}/{name}_PktIdx.csv\"\n",
        "\n",
        "    print(f\"\\n--- Starting processing for {attack}/{name} ---\")\n",
        "    print(f\"PCAP: {pcap_file_path}\")\n",
        "    print(f\"CICFlowMeter Output: {cicflowmeter_output_path}\")\n",
        "    print(f\"Merged Output: {output_merged_csv_path}\")\n",
        "\n",
        "    flow_aggregator = FlowAggregator(FLOW_TIMEOUT, FLOW_ACTIVITY_TIMEOUT)\n",
        "\n",
        "    print(f\"\\nStarting PCAP processing: {pcap_file_path}\")\n",
        "    packet_count = 0\n",
        "    try:\n",
        "        for pkt in PcapReader(pcap_file_path):\n",
        "            packet_count += 1\n",
        "\n",
        "            src_ip = None\n",
        "            dst_ip = None\n",
        "            src_port = 0\n",
        "            dst_port = 0\n",
        "            protocol = None\n",
        "            tcp_flags = 0\n",
        "\n",
        "            if IP in pkt:\n",
        "                src_ip = pkt[IP].src\n",
        "                dst_ip = pkt[IP].dst\n",
        "                protocol = pkt[IP].proto\n",
        "\n",
        "                if TCP in pkt:\n",
        "                    src_port = pkt[TCP].sport\n",
        "                    dst_port = pkt[TCP].dport\n",
        "                    tcp_flags = pkt[TCP].flags\n",
        "                elif UDP in pkt:\n",
        "                    src_port = pkt[UDP].sport\n",
        "                    dst_port = pkt[UDP].dport\n",
        "                elif ICMP in pkt:\n",
        "                    pass\n",
        "\n",
        "                timestamp_us = int(pkt.time * 1_000_000)\n",
        "\n",
        "                if src_ip and dst_ip and protocol is not None:\n",
        "                    p_info = PacketInfo(\n",
        "                        packet_index=packet_count,\n",
        "                        timestamp_us=timestamp_us,\n",
        "                        src_ip=src_ip,\n",
        "                        dst_ip=dst_ip,\n",
        "                        src_port=src_port,\n",
        "                        dst_port=dst_port,\n",
        "                        protocol=protocol,\n",
        "                        tcp_flags=tcp_flags\n",
        "                    )\n",
        "                    flow_aggregator.add_packet(p_info)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PCAP file not found at {pcap_file_path}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during PCAP processing for {pcap_file_path}: {e}\")\n",
        "        print(f\"Error occurred at packet_count: {packet_count}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "    flow_aggregator.finish_all_remaining_flows()\n",
        "    simulated_flow_data = flow_aggregator.get_simulated_flow_data()\n",
        "\n",
        "    print(f\"\\nPCAP processing complete. Processed {packet_count} packets.\")\n",
        "    print(f\"Simulated {len(simulated_flow_data)} flows with more than 1 packet.\")\n",
        "\n",
        "    print(f\"\\nLoading CICFlowMeter output from: {cicflowmeter_output_path}\")\n",
        "    try:\n",
        "        cic_df = pd.read_csv(cicflowmeter_output_path)\n",
        "\n",
        "        # Standardize the 'Flow ID' column name and type for merging\n",
        "        if 'Flow ID' in cic_df.columns:\n",
        "            cic_df.rename(columns={'Flow ID': 'Flow ID'}, inplace=True)\n",
        "        elif 'FlowId' in cic_df.columns:\n",
        "            cic_df.rename(columns={'FlowId': 'Flow ID'}, inplace=True)\n",
        "        else:\n",
        "            print(\"Error: 'Flow ID' or 'FlowId' column not found in CICFlowMeter CSV.\")\n",
        "            return None\n",
        "\n",
        "        cic_df['Flow ID'] = cic_df['Flow ID'].astype(str)\n",
        "\n",
        "        packet_indices_series = pd.Series(simulated_flow_data)\n",
        "\n",
        "        cic_df['Packet Indices Mapped'] = cic_df['Flow ID'].apply(lambda x: packet_indices_series.get(x, []))\n",
        "        # cic_df['Packet Indices Mapped'] = cic_df['Packet Indices Mapped'].apply(lambda x: f\"[{','.join(map(str, x))}]\")\n",
        "\n",
        "        print(\"\\nMerged DataFrame head (showing Flow ID, Packet Count, and Mapped Indices):\")\n",
        "        display_cols = ['Flow ID', 'Packet Indices Mapped']\n",
        "        packet_count_col_name = None\n",
        "        for col in [' Packet Count', 'Packet Count', 'Total Fwd Packets', 'Total Backward Packets', 'Total Packets']:\n",
        "            if col in cic_df.columns:\n",
        "                packet_count_col_name = col\n",
        "                break\n",
        "\n",
        "        if packet_count_col_name:\n",
        "            display_cols.insert(1, packet_count_col_name)\n",
        "\n",
        "        print(cic_df[display_cols].head())\n",
        "\n",
        "        cic_df.to_csv(output_merged_csv_path, index=False)\n",
        "        print(f\"\\nUpdated CICFlowMeter output with packet indices saved to: {output_merged_csv_path}\")\n",
        "\n",
        "        print(\"\\n--- Verification ---\")\n",
        "        total_cic_flows = len(cic_df)\n",
        "        mapped_flows_count = cic_df[cic_df['Packet Indices Mapped'] != '[]'].shape[0]\n",
        "        unmapped_flows_count = total_cic_flows - mapped_flows_count\n",
        "\n",
        "        print(f\"Total flows in CICFlowMeter output: {total_cic_flows}\")\n",
        "        print(f\"Flows successfully mapped to packet indices: {mapped_flows_count}\")\n",
        "        print(f\"Flows in CICFlowMeter output that could NOT be mapped: {unmapped_flows_count}\")\n",
        "\n",
        "        if packet_count_col_name:\n",
        "            print(\"\\nSample of Unmapped Flows:\")\n",
        "            unmapped_df = cic_df[cic_df['Packet Indices Mapped'] == '[]']\n",
        "            print(unmapped_df[['Flow ID', packet_count_col_name]].head())\n",
        "        else:\n",
        "            print(\"\\nSkipping sample of unmapped flows as 'Packet Count' column was not found.\")\n",
        "\n",
        "        # make the cic_df only return the packet_indices mapped of that csv\n",
        "        cic_df = cic_df['Packet Indices Mapped']\n",
        "        return cic_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CICFlowMeter output CSV not found at {cicflowmeter_output_path}.\")\n",
        "        return None\n",
        "    except ValueError as ve:\n",
        "        print(f\"Configuration Error: {ve}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during merging with CICFlowMeter output: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# --- Example Usage (Run this cell after defining the function) ---\n",
        "# IMPORTANT: Before running, ensure your files are uploaded to Colab\n",
        "# E.g., a folder named 'FTPBruteforce' containing 'ftpbrute-ubuntu.pcap' and 'ftpbrute-ubuntu_ISCX.csv'\n",
        "\n",
        "# Example 1: Using your provided case\n",
        "attack_folder = \"FTPBruteforce\"\n",
        "file_base_name = \"ftpbrute-ubuntu\"\n",
        "\n",
        "packet_indices = process_pcap_and_map_flows(attack_folder, file_base_name)\n",
        "\n",
        "if packet_indices is not None:\n",
        "    print(f\"\\nProcessing completed successfully for {attack_folder}/{file_base_name}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffb07agP1jj_",
        "outputId": "46e92e35-5da1-4874-a95e-4aac61a91614"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment setup complete. Libraries installed and imported.\n",
            "\n",
            "--- Starting processing for FTPBruteforce/ftpbrute-ubuntu ---\n",
            "PCAP: FTPBruteforce/ftpbrute-ubuntu.pcap\n",
            "CICFlowMeter Output: FTPBruteforce/ftpbrute-ubuntu_ISCX.csv\n",
            "Merged Output: FTPBruteforce/ftpbrute-ubuntu_PktIdx.csv\n",
            "\n",
            "Starting PCAP processing: FTPBruteforce/ftpbrute-ubuntu.pcap\n",
            "\n",
            "PCAP processing complete. Processed 20340 packets.\n",
            "Simulated 536 flows with more than 1 packet.\n",
            "\n",
            "Loading CICFlowMeter output from: FTPBruteforce/ftpbrute-ubuntu_ISCX.csv\n",
            "\n",
            "Merged DataFrame head (showing Flow ID, Packet Count, and Mapped Indices):\n",
            "                                      Flow ID Packet Indices Mapped\n",
            "0  192.168.159.159-192.168.159.160-21-44328-6            [263, 264]\n",
            "1  192.168.159.159-192.168.159.160-21-44328-6            [263, 264]\n",
            "2  192.168.159.159-192.168.159.160-21-44176-6  [278, 291, 315, 316]\n",
            "3  192.168.159.159-192.168.159.160-21-44192-6            [329, 330]\n",
            "4  192.168.159.159-192.168.159.160-21-44190-6            [347, 348]\n",
            "\n",
            "Updated CICFlowMeter output with packet indices saved to: FTPBruteforce/ftpbrute-ubuntu_PktIdx.csv\n",
            "\n",
            "--- Verification ---\n",
            "Total flows in CICFlowMeter output: 1462\n",
            "Flows successfully mapped to packet indices: 1462\n",
            "Flows in CICFlowMeter output that could NOT be mapped: 0\n",
            "\n",
            "Skipping sample of unmapped flows as 'Packet Count' column was not found.\n",
            "\n",
            "Processing completed successfully for FTPBruteforce/ftpbrute-ubuntu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "packet_indices.iloc[31]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPnlZCv8t1A-",
        "outputId": "0c9d7372-a10c-4d3e-8501-1075ddcf2aa1"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[847, 848]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine with Prediction"
      ],
      "metadata": {
        "id": "tluAIhmq0EW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "def load_model(model_name):\n",
        "  model_path = '/content/drive/MyDrive/Share to Mc4/AIBuilders5-MiN/' + model_name\n",
        "  model = joblib.load(model_path)\n",
        "  return model\n",
        "\n",
        "model_name = 'RandomForest400IntPortCIC1718-2.pkl'\n",
        "model = load_model(model_name)"
      ],
      "metadata": {
        "id": "CXVoFFwr0D95"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def map_port(port):\n",
        "    if port == 21:\n",
        "        return 1  # FTP\n",
        "    elif port == 22:\n",
        "        return 2  # SSH\n",
        "    elif port == 53:\n",
        "        return 3  # DNS\n",
        "    elif port == 80:\n",
        "        return 4  # HTTP\n",
        "    elif port == 443:\n",
        "        return 5  # HTTPS\n",
        "    else:\n",
        "        return 6  # Other\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    # replace space in columns name with underscore\n",
        "    df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "    # drop objects type columns\n",
        "    columns_to_drop = [\n",
        "        'Flow_ID','Src_IP','Dst_IP','Src_Port','Protocol','Timestamp','Label'\n",
        "    ]\n",
        "\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # remove rows with missing and infinite values\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # map destination port to 1-6 numbers\n",
        "    df['Dst_Port'] = df['Dst_Port'].apply(map_port)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess_dataframe(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "fEkAvB630965",
        "outputId": "5ed02163-c527-40f5-994c-ef0d7f55030a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Flow_ID', 'Src_IP', 'Dst_IP', 'Src_Port', 'Protocol', 'Timestamp'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-1b1ad15365f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-122-1b1ad15365f5>\u001b[0m in \u001b[0;36mpreprocess_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     25\u001b[0m     ]\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# remove rows with missing and infinite values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Flow_ID', 'Src_IP', 'Dst_IP', 'Src_Port', 'Protocol', 'Timestamp'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(df)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "3CWrwg0n4jBH",
        "outputId": "08f89b6c-3859-4866-b706-058e47e51fb1"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Label\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-98ae00783123>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2918\u001b[0m     \"\"\"\n\u001b[0;32m-> 2919\u001b[0;31m     \u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2920\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2775\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Feature names must be in the same order as they were in fit.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flow"
      ],
      "metadata": {
        "id": "_NAgP3XfhVel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data/out/ftpbrute-ubuntu_ISCX.csv')\n",
        "df.iloc[9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "fxW-teIRsmzz",
        "outputId": "06d3e88f-a505-4be3-a7d7-a3551a442d39"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Flow ID      192.168.159.159-192.168.159.160-21-44252-6\n",
              "Src IP                                  192.168.159.159\n",
              "Src Port                                             21\n",
              "Dst IP                                  192.168.159.160\n",
              "Dst Port                                          44252\n",
              "                                ...                    \n",
              "Idle Mean                                           0.0\n",
              "Idle Std                                            0.0\n",
              "Idle Max                                            0.0\n",
              "Idle Min                                            0.0\n",
              "Label                                          No Label\n",
              "Name: 9, Length: 84, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Flow ID</th>\n",
              "      <td>192.168.159.159-192.168.159.160-21-44252-6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Src IP</th>\n",
              "      <td>192.168.159.159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Src Port</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dst IP</th>\n",
              "      <td>192.168.159.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dst Port</th>\n",
              "      <td>44252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Idle Mean</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Idle Std</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Idle Max</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Idle Min</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>No Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "five_tuple = df['Src IP'][0] + '-'+ df['Dst IP'][0] + '-'+ str(df['Src Port'][0]) + '-' + str(df['Dst Port'][0]) + '-'+ str(df['Protocol'][0])\n",
        "start_time = pd.to_datetime(df['Timestamp'][0])\n",
        "end_time = df['Timestamp'][0] + pd.to_timedelta(df['Flow Duration'][0], unit='us')\n",
        "print(f\"Flow ID = {df['Flow ID'][0]}\")\n",
        "print(f\"5-tuple = {five_tuple}\")\n",
        "print(f\"Start time = {start_time}\")\n",
        "print(f\"End time = {end_time}\")\n",
        "\n",
        "start_time_unix = start_time.timestamp()\n",
        "end_time_unix = end_time.timestamp()\n",
        "\n",
        "print(f\"Start time (Unix) = {start_time_unix}\")\n",
        "print(f\"End time (Unix) = {end_time_unix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42BytthFsxyh",
        "outputId": "ae7896f4-f4b3-4de8-d572-9be02d648c8b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow ID = 192.168.1.70-192.168.1.90-36908-21-6\n",
            "5-tuple = 192.168.1.70-192.168.1.90-36908-21-6\n",
            "Start time = 1970-01-01 03:58:35\n",
            "End time = 1970-01-01 03:58:45.054168\n",
            "Start time (Unix) = 14315.0\n",
            "End time (Unix) = 14325.054168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scapy"
      ],
      "metadata": {
        "id": "tzYFkDL2sX1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scapy > /content/log.txt"
      ],
      "metadata": {
        "id": "GxxFVRS2uvgV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scapy.all import *\n",
        "\n",
        "flow_path = '/content/data/out/bruteforce_ftp_ISCX.csv'\n",
        "pcap_path = '/content/data/in/bruteforce_ftp.pcap'\n",
        "output_csv = '/content/bruteforce_ftp_packet.csv'\n",
        "\n",
        "packets = rdpcap(pcap_path)"
      ],
      "metadata": {
        "id": "TaUvx7AAuq3w"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "print(packets[0][IP].src)\n",
        "print(packets[0][IP].dst)\n",
        "print(packets[0].sport)\n",
        "print(packets[0].dport)\n",
        "print(packets[0][IP].proto)\n",
        "'''\n",
        "\n",
        "five_tuple_scapy = str(packets[0][IP].src) + '-'+ str(packets[0][IP].dst) + '-'+ str(packets[0].sport) + '-' + str(packets[0].dport) + '-'+ str(packets[0][IP].proto)\n",
        "timestamp_scapy = packets[0].time\n",
        "print(f\"5-tuple = {five_tuple_scapy}\")\n",
        "print(f\"Timestamp = {timestamp_scapy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAKW_GUtwSNt",
        "outputId": "2c93e66d-1810-45d8-b4d9-a3eedf80720d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-tuple = 192.168.1.70-192.168.1.90-36888-21-6\n",
            "Timestamp = 14315.154879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matching"
      ],
      "metadata": {
        "id": "4cQTuFVfxSmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pcap_path = '/content/data/in/ftpbrute-ubuntu.pcap'\n",
        "packets = rdpcap(pcap_path)\n",
        "\n",
        "flow_path = '/content/data/out/ftpbrute-ubuntu_ISCX.csv'\n",
        "df = pd.read_csv(flow_path)"
      ],
      "metadata": {
        "id": "ftzEYLxs7UIo"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79195363",
        "outputId": "cc138a03-dd6d-4eef-ceea-d6e4d92be6e5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_flow_info(df, index):\n",
        "    \"\"\"\n",
        "    Calculates flow information for a given index in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing flow data.\n",
        "        index (int): The index of the row to process.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the 5-tuple, start time (Unix), and end time (Unix).\n",
        "    \"\"\"\n",
        "    flow_id_reverse = df['Dst IP'][index] + '-' + df['Src IP'][index] + '-' + str(df['Dst Port'][index]) + '-' + str(df['Src Port'][index]) + '-' + str(df['Protocol'][index])\n",
        "    start_time = pd.to_datetime(df['Timestamp'][index], utc=True)\n",
        "    end_time = start_time + pd.to_timedelta(df['Flow Duration'][index], unit='us')\n",
        "\n",
        "    start_time_unix = start_time.timestamp()\n",
        "    end_time_unix = end_time.timestamp()\n",
        "\n",
        "    flow_id = df['Flow ID'][index]\n",
        "\n",
        "    return flow_id,flow_id_reverse, start_time_unix, end_time_unix\n",
        "\n",
        "# Example usage of the function\n",
        "flow_id, five_tuple, start_time_unix, end_time_unix = get_flow_info(df, 25)\n",
        "\n",
        "print('--------------------------------------------')\n",
        "print(\"Flow Feature:\")\n",
        "print('--------------------------------------------')\n",
        "print(f\"Flow ID = {flow_id}\")\n",
        "print(f\"5-tuple = {five_tuple}\")\n",
        "print(f\"Start time (Unix) = {start_time_unix}\")\n",
        "print(f\"End time (Unix) = {end_time_unix}\")\n",
        "print('--------------------------------------------')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "Flow Feature:\n",
            "--------------------------------------------\n",
            "Flow ID = 192.168.159.159-192.168.159.160-21-44224-6\n",
            "5-tuple = 192.168.159.159-192.168.159.160-21-44224-6\n",
            "Start time (Unix) = 1748499876.0\n",
            "End time (Unix) = 1748499876.11246\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6be61d01",
        "outputId": "65ee4e2e-297b-49a7-efd4-a37c1163f764"
      },
      "source": [
        "from scapy.all import *\n",
        "\n",
        "def get_packet_info(packets, index):\n",
        "    \"\"\"\n",
        "    Extracts 5-tuple and timestamp information from a packet at a given index.\n",
        "\n",
        "    Args:\n",
        "        packets (PacketList): The Scapy PacketList object.\n",
        "        index (int): The index of the packet to process.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the 5-tuple string and the timestamp.\n",
        "    \"\"\"\n",
        "    packet = packets[index]\n",
        "\n",
        "    try:\n",
        "      src_IP = packet[IP].src\n",
        "      dst_IP = packet[IP].dst\n",
        "      sport = packet.sport\n",
        "      dport = packet.dport\n",
        "      proto = packet[IP].proto\n",
        "    except Exception as e:\n",
        "      return None, None, None # Return None for all three values in case of an error\n",
        "\n",
        "\n",
        "    packet_id = f\"{src_IP}-{dst_IP}-{sport}-{dport}-{proto}\"\n",
        "    packet_id_reverse = f\"{dst_IP}-{src_IP}-{dport}-{sport}-{proto}\"\n",
        "\n",
        "\n",
        "    timestamp = packet.time\n",
        "    return packet_id, packet_id_reverse, timestamp\n",
        "\n",
        "# Example usage of the function\n",
        "packet_id, pkt_rev, timestamp_scapy = get_packet_info(packets, 0)\n",
        "\n",
        "print('--------------------------------------------')\n",
        "print(\"Packet Feature:\")\n",
        "print('--------------------------------------------')\n",
        "print(f\"Packet ID = {packet_id}\")\n",
        "print(f\"Timestamp (Unix) = {timestamp_scapy}\")"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "Packet Feature:\n",
            "--------------------------------------------\n",
            "Packet ID = 192.168.159.159-192.168.159.160-21-44176-6\n",
            "Timestamp (Unix) = 1748499869.134839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def match_index(df,packets,flow_index,packet_index):\n",
        "  flow_id, flow_id_reverse, start_time_unix, end_time_unix = get_flow_info(df, flow_index)\n",
        "  packet_id , packet_id_reverse, timestamp = get_packet_info(packets, packet_index)\n",
        "  if packet_id is None or timestamp_scapy is None:\n",
        "    return False\n",
        "\n",
        "  matched_id = (flow_id == packet_id or flow_id == packet_id_reverse or\n",
        "                  flow_id_reverse == packet_id or flow_id_reverse == packet_id_reverse)\n",
        "  matched_time = start_time_unix <= timestamp <= end_time_unix\n",
        "  return matched_id and matched_time\n",
        "\n",
        "print(len(df))\n",
        "print(len(packets))\n",
        "for i in range(len(df)):\n",
        "  matched_index = []\n",
        "  print(f\"Processing Flow {i}...\")\n",
        "  for j in range(len(packets)):\n",
        "      if(match_index(df,packets,i,j)):\n",
        "        matched_index.append(j)\n",
        "  print(f\"Flow {i} : {matched_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJCA_ywlzker",
        "outputId": "e013e91f-c1c5-49ce-b487-354b2367c453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1462\n",
            "20340\n",
            "Processing Flow 0...\n",
            "Flow 0 : [58, 59, 101, 102, 103, 104, 125, 127, 182, 183, 228, 229, 230, 231, 253, 255]\n",
            "Processing Flow 1...\n",
            "Flow 1 : []\n",
            "Processing Flow 2...\n",
            "Flow 2 : [0, 1, 24, 25, 26, 28, 76, 92, 128, 129, 150, 151, 154, 157, 204, 225]\n",
            "Processing Flow 3...\n",
            "Flow 3 : [2, 3, 32, 33, 34, 35, 89, 100, 130, 131, 160, 161, 162, 163, 213, 227]\n",
            "Processing Flow 4...\n",
            "Flow 4 : [4, 5, 36, 38, 40, 42, 90, 106, 132, 134, 164, 165, 169, 171, 218, 234]\n",
            "Processing Flow 5...\n",
            "Flow 5 : [6, 7, 37, 39, 41, 43, 91, 105, 133, 135, 166, 167, 168, 170, 219, 233]\n",
            "Processing Flow 6...\n",
            "Flow 6 : [8, 9, 44, 45, 46, 47, 93, 110, 136, 137, 172, 173, 174, 175, 220, 238]\n",
            "Processing Flow 7...\n",
            "Flow 7 : [11, 14, 51, 53, 54, 56, 99, 112, 138, 139, 178, 179, 180, 181, 226, 240]\n",
            "Processing Flow 8...\n",
            "Flow 8 : [12, 15, 50, 52, 55, 57, 98, 113, 140, 141, 184, 185, 186, 187, 232, 241]\n",
            "Processing Flow 9...\n",
            "Flow 9 : [18, 19, 68, 69, 70, 71, 109, 119, 145, 147, 190, 191, 192, 194, 235, 243]\n",
            "Processing Flow 10...\n",
            "Flow 10 : [10, 13, 61, 63, 64, 66, 108, 117, 142, 143, 193, 195, 197, 199, 237, 244]\n",
            "Processing Flow 11...\n",
            "Flow 11 : [16, 17, 60, 62, 65, 67, 107, 118, 144, 146, 188, 189, 196, 198, 236, 245]\n",
            "Processing Flow 12...\n",
            "Flow 12 : [20, 21, 72, 73, 74, 75, 111, 120, 148, 149, 200, 201, 202, 203, 239, 248]\n",
            "Processing Flow 13...\n",
            "Flow 13 : [27, 29, 78, 80, 81, 83, 115, 121, 152, 155, 205, 206, 207, 208, 242, 250]\n",
            "Processing Flow 14...\n",
            "Flow 14 : []\n",
            "Processing Flow 15...\n",
            "Flow 15 : [22, 23, 77, 79, 82, 84, 114, 122, 153, 156, 210, 212, 215, 217, 247, 251]\n",
            "Processing Flow 16...\n",
            "Flow 16 : [30, 31, 85, 86, 87, 88, 116, 124, 158, 159, 209, 211, 214, 216, 246, 252]\n",
            "Processing Flow 17...\n",
            "Flow 17 : []\n",
            "Processing Flow 18...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def match_index(df,packets,flow_index,packet_index):\n",
        "  flow_id, flow_id_reverse, start_time_unix, end_time_unix = get_flow_info(df, flow_index)\n",
        "  packet_id , timestamp = get_packet_info(packets, packet_index)\n",
        "  if packet_id is None or timestamp_scapy is None:\n",
        "    return False\n",
        "  matched_id = (flow_id == packet_id) or (flow_id_reverse == packet_id)\n",
        "  matched_time = start_time_unix <= timestamp <= end_time_unix\n",
        "  return matched_id and matched_time\n",
        "\n",
        "i=25\n",
        "for j in range(len(packets)):\n",
        "    if(match_index(df,packets,i,j)):\n",
        "      matched_index.append(j)\n",
        "print(f\"Flow {i} : {matched_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkyDr3xbA_zq",
        "outputId": "819aa961-5d7e-4c26-f504-1e3f0a005877"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow 25 : [20, 21, 72, 73, 74, 75, 111, 120, 148, 149, 200, 201, 202, 203, 239, 248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------------------------------------------')\n",
        "print(\"Flow Feature:\")\n",
        "print('--------------------------------------------')\n",
        "print(f\"Flow ID = {flow_id}\")\n",
        "print(f\"5-tuple = {five_tuple}\")\n",
        "print(f\"Start time (Unix) = {start_time_unix}\")\n",
        "print(f\"End time (Unix) = {end_time_unix}\")\n",
        "print('--------------------------------------------')\n",
        "print(\"Packet Feature:\")\n",
        "print('--------------------------------------------')\n",
        "print(f\"5-tuple = {five_tuple_scapy_0}\")\n",
        "print(f\"Timestamp (Unix) = {timestamp_scapy_0}\")\n",
        "print('--------------------------------------------')\n",
        "print(\"Matching:\")\n",
        "print('--------------------------------------------')"
      ],
      "metadata": {
        "id": "76H68JO-zY72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}